<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Memo</title>
    <link>https://evansin100.github.io/</link>
    <description>Recent content on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 May 2020 22:59:03 +0800</lastBuildDate>
    
	<atom:link href="https://evansin100.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About Shu-Hsin</title>
      <link>https://evansin100.github.io/about/</link>
      <pubDate>Sat, 23 May 2020 22:59:03 +0800</pubDate>
      
      <guid>https://evansin100.github.io/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-adversarial_example_attack/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-adversarial_example_attack/readme/</guid>
      <description>Concept An attack method that uses an adversarial example to cause a deep learning model to make judgment errors. The so-called confrontation example is a deliberately created input data that allows the machine learning model to judge errors. The earliest was Szegedy et al (2013) [2] found that for image recognition models trained with ImageNet, AlexNet and other data sets, only small changes in the input end are often required , and the output results can be greatly changed.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-ai_conference/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-ai_conference/readme/</guid>
      <description>Ranking   (1) Related to artificial intelligence methodology
 First gear: NIPS, ICML, ICLR It can be seen that the three top conferences are also the top three influencers in Google Scholar&amp;rsquo;s 2019 artificial intelligence publications! Second gear: AAAI, IJCAI, COLT     (2) Application related
 Computer Vision CVPR, ECCV, ICCV (Top 3 in Computer Vision and Pattern Recognition     (3) Natural language processing</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-anti_aliasing/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-anti_aliasing/readme/</guid>
      <description>Concept 首先必須談到「反鋸齒」（Anti-aliasing）在電腦繪圖領域的意義。反鋸齒顧名思義，
是減少電腦輸出畫面中，圖像邊緣出現凹凸鋸齒的技術，
而鋸齒的出現是由於 3D 圖形坐標定位時，發生不可避免的圖形混疊所導致的 反鋸齒技術從過去到現在，原理並沒有太大的改變，簡而言之就是用「模糊」換取「精確」，
如果要用修圖的概念來比喻，就有點類似「柔邊」的效果。既然要「柔邊」，
那一定會經歷的過程，就是對影像進行「採樣」（Sampling）
使其模糊化的時候，與原本的鋸齒邊緣看起來相似
  SSAA（Super-Sampling Anti-aliasing）
 SSAA 技術非常消耗運算資源，因為其原理是將原解析度的遊戲畫面放大， 例如 4 倍的 SSAA 就是把原先 1920 X 1080 的畫面 運算出一個 3840 X 2160 的暫存影像，經過取樣後再套用回原畫面中
    MSAA (MultiSampling Anti-Aliasing)
 為了改善 SSAA 過度消耗電腦資源的問題，其變體「多重採樣反鋸齒」 但因為 MSAA 會先判斷畫面的邊緣，再針對該處進行放大後才採樣，所以大幅減輕了電腦運算的負擔。    FXAA (Fast Approximate Anti-Aliasing)
 在 MSAA 成為玩家與遊戲開發商，實現反鋸齒方案的寵兒後， NVIDIA 在 2012 年推出了「快速近似反鋸齒」（Fast Approximate Anti-Aliasing）解決方案， 即 FXAA，在消耗更少硬體資源的前提下，達到與 MSAA 同等級的反鋸齒效果。     DLSS（Deep Learning Super-Sampling）</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-autoencoder/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-autoencoder/readme/</guid>
      <description>Concept 在處理異常偵測時。可以像PCA一樣，可以從input擷取重要特徵，代表全體。
當新的測試資料進來，和這樣的代表特徵比對，就可以判斷是不是異常。
因此設計了一個非監督式學習的神經網路，
其中中間的Internal Representation可以看做是對輸入的資料做壓縮(維度限制)或是加入雜訊到輸入資料
Internal Representation (Bottleneck) 也就是資訊會先被壓縮到比較小的範圍(代表全體),後來再做decode 所以可以用來作為super resolution(U-Net) or Speech(先把input都encode到一個vector,當作看完全部句子) 再做decode
Autoencoder summary 總共有四種
Autoencoder (1. AutoEncoder - AE)) AutoEncoder 是多層神經網絡的一種非監督式學習算法，稱為自動編碼器， 它可以幫助(1)資料分類、(2)視覺化、(3)儲存
其架構中可細分為 Encoder（編碼器）和 Decoder（解碼器）兩部分， 它們分別做壓縮與解壓縮的動作，讓輸出值和輸入值表示相同意義
透過重建輸入的神經網路訓練過程，隱藏層的向量具有降維的作用。 特點是編碼器會建立一個隱藏層（或多個隱藏層）包含了輸入資訊的低維向量。 然後有一個解碼器，會通過隱藏層的低維向量重建輸入資料。 通過神經網路的訓練最後AE會在隱藏層中得到一個代表輸入資料的低維向量
Autoencoder (2. Variational Autoencoder - VAE) Variational =&amp;gt; 變異的
VAE 是 AutoEncoder 的進階版，結構上也是由 Encoder 和 Decoder 所構成
可以看出與 AutoEncoder 不同之處在於 VAE 在&amp;quot;編碼(encode)&amp;ldquo;過程增加了一些限制，迫使生成的向量遵從高斯分佈 由於高斯分佈可以通過其mean 和 standard deviation 進行參數化，因此 VAE 理論上是可以讓你控制要生成的圖片
VAE 的內部做法：
 先輸出兩個向量：mean 和 standard deviation 用normal distribution產生第三個向量</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-camera-3a/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-camera-3a/readme/</guid>
      <description>Overview </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-cnn_image_transformation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-cnn_image_transformation/readme/</guid>
      <description>Concept 古往今來，大家都知道：只要圖像一平移，CNN就認不出來了。
=原因就在降採樣身上。不管是最大池化，跨步卷積，還是平均池化，都對平移太敏感
比如，0、0、1、1、0、0、1、1……這樣的周期，最大池化是這樣：
CNN存在采样缺陷。现代CNN中普遍包含二次采样（subsampling）操作，它是我们常说的降采样层，也就是池化层、stride。
它的本意是为了提高图像的平移不变性，同时减少参数，但它在平移性上的表现真的很一般
Solution : More data? 为什么CNN不能从数据中学习平移不变性？ 虽然上一节论证了CNN在架构上就无法保证平移不变性，
但为什么它就不能从大量数据里学到不变性呢？
事实上，它确实能从数据中学到部分不变性，那么问题还出在哪儿？
论文的观点是数据集里的图像自带“摄影师偏差”，
很可惜论文作者做出的解释很糟糕，一会儿讲分布，一会儿讲数据增强，非常没有说服力。
但是这个观点确实值得关注，心理学领域曾有过关于“摄影师偏差”对人类视角影响的研究，
虽然缺乏数据集论证，但很多人相信，同样的影响也发生在计算机视觉中。
Solution : Add filter in down-sample 現在，來自Adobe的Richard Zhang (簡稱「理查」) ，讓抗鋸齒和各種降採樣和平共處了。
在保留平移不變性的情況下，還能提升ImageNet上的分類準確率。VGG、ResNet、DenseNet……各種架構都適用。 不止如此， 面對其他干擾更穩定了，如旋轉如縮放；面對輸入圖像的損壞，還更魯棒了
想知道怎樣幫助CNN保留平移不變形，就要了解平移不變性是怎樣打破的。
理查觀察了VGG的第一個卷積層，發現它對平移毫無波瀾，並不是在這裡打破的。
但再觀察第一個池化層，對平移有了反應：平移偶數個像素，表征還不改變，
平移奇數個像素，表征就完全變了。 向網絡深處走，經過的池化層越多，問題就越嚴重。
想解決這個問題，就要把抗鋸齒和降採樣友好地結合到一起。
於是，理查又仔細查看了降採樣過程，把它 (按順序) 分成了兩個部分
第一步沒有問題，完全不會出現鋸齒。
第二步就要改了。理查給中間特徵圖，加了個模糊濾波器 (Blur Filter) 來抗鋸齒，然後再做子採樣：
那麼，「抗鋸齒版最大池化」效果怎麼樣？
平移不變性與準確率兼得
e.g., 要做max pool downsample 的時候
先做(1) dense max (2) blur filter(將資訊模糊) (3) down sample
如果降採樣方法不是最大池化，又怎麼樣呢？
所以，理查還測試了ResNet和MobileNetv2，它們用的是跨步卷積；以及DenseNet，它用的是平均池化：
這些架構也獲得了類似的提升。全面成功。
上面只講了分類這一項任務。而進化後的降採樣，在圖到圖翻譯任務上也同樣有效。
不懼各種變換，以及圖像損壞
不只是平移，像旋轉、縮放這樣的干擾，都可以應對自如；另外，面對輸入圖像損壞 (Image Corruption) ，也變得更加魯棒了。 數據顯示，各種濾波器都能有效增強，應對干擾的穩定性，和應對圖像損壞的魯棒性。其中，最強的過濾器Bin-5表現最佳。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-cnn_position_encoding/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-cnn_position_encoding/readme/</guid>
      <description>Concept 文章是ICML2020的一个工作，探究了CNN到底有没有编码位置信息 这些位置信息在哪些神经元中被编码、这些位置信息又是如何被暴露给神经网络学习的。
文章通过大量实验表明，CNN不仅可以编码位置信息，而且越深的层所包含的位置信息越多 (而往往越深的层解释性越差，浅层学习到的形状、边缘等比较容易解释)，
而位置信息是通过zero-padding透露的，显然，图像边缘的zero-padding暗示了图像的边界，
能利用zero-padding带来的位置信息，编码物体所在图像中的位置，
而这一点在显著性目标检测和语义分割等任务中是非常有用的。
上图是三组显显著性区域的heatmap可视化结果，
在每一组实验中，如果给定左边的图，显著性区域是偏中间的，
而把图片右边crop掉(每组的右边的图)，
发现显著性区域有了变换，shift到了对应crop图的中心。
显著性区域一般都是在图像中间的，所以可以简单得出一个结论， CNN学习到了哪里是输入图像的中间，所以将显著性区域的预测结果向中间shift。
Zero padding 位置信息是zero-padding透露的。
足够大的网络（多层或者大kernel）可以把padding透露的边界信息扩散出去，得到粗糙的全局位置信息。
在没有padding的情况下，输出只会直接响应在输入的内容上，不能预测和内容无关的位置信息：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-computer-vision/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-computer-vision/readme/</guid>
      <description>Computer-Vision </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-data-distillation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-data-distillation/readme/</guid>
      <description>Concept 有點像是data augementation的感覺
從未標記資料產生的新知識，能夠用來改善模型。為此，
必須將原本的人工標記資料，和自動標記資料合併，再用合併後的資料來訓練
主要思想是用通過使用一個在大量已標記資料上訓練過的模型在未標記資料上生成annotations，
然後再將所有的annotations（已有的或者新生成的）對模型進行重新訓練
Data Distillation 資料蒸餾主要包括4步：
 (1)首先在人工標記的資料上訓練一個模型； (2)將(1)中訓練好的模型運用到各種轉變形式過後的未標記的資料； (3)將(2)中得到的未標記資料轉變成它們通過模型獲得的預測； (4)將人工標記資料和預測資料進行彙總之後，對模型進行訓練。  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-dataset/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-dataset/readme/</guid>
      <description>Overview  它有60000个训练样本集和10000个测试样本集， 每个样本图像的宽高为28*28。此数据集是以二进制存储的，不能直接以图像格式查看， 不过很容易找到将其转换成图像格式的工具 &amp;lt;/td&amp;gt;   COCO : segmentation COCO的 全称是Common Objects in COntext， COCO数据集是微软团队获取的一个可以用来 图像recognition+segmentation+captioning 数据集， 该数据集主要有的特点如下： （1）Object segmentation （2）Recognition in Context （3）Multiple objects per image （4）More than 300,000 images （5）More than 2 Million instances （6）80 object categories （7）5 captions per image （8）Keypoints on 100,000 people 从这篇文章中，我们了解了这个数据集以scene understanding为目标， 主要从复杂的日常场景中截取，图像中的目标通过精确的segmentation进行位置的标定。 图像包括91类目标，328,000影像和2,500,000个label。 该数据集主要解决3个问题：目标检测，目标之间的上下文关系， 目标的2维上的精确定位。数据集的对比示意图： &amp;lt;/td&amp;gt;   Cifar 10 : classification Cifar 是加拿大政府牵头投资的一个先进科学项目研究所。 Cifar-10 由60000张32*32的 RGB 彩色图片构成， 共10个分类。50000张训练，10000张测试（交叉验证）。 这个数据集最大的特点在于将识别迁移到了普适物体， 而且应用于多分类（姊妹数据集Cifar-100达到100类，ILSVRC比赛则是1000类） &amp;lt;/td&amp;gt;   VOC : classification, detection, segementation PASCAL VOC挑战赛 （The PASCAL Visual Object Classes ） 是一个世界级的计算机视觉挑战赛, PASCAL全称：Pattern Analysis, Statical Modeling and Computational Learning，是一个由欧盟资助的网络组织。 很多优秀的计算机视觉模型比如分类，定位，检测，分割，动作识别等模型 都是基于PASCAL VOC挑战赛及其数据集上推出的， 尤其是一些目标检测模型（比如大名鼎鼎的R CNN系列，以及后面的YOLO，SSD等）。 PASCAL VOC从2005年开始举办挑战赛，每年的内容都有所不同， *从最开始的分类，到后面逐渐增加检测，分割，人体布局，动作识别** （Object Classification 、Object Detection、 Object Segmentation、Human Layout、Action Classification）等内容， 数据集的容量以及种类也在不断的增加和改善。 该项挑战赛催生出了一大批优秀的计算机视觉模型（尤其是以深度学习技术为主的） &amp;lt;/td&amp;gt;  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-dct/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-dct/readme/</guid>
      <description>Concept DCT變換和FFT變換都屬於變換壓縮方法(TransformCompression)，
=&amp;gt; 變換壓縮的一個特點是將(a)從前密度均勻的資訊分佈變換為(b)密度不同的資訊分佈。
在影象中，低頻部分的資訊量要大於高頻部分的資訊量，
儘管低頻部分的資料量比高頻部分的資料量要小的多
例如刪除掉佔50%儲存空間的高頻部分，資訊量的損失可能還不到5%。
變換編碼有很多種。K–L變換的壓縮效率很高，但演算法實現困難； FFT變換演算法實現簡單，但壓縮效率不是很理想。
經過多方面的比較，最終勝出的演算法是DCT，一種源自FFT的變換編碼。
與FFT變換同時使用正弦和餘弦函式來表達訊號不同，
DCT只使用餘弦函式來表達訊號。DCT變換有多個版本，
有一種常用的DCT實現過程是這樣的：對一個長度為129（0到128）的訊號進行DCT變換。
首先，複製點127到點1，使整個訊號變為：
More DCT DCT变换，也就是离散余弦变换（Discrete Cosine Transform)是图像频域变换的一种，
实际上可以看成是一种空域的低通滤波器，DCT也可以看做是傅里叶变换的一种特殊情况。
在傅里叶级数中，如果被展开的函数是实偶函数，那么在傅里叶级数中则只包含余弦项，再将其离散化，由此便可导出离散余弦变化。
目前，离散余弦变换以及它的改进算法已经成为广泛应用于信号处理和图像处理，
特别是用于图像压缩和语音压缩编解码的重要工具和技术。这是由于DCT具有很强的“能量集中”的特性，
大多数的自然信号（包括声音和图像）的能量都集中在离散余弦变换后的低频部分， DCT的作用是吧图像中点和点间的规律呈现出来，虽然DCT本身并没有压缩作用，
但是却为以后压缩时的“取舍”奠定了必不可少的基础。
=&amp;gt; 所以壓縮可以針對高頻部份進行壓縮
Suggest 建議可以看一下範例
有講說DCT做完候 實際的圖長的怎樣
還有高低頻的結果分別放在那裡</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-decision_tree/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-decision_tree/readme/</guid>
      <description>Concept 什么是提升(boost)算法呢？ 直观的理解便是梯度下降 机器学习要做的事情是找到目标函数，优化它，通过每次迭代都使目标函数值最小，
最优解就是目标函数最小化时侯对应的模型参数。
Summary
 (1) 決策樹是一個多功能的機器學習演算法，不僅可以進行分類亦可進行回歸任務。  =&amp;gt; 所以依然還是機器學習的一種   (2) 可以配適複雜的資料集，是個強大的演算法。 (3) 屬於無母數回歸方法(non-parametric)：對資料長相的要求不像回歸模型（有母數法，parametric）嚴格，不需要假設資料的線性關係與常態分佈。  適用於&amp;quot;母體分佈情況未知、小樣本&amp;rdquo;、母體分佈不為常態或不易轉換為常態，對資料長相的要求小。 無母數統計推論時所使用的樣本統計量分配通常與母體分配無關，不需要使用樣本統計量去推論母體中位數、適合度、獨立性、隨機性。 =&amp;gt; 所以可以不用設定參數   (4) 決策樹演算法也是隨機森林演算法的基礎(隨機森林也是至今具潛力的演算法之一)。 (5) 有諸多演算法，常見的包括CART, CHAID。 (6) 決策樹可以用來建立非線性模型，通常被用在迴歸，也可以用在對於遞迴預測變數最二元分類。  Decision tree breifing and VS NN 和回归模型流程相反的模型—决策树，它是通过建立树模型之后，才得到的损失函数，并且成为衡量决策树模型的指标。
有时候数据特征众多且巨大，可以利用这种直观的树结构对数据特征进行切分，然后再构建模型
=&amp;gt; 所以可以先給定資料, 再透過資料產生模型**
=&amp;gt; 這個和一般NN不同,是相反的, NN是先設計模型, 再給資料收斂模型參數
决策树( Decision tree)是在已知各种情况发生概率的基础上,
通过构建决策树来进行分析的一种方式,是一种直观应用概率分析的一种图解法;
 (1) 决策树是一种预测模型,代表的是对象属性与对象值之间的映射关系; (2) 决策树是-种树形结构,其中每个内部节点表示—个属性的测试, (3) 每个分支表示一个测试输出, (4) 每个叶节点代表一种类别; (5) 决策树是一种非常常用的有监督的分类算法 (6) 决策树算法是一种“贪心&amp;quot;算法策略,只考虑在当前数据特征情况下的最好分割方式,不能进行回溯操作。  对于整体的数据集而言,按照所有的特征属性进行划分操作, 对所有划分操作的结果集的“纯度”进行比较,选择“纯度”越高的特征属性作为当前需要分割的数据集进行分割操作, 持续迭代,直到得到最终结果。决策树是通过“纯度”来选择分割特征属性点的。 Example:     Decision tree construction 决策树算法的重点就是决策树的构造;决策树的构造就是进行属性选择度量,</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-deep_hashing/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-deep_hashing/readme/</guid>
      <description>Hash Background Hash，就是把任意长度的输入通过散列算法变换成固定长度的输出，该输出就是散列值。
这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，
所以不可能从散列值来确定唯一的输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。
Deep Hash Concept 最早是從2016提出
Deep Supervised Hashing for Fast Image Retrieval[C]. computer vision and pattern recognition, 2016:
新的Hashing方法用于学习图像紧密的二值编码。在图像检索领域，尽管图像的形貌变化带来非常大的挑战，
但是利用CNN学习一个鲁棒性的图像表达为解决这个挑战带来了曙光。
这边文章就是利用CNN来学习高相似紧凑的二值编码形式，也就是原文作者提到的深度监督Hashing。
特别的，作者设计了CNN结构，利用一对图像输入，输出判别分类
作者方法中的网络框架。这个网络包含3个卷积-池化层 和 两个全连接层</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-denoise/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-denoise/readme/</guid>
      <description>Preliminary super resolution相關的model
和denoise (see in the dark - CAN or U-Net)
因為都是處理per pixel, 所以是類似的應用, 原理也都很像 https://github.com/evansin100/super-resolution/tree/master
autoencoder架構可以用來做denoise
https://github.com/evansin100/ALG-Autoencoder/tree/master</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-differential_equation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-differential_equation/readme/</guid>
      <description>Concept 「微分方程式（differential equation）」：
含有某些未知函數（unknown function）及其導函數（可能不含未知函數自身，但必含未知函數的導函數）的方程式。
微分方程（Differential Equation ）起源與應用 :
許多科學與工程問題皆可以微分方程加以描述與解決，例如：
 衛星、拋射體、火箭、行星等運動問題之應用 某電路之電流與電壓特性探討 熱傳導之研究 放射性物質衰減週期的決定 化學反應與人口成長的研究等等 一般而言，這類問題皆遵守一定之科學定律，
且問題常包含某因子隨其他因子之變化改變狀
況之變化率，因此在數學模型化的過程即會產生微分方程。  Definition 微分方程(differential equation)有3個因素組成
 (1) 函式Y (2) 一個以上的導數(Derivative),對那個變數做微分 (3) 是個方程式(有等於)
解微分方程的意思是求函数 y（或函数 y 的集合）
  Meaning 在这个世界上，事物不停变动，而微分方程往往就是形容这些变动的好方法 微分方程可以形容人口变化、热量移动、弹簧震动、放射性物体衰变及很多其他现象。微分方程是形容宇宙里很多事物的正常并合理的方法 微分方程是表达事物的好方法，但用起来并不容易。
所以我们在解微分方程时，尝试把微分方程转变为比较简单的代数式方程（没有微分）。这样我们便可以计算、画图、预测、等等。
這個例子和上面的不同是他要算總數 =&amp;gt; 以複利的case V 當時的金額 是會一直在變化的 分离变量法(The method of separation of variables) Example 例 1. 今透過廣告推銷某新產品給一百萬的潛在顧客. 若
聽到廣告的顧客數的變化率與未聽到廣告的顧客數成正比.
又一年後有半數的顧客聽過此種產品, 試問兩年後有多少
顧客聽過此種產品?
&amp;lt;解&amp;gt; 此問題是探討聽過廣告的顧客數, 且題目所提供的
訊息是聽到廣告的顧客數的變化率, 故根據導函數的變化
率意義, 令 y 為經過 t 年後聽過此產品的顧客數 (單位:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-end_to_end_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-end_to_end_learning/readme/</guid>
      <description>Concept explanation 1
端到端指的是输入是原始数据，输出是最后的结果，
explanation 2 end-to-end 的本质是你要解决的问题是多阶段的或多步的
如果分阶段学习的话，第一阶段的最优解不能保证第二阶段的问题达到最优。
end-to-end把他们堆在一起来优化，确保最后阶段的解达到最优。
explanation 3 end to end的好处：通过缩减人工预处理和后续处理
尽可能使模型从原始输入到最终输出
给模型更多可以根据数据自动调节的空间，增加模型的整体契合度
=&amp;gt; 好出是方便。坏处是越来越像一个黑箱。
=&amp;gt; 因為只要管好input and output即可
Case not end-to-end (不是現在的主流)： RCNN这种方法需要先在图像中提取可能含有目标的候选框（region proposal），
=&amp;gt; 人工预处理
=&amp;gt; 所以不是原本的圖直接input,先要預選(但後續的maskrcnn2go,也變不用預選了)
然后将这些候选框输入到CNN模型，让CNN判断候选框中是否真的有目标，以及目标的类别是什么。
在我们看到的结果中，往往是类似与下图这种，在整幅图中用矩形框标记目标的位置和大小，并且告诉我们框中的物体是什么。 这种标记的过程，其实是有两部分组成，一是目标所在位置及大小，二是目标的类别。
在整个算法中，目标位置和大小其实是包含在region proposal的过程里，而类别的判定则是在CNN中来判定的
两篇文章一作是一个人，前者不是end-end，后者是
Case end-to-end (主流)： 圖片的處理
現在的物件偵測(Fast RCNN就是了, not RCNN) yolo这种方法就是只通过CNN网络，就能够实现目标的定位和识别。
也就是原始图像输入到CNN网络中，直接输出图像中所有目标的位置和目标的类别。
这种方法就是end-to-end（端对端）的方法，
一端输入我的原始图像，一端输出我想得到的结果。只关心输入和输出，中间的步骤全部都不管</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-entropy/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-entropy/readme/</guid>
      <description>熵 - Entropy 词源 - 最初来源于热力学
Entropy来源于希腊语，原意：内向，
即：一个系统不受外部干扰时往内部稳定状态发展的特性。
定义的其实是一个热力学的系统变化的趋势 1923年，德国科学家普朗克来中国讲学用到entropy这个词， 胡刚复教授看到这个公式，创造了“熵”字，因为“火”和热量有关，定义式又是热量比温度，相当自洽
是接受的每条消息中包含的信息的平均值。又被称为信息熵、信源熵、平均自信息量。
可以被理解为不确定性的度量，熵越大，信源的分布越随机
广义的定义
熵是描述一个系统的无序程度的变量；
同样的表述还有，熵是系统混乱度的度量，
一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行
Summary  (1) 信息熵(information entropy) 是衡量随机变量分布的混乱程度，  是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。 当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在 X 给定条件下，Y 的条件概率分布的熵对 X的期望。   (2) 相对熵(relative entropy - KLD) 可以用来衡量两个概率分布之间的差异。 (3) 交叉熵(cross entropy)  可以来衡量在给定的&amp;quot;真实分布&amp;rdquo;(NN的golden)下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。    </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-exploratory_data_analysis_eda/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-exploratory_data_analysis_eda/readme/</guid>
      <description>Concept EDA (Exploratory Data Analysis) 探索式資料分析
簡單來說，探索式資料分析是運用視覺化或基本統計等工具，
來對資料有個初步的認識，以利後續對資料進行複雜或嚴謹的分析。主要能幫助我們認識資料中三個部分：
 (1) 瞭解資料，獲取資料的資訊、結構和特點。 (2) 檢查有無離群值或異常值，看資料是否有誤。 (3) 分析各變數間的關聯性，找出重要的變數。
進行EDA能檢查資料是否符合分析前的假設、在模型建立前先發現潛在的錯誤，並進一步調整分析方向。  可以用在AI上 範例在
例如target distribution,missing data,outlier ..etc.
https://github.com/evansin100/ALG-gradient_boost/tree/master/3_LightGBM/3_example/kaggle_house_price</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-face-recognition/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-face-recognition/readme/</guid>
      <description>Face-recognition https://github.com/erhwenkuo/face-recognition
人臉檢測是對人臉進行識別和處理的第一步，主要用於檢測並定位圖片中的人臉，
返回高精度的人臉框座標及人臉特徵點座標。人臉識別會進一步提取每個人臉中所藴涵的身份特徵，
並將其與已知的人臉進行對比，從而識別每個人臉的身份。
目前人臉檢測/識別的應用場景逐漸從室內演變到室外，從單一限定場景發展到廣場、車站、地鐵口等場景，
人臉檢測/識別面臨的要求也越來越高，比如：人臉尺度多變、數量宂大、姿勢多樣包括俯拍人臉、
戴帽子口罩等的遮擋、表情誇張、化粧偽裝、光照條件惡劣、分辨率低甚至連肉眼都較難區分等。
隨着深度學習的發展，基於深度學習技術的人臉檢測/識別方法取得了巨大的成功
Flow FaceID(判斷是誰) flow 細節
 或許你會有個疑問:是否可以用SSD來抓圖片中人臉的obejct 自从anchor-based method出现之后，物体检测基本上就离不开这个神奇的anchor了。只因有了它的协助 (anchor就是以pixel為anchor會有多幾個bouding box) 人类才在检测任务上第一次看到了real time的曙光 人脸相对于其他物体来说有一个普遍的特点，就是在图像中所占像素少。 比如，coco数据集中，有一个分类是“人”，但是人脸在人体中只占很少一部分，在全图像上所占比例就更少了 所以SSD不好做face object deteciton &amp;lt;/td&amp;gt;   人臉跟蹤(視頻中跟蹤人臉位置變化)； 人臉驗證(輸入兩張人臉，判斷是否屬於同一人)； 人臉識別(輸入一張人臉，判斷其屬於人臉數據庫記錄中哪一個人)； 人臉聚類(輸入一批人臉，將屬於同一人的自動歸爲一類)； &amp;lt;/td&amp;gt;  Summary  MTCNN方法可以概括为：图像金字塔+3阶段级联CNN 它是由三個不能一起訓練的獨立神經網絡組成的 (所以要分別訓練) PNet: 獲得了人臉區域的候選窗口和邊界框的迴歸矢量 (花的時間最久) RNet: 利用邊界框的迴歸值微調候選窗體，再利用NMS去除重疊窗體 ONet: 去除重疊候選窗口的同時，同時顯示&amp;quot;五個人臉關鍵點定位&amp;quot; &amp;lt;/td&amp;gt;   使用人脸内嵌的欧式距离判断两个人脸是否为同一人， 同一个人的人脸之间的类间距很小，不同的人的人脸之间类间距很大 Flow: (0) 拿到人臉圖(且也效正過) (1) 深度网络处理(抽取出features) facenet源码中可供采用的inception_resnet_v1，inception_resnet_v2和squeezenet，squeezenet网络模型较小， 作者采用的inception_resnet_v1，官方的预训练模型也是居于inception_resnet_v1的。 (2) 进行L2正则化， (3) 然后内嵌得出128维的特征值(將人臉轉換到另外一個維度) 所谓的内嵌，可以理解成一种映射关系，就是把原有的特征空间转换为新的特征空间 (4) Loss(這個就可以和其他人臉就比對,如果loss小於1.06,就是同一個人) 正负样本训练，训练的目标是与正样本之间的类间距最小，与负样本之间的类间距最大 由於Facenet模型輸出的是量化的數值，因此我們就能利用此數值來比對多張臉孔的差異度， 並應用於如下的人臉辨識領域，顯示出Facenet相較其它的技術更為通用 （因為將人臉embedded as code) Face Verification 驗証是否為同一人 Face Identification 辨識身份或姓名 Face Cluster 相似的人分類在一起 Face Search 搜索相似的人 Face Tracking 跟蹤特定的人臉 &amp;lt;/td&amp;gt;  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-gan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-gan/readme/</guid>
      <description>Concept GAN是2014年的一個大神 Ian Goodfellow 提出來的方法，
我用簡單一點的話來表達什麼是GAN，在GAN組織裡面中有二個角色，
(1) 一個是專門偽造假名畫來去賣的G先生，
(2) 一個是專門鑑定此符畫是不是真畫的D先生，
D先生會從G先生那邊拿到假畫來辨斷真假，G先生則是利用D先生的鑑定來改良自己製造假畫的技術，
G先生跟D先生互相共同合作，GAN! 這跟本要大賺了。
下面的Flow是一個最基本的GAN的Flow，可以看出GAN中有二個Neural Network需要去Train
=&amp;gt; 所以要分兩個model來做training
Training Overall Flow 主要是不斷做iteration 而實現的方法，是讓兩個網絡相互競爭
 讓第一代的Discriminator能夠真實的分辨生成的圖片和真實的圖片 輸入真實圖片（real image），標籤 1； 輸入生成圖片（fake image），標籤 0； =&amp;gt; 這個就是一般的辨別model的順練 =&amp;gt; Discriminator可以單獨訓練   (1) Generator和Discriminator合併 (2) 但只有Generator weight會變, Discriminator weight不動 (3) 順練方式: 輸入noise, Generator產生 fake image, fake image輸入進去Discriminator, 我們設定的label = 1 然後調整Generator的weight 讓他可以騙過Discriminator得到prediction =1 =&amp;gt; Generator(W會動)和Discriminator(W不動)要一起順練  Discriminator Network 鑑別器網路簡單一點說明的就是，訓練出一個Neural Network可以分辨偽造出來的圖跟真實的圖 那要怎麼訓練這個網路呢？我自己畫了用以下的圖來理解
沒錯，就是很直觀的，我們把Generator出來的圖標記為0(fake image)，然後把真實的圖標記為1，
這樣的training data 丟進我們的Discriminator Network做訓練，這就是每一次Discriminator訓練的步驟了，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-gmac/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-gmac/readme/</guid>
      <description>calculation method flops就等於 所有的output 所需要的計算
可以先做1個output要的計算
= h x w (filter的計算) x c (有多少組的input channel,因為要相加) + 1
=&amp;gt; 最後是total 就等於
= H&amp;rsquo; x W&amp;rsquo; x n (output的個數) x 1個output要的計算
= H&amp;rsquo; x W&amp;rsquo; x n (output的個數) x (h x w (filter的計算) x c (有多少組的input channel,因為要相加) + 1)
parameter的個數就比較簡單
所有output layer要的參數
因此先計算一層output layer要的參數
= (h x w x c + 1) (這樣可以產生一層output layer)
所有的就是
= n (output channel) x (h x w x c + 1)</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-gradient_boost/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-gradient_boost/readme/</guid>
      <description>Prelimanary - Ensemble Learning RF、GBDT和XGBoost都屬於整合學習（Ensemble Learning），
整合學習的目的是通過結合多個基學習器的預測結果來改善單個學習器的泛化能力和魯棒性
根據個體學習器的生成方式，目前的整合學習方法大致分為兩大類：
即個體學習器之間存在強依賴關係、必須序列生成的序列化方法，以及個體學習器間不存在強依賴關係、可同時生成的並行化方法； 前者的代表就是Boosting，後者的代表是Bagging和“隨機森林”（Random Forest）。
Boosting 在機器學習中，Boosting 是一種透過組合一群 Weak Learners、嘗試改進每一次的錯誤、從而獲得一個 Strong Learner 的方法
Weak Learner 指的是「比亂猜好一點」的模型，這種模型的好性質包含：
複雜度低、訓練的成本低、不容易 Overfitting，例如 Decision Stump（決策樹墩？），
這個模型其實就是把 Decision Tree 的深度限制在一層，
可想而知，只能切一刀的 Decision Tree 大概不會太好用，
但是它卻滿足 Weak Learner 的性質，能夠快速的訓練、並且做出的預測能夠比亂猜好一些
當使用 Weak Learner 作為 Boosting 的 Base Learner，
我們除了能夠快速的訓練出許多模型來組合外，Weak Learner 的低複雜度也為我們帶來一個好性質：
最終組合出的 Strong Learner 能夠對 Overfitting 有良好的抵抗性
Gradient Boosting  (1) 這篇要介紹的 Gradient Boosting 除了 Boosting 一般擁有的性質外，還具備一些好處： (2) Gradient Boosting 可以應用在許多不同的（可微分）Loss Function 上 (3) 利用不同的 Loss Function，我們可以處理 Regression / Classification / Ranking 等不同的問題  Gradient Descent vs Gradient Boosting Gradient Boosting 這東西其實是兩個部分所組成的，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-gradient_descent/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-gradient_descent/readme/</guid>
      <description>Concept 梯度下降法(gradient&amp;quot;梯度&amp;rdquo; descent)是最佳化理論裡面的一個一階找最佳解的一種方法 =&amp;gt; 所以他是一種求解的方式,只是被拿到deep learning使用
主要是希望用梯度下降法找到函數(剛剛舉例的式子)的局部最小值，
因為梯度的方向是走向局部最大的方向，所以在梯度下降法中是往梯度的反方向走。
梯度下降法就好比『我們在山頂，但不知道要下山的路，
於是，我們就沿路找向下坡度最大的叉路走，直到下到平地為止』。要找到向下坡度最大， 在數學上常使用**『偏微分』(Partial Differential)，求取斜率**，
一步步的逼近，直到沒有顯著改善為止，這時我們就認為是最佳解了，過程可參考下圖說明。
這邊我們先大概說一下梯度， 要算一個函數J(w) &amp;ldquo;loss function&amp;quot;的梯度有一個前提，就是這個函數要是任意可微分函數，
這也是深度學習為什麼都要找可微分函數出來當激活函數(activation function)
=&amp;gt; 因為loss function可以微分才有梯度 底下是一個類似NN找weight併讓loss可以最低的範例 gradient descent example function 1 剛有提到我們需要先設定一個初始化的「解」，此例我設定x(0)=20(故意跟最佳值有差距)
紅色的點是每一次更新找到的解
紅色線是法線，藍色線是切線，法線和切線這兩條線是垂直的，但因為x軸和y軸scale不一樣，所以看不出來它是垂直的。 learning rate = 0.01
一開始loss下降很快,因為斜率很大,所以w的一點點改動就會影響到loss的值
一開始loss下降很慢,因為斜率變很小了
function 2
如果使這個case, learning rate太小
就會找到loss的local optmization解,找不到最低點
因為不會振動到那邊
Forward Propagation and Backpropagation 依據上圖，我們先建構好模型，決定要做幾層的隱藏層，
接著，Neural Network 就會利用 Forward Propagation 及 Backpropagation 機制，如下圖，
幫我們求算模型中最重要的參數 &amp;ndash; 『權重』(Weight)，這個過程就稱為『最佳化』(Optimization)， 最常用的技巧就是『梯度下降』(Gradient Descent)。
接著，我們就來用圖說故事，這部份主要參考 DataCamp Deep Learning in Python 的投影片，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-graph_neural_networks/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-graph_neural_networks/readme/</guid>
      <description>CNN Limitation CNN的捲積不是數學定義上的連續捲積，而是一種定義的離散捲積。
我們用這種捲積來處理圖像(image)，從圖像中提取特徵，並且透過神經網路來學習捲積的權重(weight)
CNN捲積有一些特性，
(1)平移不變性(shift-invariance)
(2)局部性(local connectivity)
(3)多尺度(multi-scale)
這些特性，暫且把它們稱作組合性(Compositionality)，
讓CNN捲積只能處理在歐幾里德空間(Euclidean Structure)的數據。
什麼數據是Euclidean Structure的數據呢？
最經典的就是影像(image)，影片(video)和音頻(speech/voice)，
自然語言文本(text)透過特殊處理，也可以固定維度，投影到歐幾里德空間來操作
Motivation 有一些數據結構是Non-Euclidean的 Non-Euclidean的數據，大致上可以當作圖(Graph)來理解。也就是可以理解成，
由頂點(vertex)和邊(edge)所構成的一種資料格式。 Euclidean數據可以當作是Non-Euclidean的一種特例。
因此對Non-Euclidean理解和處理，可以在Euclidean數據上通用，
這就是為什麼Non-Euclidean數據的表示法學習有其重要性
Implemtation 一般來說GCN分為兩大類，
 (1) 一種是譜圖卷積，是在傅立葉域進行卷積變換; (2) 另一種是非譜圖卷積(也叫做「空間域卷積」)，  是直接在Graph上進行卷積。我們分別介紹這兩類     </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-knapsack_problem/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-knapsack_problem/readme/</guid>
      <description>Concept 將一群物品儘量塞進背包裡面，
 (1) 令背包裡面的物品總價值最高。 (2) 背包沒有容量限制，無論物品是什麼形狀大小，都能塞進背包； (3) 但是背包有重量限制，如果物品太重，就會撐破背包。
以數學術語來說，背包問題就是選擇一個最理想的物品子集合， 在符合重量限制的前提下、求得最大的利益！ 背包問題是經典的 NP-complete 問題，無法快速求得精確解，只能折衷求得近似解  Solution 1: Greedy 窮舉法是最基本的方法。針對全部物品，窮舉所有子集合，找出物品總重量符合限制、物品總價值最大的子集合。
所有的子集合總共 O(2ᴺ) 個，驗證一個子集合需時 O(N) ，所以時間複雜度為 O(2ᴺ N) 。其中 N 是物品的數量。
Solution 2: Dynamic programing 當數值範圍不大時，得以用動態規劃快速求得精確解
動態規劃是比較有效率的方法。分割問題的方式很簡單：
對某一件物品來說，我們可以選擇放或不放；然後移去這件物品，縮小問題範疇 =&amp;gt; 動態規劃, 先看一個問題,算目前最佳解,在看另一個問題(併based on之前結果),不斷去縮小問題的範圍
Example
假設有一個背包的負重最多可達8公斤，而希望在背包中裝入負重範圍內可得之總價物品，
假設是水果好了，水果的編號、單價與重量如下所示：
從空集合開始，每增加一個元素就先求出該階段的最佳解，
直到所有的元素加入至集合中，最後得到的就是最佳解
以背包問題為例，我們使用兩個陣列value與item，
 (1) value表示目前的最佳解所得之總價， (2) item表示最後一個放至背包的水果，  假設有負重量 1～8的背包8個， 並對每個背包求其最佳解。
逐步將水果放入背包中，並求該階段的最佳解：
    </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-kullback_leibler_divergence/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-kullback_leibler_divergence/readme/</guid>
      <description>Preliminary 建議先看一下entropy的定義
因為relative entropy(KLD)是建構在information entropy的基礎上的
此外還有一個information entropy的變形是cross-entropy
https://github.com/evansin100/ALG-Entropy
Concept 相對熵（relative entropy）又稱為KL散度（Kullback–Leibler divergence，簡稱KLD），
資訊散度（information divergence），資訊增益（information gain）。
KL散度是兩個概率分佈P和Q差別的非對稱性的度量。
KL散度是用來度量使用基於Q的編碼來編碼來自P的樣本平均所需的額外的比特個數。
典型情況下，P表示資料的真實分佈，Q表示資料的理論分佈，模型分佈，或P的近似分佈。
KL-divergence，俗稱KL距離，常用來衡量兩個&amp;quot;概率&amp;quot;分佈的距離
我们知道，现实世界里的任何观察都可以看成表示成信息和数据，
一般来说，我们无法获取数据的总体，我们只能拿到数据的部分样本，根据数据的部分样本，
我们会对数据的整体做一个近似的估计，而数据整体本身有一个真实的分布（我们可能永远无法知道），
那么近似估计的概率分布和数据整体真实的概率分布的相似度，或者说差异程度，可以用 KL 散度来表示
Usage 相對熵可以衡量兩個隨機分佈之間的距離，
 當兩個隨機分佈相同時，它們的相對熵為零 當兩個隨機分佈的差別增大時，它們的相對熵也會增大
=&amp;gt; 所以相對熵（KL散度）可以用於比較文字的相似度，先統計出詞的頻率，然後計算  Equation KL 散度，最早是从信息论里演化而来的，所以在介绍 KL 散度之前，我们要先介绍一下信息熵。信息熵的定义如下：
Example 舉一個實際的例子吧：比如有四個類別，一個方法A得到四個類別的概率分別是0.1,0.2,0.3,0.4。
另一種方法B（或者說是事實情況）是得到四個類別的概率分別是0.4,0.3,0.2,0.1,
那麼這兩個分佈的KL-Distance(A,B)= 0.1log(0.1/0.4) + 0.2log(0.2/0.3) + 0.3log(0.3/0.2) + 0.4log(0.4/0.1)
這個裡面有正的，有負的，可以證明KL-Distance()&amp;gt;=0.
從上面可以看出， KL散度是不對稱的。即KL-Distance(A,B)!=KL-Distance(B,A)</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-lstm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-lstm/readme/</guid>
      <description>RNN Problem 在RNN訓練期間，信息不斷地循環往復，神經網絡模型權重的更新非常大。
因為在更新過程中累積了錯誤梯度，會導致網絡不穩定。極端情況下，
權重的值可能變得大到溢出並導致NaN值。爆炸通過擁有大於1的值的網絡層反覆累積梯度導致指數增長產生，
如果值小於1就會出現消失
請注意RNN、LST及其變體主要是隨著時間的推移使用順序處理。請參閱下圖中的水平箭頭 這個箭頭意味著長距離信息必須在到達當前處理單元之前順序穿過所有單元。
這表示它可以很容易地被小於0的數相乘很多次而損壞。這就是梯度消失的原因。
因為有多個cell,每個cell也會有output,但可以只拿最後的cell output如下圖 然後input可以依據時間關係,不斷輸入 LSTM Concept RNN的上述缺點促使科學家開發了一種新的RNN模型變體，
名為長短期記憶網絡（Long Short Term Memory）。 由於LSTM使用門來控制記憶過程，它可以解決這個問題 (它可以繞過單元節點從而記住更長的時間步驟。因此，LSTM可以消除一部分的梯度消失問題)
=&amp;gt; 因為有個快速道路bypass
一個LSTM單位
這裡使用的符號具有以下含義：
a）X：縮放的信息 b）+：添加的信息 c）σ：Sigmoid層
d）tanh：tanh層
e）h（t-1）：上一個LSTM單元的輸出
f）c（t-1）：上一個LSTM單元的記憶
g）X（t）：輸入
h）c（t）：最新的記憶
i）h（t）：輸出
圖中我們可以看出有兩個記憶的state 向量，分別是c和h, 有三個操作閘，分別是forget gate （遺忘閘）、input gate （輸入閘）和output gate （輸出閘）， 有一個輸入向量 x，有三個輸出分別是c和h和y，
如果當前時階沒有取出輸出則沒有y輸出。圖中FC 代表Fully-connected，全連接
因為裡面有fully-connected layer, 所以需要大量的內存(weight) 且由於輸入資料在內部反覆遞歸，參數的數目指數級爆炸(所以有多個cell,weight多的狀況更嚴重) Why tanh 為了克服梯度消失問題，我們需要一個二階導數在趨近零點之前能維持很長距離的函數。
tanh是具有這種屬性的合適的函數
Why sigmoid =&amp;gt; 由於Sigmoid函數可以輸出0或1，它可以用來決定忘記或記住信息。
=&amp;gt; 注意LSTM沒有sofmax
信息通過很多這樣的LSTM單元。圖中標記的LSTM單元有三個主要部分：
LSTM有一個特殊的架構，它可以讓它忘記不必要的信息。
Sigmoid層取得輸入X（t）和h（t-1），
並決定從舊輸出中刪除哪些部分（通過輸出0實現）。
在我們的例子中，當輸入是「他有一個女性朋友瑪麗亞」時，「大衛」的性別可以被遺忘，
因為主題已經變成了瑪麗亞。這個門被稱為遺忘門f（t）。這個門的輸出是f（t）* c（t-1）。
下一步是決定並存儲記憶單元新輸入X（t）的信息。
Sigmoid層決定應該更新或忽略哪些新信息。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-makeup/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-makeup/readme/</guid>
      <description>Concept 主要写下后面涉及的美颜子项目及美颜的效果。
 (1) 美白&amp;mdash;肤色变白，饱和度，色调等 (2) 磨皮&amp;mdash;美肤，皮肤光滑 (3) 滤镜&amp;mdash;徕卡滤镜，ins滤镜 (4) 特效&amp;mdash;抖动，灵魂出窍，马赛克特效 (5) 美形&amp;mdash;瘦脸，大眼，瘦鼻，改眉形 (6) 美妆&amp;mdash;美瞳，眼线，上妆 (7) 贴纸&amp;mdash;脸萌 (8) 3d面具&amp;mdash;套头，3d虚拟形象 (9) 人脸驱动&amp;mdash;表情映射到另外一个人脸上 (10) 换脸&amp;mdash;可以换喜欢的人脸 (11) 换发&amp;mdash;发色可选择 (12) 人脸关键点识别&amp;mdash;68，106，240人脸关键点级联识别，深度学习识别 1-3涉及图像处理，
4-9涉及图形学、渲染等，
10-12涉及机器学习、深度学习。
其中不是独立的，各个模块直接的知识相互交叉。
=&amp;gt; 所以和super resolution感覺不同
=&amp;gt; makeup (Perceptual Image Enhancement) 目標不是儘量還原原圖,而是做出不同的效果(fake),但不被發現  Papers 2016&amp;mdash; Beauty eMakeup: a Deep Makeup Transfer System
2016&amp;mdash; A New Digital Face Makeup Method 2017&amp;mdash; Smart Mirror: Intelligent Makeup Recommendation and Synthesis
2017&amp;mdash; Examples-Rules Guided Deep Neural Network for Makeup Recommendation</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-memc_motion_estimate_and_motion_compensation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-memc_motion_estimate_and_motion_compensation/readme/</guid>
      <description>Concept 深度學習的視頻幀內插技術
利用時域運動關係的視圖擴展，俗稱為視頻內插或者視頻幀率轉換
（frame rate up conversion，FRUC） Super_SloMo : NVIDIA 2018 CVPR
MEMC-Net : 2018 CVPR
IM-NET : 2019
DAIN : 2019 CVPR
Usage  &amp;ldquo;视频插值&amp;quot;主要一般会用于一下几个方面：  (1) 慢动作视频生成（slow-motion） (2) 视角合成（view synthesis） (3) 视频码率提升（frame rate up-conversion）    Chanllege  近年来的深度学习网络在应对视频插值任务时所遇到的主要问题和瓶颈，主要有两点:  (1) 无法处理大规模运动场景（large motions） (2) 只有隐式的（implicitly）或干脆没有处理遮挡区域填充问题（occlusions）    Training HOWTO 就是test frame會包裝成3個實際的frame (image triplets) 1 and 3 當作 t and t+1
2 當作中間插禎的 ground truth(用來確認演算法和ground truth的差異) The training datasets for learning-based methods typically</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-monte-carlo/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-monte-carlo/readme/</guid>
      <description>Concept 蒙特卡羅方法又稱統計模擬法、隨機抽樣技術， 是一種隨機模擬方法，以概率和統計理論方法為基礎的一種計算方法，
是使用隨機數（或更常見的偽隨機數）來解決很多計算問題的方法。 將所求解的問題同一定的概率模型相聯繫，用電子電腦實現統計模擬或抽樣，以獲得問題的近似解
Suitable for 蒙地卡羅方法最適合解決哪些問題？ 舉凡在所有目前具有隨機效應的過程，均可能以蒙地卡羅方法大量模擬單一事件， 藉統計上平均值獲得某設定條件下實際最可能測量值
蒙地卡羅模擬法，是基於大數法則的實證方法，當實驗的次數越多，其平均值也就會越趨近於理論值。
其法則亦可以估算投資組合的各種風險因子，特別是一些難以估算的非線性投資組合。
另外也可處理具時間變異的變異數、不對稱等非常態分配和極端狀況等特殊情形，甚至也可用來計算信用風險。
雖然蒙地卡羅模擬法具有以上優點，但因需要繁雜的電腦技術和大量重複的抽樣，
所須計算成本高且耗時的缺點。最後，若是僅處理非線性及非常態分配的投資組合，
則可以選擇此模擬法，以加速其運算的速度和準確性
Example:Pi 假設披薩的半徑為 r，則盒子邊長為 2r。
那麼，盒子面積為 2r × 2r = 4r^2，
則這一整塊披薩的面積就是 πr^2。
=&amp;gt; 披薩面積與盒子的面積之比就是 πr^2 : 4r^2 = π : 4
「如果我在披薩盒上空均勻地撒芝麻，芝麻就會隨機地落在披薩上和盒子的其他地方，
如果芝麻撒得足夠多，鋪滿整個披薩和包裝盒，
那麼落在披薩上的芝麻數量和處於整個方盒內的芝麻數量就近似地表示披薩的面積和盒子的面積！ =&amp;gt; random灑芝麻
=&amp;gt; 所以有幾個要素 (1) random量要夠大 (2) 要有個判斷的依據求解
import time import random hits=0 pi=0 DARTS=10000*10000 start=time.perf_counter() for i in range(DARTS): =&amp;gt; 灑點 x,y=random.random(),random.random() dist=pow(x ** 2+y**2,0.5) =&amp;gt; 開根號(x^2+y^2), 所以就是變成算和原點的距離 if dist &amp;lt;= 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-nas/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-nas/readme/</guid>
      <description>concept NAS 也就是 Neural Architecture Search，目标是从一堆神经网络组件中，搜索到一个好的神经网络模型。
我们知道神经网络模型是一种可任意堆砌的模型结构， 基础的组件包括 FC（全连接层）、Convolution（卷积层）、Polling（池化层）、Activation（激活函数）等， 后一个组件可以以前一个组件作为输入，不同的组件连接方式和超参配置方式在不同应用场景有不同的效果，
例如下面就是在图像分类场景非常有效的 Inception 模型结构
图内结构比较复杂不理解也没关系，我们只需要知道这个神经网络结构是由图像领域专家花费大量精力设计出来的，
并且经过了巨量的实验和测试才能（在不能解释深度学习原理的情况下）确定这个网络结构。
那么计算机是否可以自己去学习和生成这个复杂的网络结构呢？
目前是不行的，包括各种 NAS 算法的变形还有 ENAS 算法暂时也无法生成这样的网络结构，
这里抛出本文第三个观点，绝大部分机器学习都不是人工智能，计算机不会无缘无故获得既定目标以外的能力。
因此，计算机并不是自己学会编程或者建模，我们还没有设计出自动建模的数据集和算法，
所谓的“AI 设计神经网络模型”，其实只是在给定的搜索空间中查找效果最优的模型结构。(所以還是有限制的solution space) example 例如我们假设模型必须是一个三层的全连接神经网络（一个输入层、一个隐层、一个输出层），
(1) 隐层可以有不同的激活函数和节点个数，
(2) 假设激活函数必须是 relu 或 sigmoid 中的一种，
而隐节点数必须是 10、20、30 中的一个，那么我们称这个网络结构的搜索空间就是{relu, sigmoid} * {10, 20 ,30}。
在搜索空间中可以组合出 6 种可能的模型结构，在可枚举的搜索空间内我们可以分别实现这 6 种可能的模型结构，
最终目标是产出效果最优的模型，那么我们可以分别训练这 6 个模型并以 AUC、正确率等指标来评价模型，
然后返回或者叫生成一个最优的神经网络模型结构
因此，NAS 算法是一种给定模型结构搜索空间的搜索算法，
当然这个搜索空间不可能只有几个参数组合，
在 ENAS 的示例搜索空间大概就有 1.6*10^29 种可选结构，
而搜索算法也不可能通过枚举模型结构分别训练来解决，
而需要一种更有效的启发式的搜索算法，这种算法就是后面会提到的贝叶斯优化、增强学习、进化算法等 search what ? hyper parameters (solution space) 使用超参自动调优前面提到 NAS 是一种搜索算法，是从超大规模的搜索空间找到一个模型效果很好的模型结构，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-nlp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-nlp/readme/</guid>
      <description>NLP coverage 自然語言處理』(Natural Language Processing, 包含speech
它包括文字/語音的辨識、解析與生成，實際應用範疇很廣泛 所以transformer也算是其中一個技術
Example - ChatBot 當人對機器說了一句話，機器要先把那句話轉成文字，稱之為『語音識別』(Speech To Text or Speech recognition)。
機器對那段文字做解析，了解那段文字代表甚麼意義，或者該回甚麼話，稱之為『Text Understanding』。
機器回覆有兩種方式：
以文字回覆：必須自詞庫找出要回覆的一段文字，稱之為『Text Generation』。
以語言回覆：將文字轉為語音合成，稱之為『Text To Speech』</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-object-detection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-object-detection/readme/</guid>
      <description>One-stage Object-detection 速度快, 在feature map每個cell產生bounding box candidate的時候(同步做object分類-class) =&amp;gt; step 1: 就同時進行&amp;quot;分類&amp;rdquo; and &amp;ldquo;box回歸&amp;rdquo;,一步完成
代表作有SSD and Yolo
将物体探测作为一个简单的回归问题，它将输入图像作为输入图像并学习类概率，边界框坐标 所以就是直接進行回歸,e.g.調整box and 分類
因為正負樣本不平衡,且同時進行回歸,所以精度比two-stage差
Two-stage Object-detection 比較準,
=&amp;gt; Step 1: 先產生bounding box candidate (透過selective search, RPN ..etc) 不做分類 e.g. region proposal network會產生很多bounding box,並且透過ROI align/ROI pooling,得到直,再做softmax算score RPN 在 feature map 上取 sliding window，每個 sliding window 的中心點稱之為 anchor point， 然後將事先準備好的 k 個不同尺寸比例的 box 以同一個 anchor point 去計算可能包含物體的機率(score)，取機率最高的 box。 這 k 個 box 稱之為 anchor box。所以每個 anchor point 會得到 2k 個 score， 以及 4k 個座標位置 (box 的左上座標，以及長寬，所以是 4 個數值)。 在 Faster R-CNN 論文裡，預設是取 3 種不同大小搭配 3 種不同長寬比的 anchor box，所以 k 為 3x3 = 9</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-ocr/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-ocr/readme/</guid>
      <description>Optical Character Recognition Our pipeline to automatically recognize handwritten text includes: page segmentation (先分一個大塊的)[1] and line segmentation（再拆成line) [2], followed by handwriting recognition(接著再做辨識) is illustrated in Figure The pipeline of this component is presented in Figure 2. The input is an image containing a line of text and
the output of the module is a string of the text. 另外一個例子是辨識圖中的某塊文字 method 1 : LSTM/RNN (1) Handwriting detection (CNN-biLSTM)
The handwriting detection takes the input image containing a line of text and returns a matrix containing probability of each character appearing.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-openpose/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-openpose/readme/</guid>
      <description>Problem for Pose Estimation (1) 一張圖片裡有多少人，而這些人擺什麼姿勢和人的大小？
(2) 有幾個人是相互疊在一起（overlap）的，他們彼此摭蓋面積？
(3) 無法即時（realtime）
另外論文中也提到了一些現有方法存在的瓶頸，現有方法主要是透過 top-down 的方式：
person detector
single-person pose estimation 來解決此類問題，而這很依賴效能，如果 person detector 失敗了
Key point 文章的核心是提出一种利用Part Affinity Fields（PAFs）的自下而上的人体姿态估计算法。
(1) 研究自下而上算法（得到关键点位置再获得骨架）
(2) 而不是自上而下算法（先检测人，再回归关键点) 是因为后者运算时间会随着图像中人的个数而显著增加，而自下而上所需计算时间基本不变。 所以時間不會隨著人變多而有影響
Main Flow (1) 讀進一張圖片大小為 w×h 的圖片 I。
(2) 送進 model VGG-19 的前 10 層 layer train 出大小一樣為 w×h 的 features F。
(3) 再送進 paper 中提到的 model，會得到以下兩個：
(4) 再將 confidence maps S 和 affinity fields L 送到 greedy inference，就能產生所有人的 2D keypoints。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-poisson_distribution/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-poisson_distribution/readme/</guid>
      <description>Concept 卜瓦松分布 (Poisson distribution)
常常在電視上聽到某某交通法規實施或嚴格取締交通違規行動後，道路上每月發上車禍的次數明顯減少了。
但是所謂的「明顯減少」是怎麼判斷的呢，每個月都會偶爾發生一些意外事故，事件數量到底要有多大的改變才算是明顯的變化？
或者家裡附近的警察每兩個小時固定會出來巡邏一次，那麼家裡門前在兩個小時之內都沒有任何警察經過的機率是多少呢？
這些問題都可以仰賴卜瓦松分配來解決。
 (1) 假設某區域單位時間之內平均事件發生次數為λ， (2) 那麼在這區域中事件發生的次數X就符合卜瓦松分配。
還有許多日常生活中週遭的現象也符合卜瓦松分配，
例如：每小時進入學校大門口的人數、隔壁麵店每小時的客人數量、每次紅綠燈之間的車流量等等。
=&amp;gt; 所以可以用來解決個數分配到時間的問題  以下是卜瓦松分布的數學式：
卜瓦松分布只有一個參數，單位時間平均事件發生次數λ
令X為一離散隨機變數，若X符合卜瓦松分布，其機率密度分布函數為(P.D.F)為:
Example 回到一開始的問題，已知警察平均每兩個小時巡邏一次。那麼家裡門前在兩個小時內都沒有警察經過的機率是: 結論是如果真的警察平均每兩個小時巡邏一次，那麼小偷在兩個小時期間闖空門不被抓到的機會是三成六七。 卜瓦松分布的機率密度函數圖:
=&amp;gt; 因為有公式,所以可以再轉成不同次數底下的機率圖 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-proportional_derivative/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-proportional_derivative/readme/</guid>
      <description>Concept PID 控制​概述：​PID (比例​微積分) 控制​為​業界​最​常見​的​控制​演算法，​是​受到​廣泛​認可​的​一種​工業​控制​做法。 ​本文​將​詳細​講解 PID 控制​原理、​探討​閉​迴圈​系統、​如何​實現 PID 控制​的​參數​調整、​以及​調整​對​控制​系統的​影響
PID 控制器​之所以​大受歡迎，​一方面​是​因為​強勁​的​效能​適合​各式各樣​的​作業​條件；
​另一方面​則是​因為​其​使用​簡單，​讓​工程師​能​輕鬆​簡單​地​操作。 PID 控制​演算法​如其​名​包含​三項​基本​係數：
  (1) ​比例 (Proportional)、
  (2) ​積分 (Integral)、
  (3) ​微分 (Derivative, Differential?)，
​可​通過​進行​控制​參數​調整​以​獲得​最佳​響應
  Working flow PID 控制器​的​基本​工作​原理​就是​要​能​讀取​感​測​器，
​再​計算​比例、
​積分​與​微分​響應​以​得出​所需​的​致​動​器​輸出，
​最後​再​將​這​三大​元件​加總​來​計算​輸出。 在​開始​定義 PID 控制器​參數​之前，​我們​必先​了解​閉​迴圈​系統的​定義，​以及​與其​相關​的​一些​詞彙
  Example 閉​迴圈​系統
​在​典型​的​控制​系統​中，​「程序​變數」​指​的是​需要​加以​控制​的​系統​參數，​例如​溫度 (ºC)、​壓力 (psi) 或​流量 (公升/​分鐘)。 我們​使用​感​測​器​來​量​測​程序​變數，​並將​反饋​提供​給​控制​系統。 「設定​點」​指​的是​程序​變數​所需​值​或是​指令​值，​例如​溫度​控制​系統​中​顯示​的​攝氏 100 度。 在​任何​特定​時間，​控制​系統​演算法 (補償​器) 會​利用​程序​變數​與​設定​點​之間​的​差異，​來​決定​驅動​系統 (機​板) 所需​的​致​動​器​輸出。 舉例來說，​當量​測​的​溫度​程序​變數​為 100 ºC 且​所需​的​溫度​設定​點​為 120 ºC，​則​控制​演算法​指定​的​「致​動​器​輸出」​有​可能​用來​驅動​加熱器。 驅動​致​動​器​來​開啟​加熱器，​會​讓​系統​變成​一台​暖爐，​並​讓​溫度​程序​變數​增加。 這個​現象​稱為​閉​迴圈​控制​系統，​</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-pynet_to_replace_isp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-pynet_to_replace_isp/readme/</guid>
      <description>Motivation 拍照已经是手机最大的卖点，各大厂商在DxO上刷分刷得不亦乐乎，一亿像素、超级夜景、光学变焦……花样越来越多。
 然而不同手机拍出来的质感却不一样，  (1) 一方面是镜头模组不同， (2) 另一方面是对图像处理器（ISP）的软件调教也不同。    最近，苏黎世联邦理工学院（ETHZ）2020 提出一个新的算法PyNet，
Replacing Mobile Camera ISP with a Single Deep Learning Model
PyNET, a novel pyramidal CNN architecture
只需单个端到端深度学习模型，就能替代手机的ISP
=&amp;gt; Benifit 1: 所以就是不用ISP硬體了,camera進直接用AI做處理
可以看到效果很好,和ISP出來的圖幾乎差不多
它调教出的相机算法可以从一个手机移植到另一个手机上，
而不必使用手机的ISP。即使两款手机芯片来自两家厂商，也完全没问题。
用华为P20和佳能单反相机5D Mark IV调教的算法，
移植到黑莓手机KeyOne上，照片质量与原始ISP输出相比，有了很大的改善。
=&amp;gt; Benifit 2: 訓練的model不會綁定特定型號手機
Paper contribution  (1) An end-to-end deep learning solution  for RAW-toRGB image mapping problem that is incorporating all image signal processing steps by design.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-reinforcement_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-reinforcement_learning/readme/</guid>
      <description>Key Example 建議看這個範例
了解怎麼做到reinforcement learning的簡單訓練方式
以及再一開始不會玩的時候,怎麼做random action
https://github.com/evansin100/ALG-Reinforcement_Learning/tree/master/3_Gym/Cartpole-tf-keras
如果想了解怎麼建構一個env(不靠Gym)
則可以看這篇(Maze)
可以看這邊 有完整的Depp reinformance learning的範例(包含training)
https://github.com/evansin100/ALG-Reinforcement_Learning/tree/master/2_Deep-Q-Learning/Maze_Example youtube教學 https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/
Summary  訓練資料必需有input跟label(答案)， 讓機器去mapping出一個最好的模型，常用的演算法為分類、回歸等演算法 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Unsupervised Learning (非監督式學習): &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note 訓練資料只有input沒有label(答案)， 讓機器從訓練資料中找出規則，常用的演算法為集群演算法 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Reinforcement Learning (強化學習): &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note 從現在的環境來決定行為，是一種互動式的學習過程 =&amp;gt; 根據環境, 是一種互動式的學習 強化學習其實就是訓練一個AI 可以通過每一次的錯誤來學習，就跟我們小時候學騎腳踏車一樣， 一開始學的時候會一直跌倒，然後經過幾次的失敗後，我們就可以上手也不會跌倒了 強化式學習的特徵是訓練必須要有正負回報(positive/negative reward)， 在訓練過程中，模型會根據不同的狀況(state)嘗試各種決定(action)， 再根據此決定得到的結果內化吸收， 下方的AI 玩 Mario遊戲影片便是一種應用，在模型最初時， 可以看到角色就是站在原地閒置太久，拿到了負面回報， 所以它改變開始學習向前移動，走了一段路之後被棒球K到，它又得到了負面回報， 所以它開始加上跳躍來閃避傷害，最後最後，破關方式就被電腦試了出來。 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt;  Application (RL) 一般人聽到RL，可能只想到Alpha Go，或是使用在遊戲上面，但其實RL現在已經開始有很多的應用了。
=&amp;gt;對話系統：有些已經把RL用在對話系統上，利用互動式學習，隨著時間不斷的提升對話系統
=&amp;gt;醫療：利用RL來尋找最佳的治療方案
=&amp;gt;Google auto ML： 使用RL來為計算機視覺和語言建模生成神經網路架構</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-segmentation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-segmentation/readme/</guid>
      <description>Category  語意的部份就是全部的氣球,都是同一種 &amp;lt;/td&amp;gt;   =&amp;gt; 找到7個objects &amp;lt;/td&amp;gt;   =&amp;gt; instance segmentation,和object detection差異是 除了找到7個objects, 還有找到這些objects對應的pixels(MaskRCNN就是屬於這一種) &amp;lt;/td&amp;gt;  Comparison  Encoder交替采用 (1)conv + (2)pooling， Decoder交替采用 (1)deconv+ (2)upsampling，用 (3)Softmax做像素分类。 在Encoder-Decoder过程中， 采用Pooling Indices（pooling时的位置信息）转移 Decoder &amp;lt;/td&amp;gt;   U-Net没有利用池化位置索引信息， 而是将编码阶段的整个特征图传输到相应的解码器 （以牺牲更多内存为代价） =&amp;gt; short-cut &amp;lt;/td&amp;gt;   然後再透過CFF(feature fusion)將特徵合為一起,然後再做upsample, 即可變回原圖大小 with semantic segmentation 比SegNet 大大地提高了准确率，足以与Deeplab v2媲美 &amp;lt;/td&amp;gt;   encode端： deeplabv3用&amp;quot;原圖&amp;quot;搭配不同的dilation conv來擷取特徵 =&amp;gt; 所以圖不會變小 encode端： U-Net透過不同的conv,不斷做down sample, 得到最後的encode結果,但decode的時候,會用之前的資料 &amp;lt;/td&amp;gt;   deeplabv3 是直接upsample回目標大小 (DeepLabv3直接用encode到最後的值 (不同dilation conv)直接upsample deeplabv3+ =&amp;gt; 現在則是分兩段upsample，第一段upsample完之後， =&amp;gt; 與encoder中feature map大小一樣的low level feature組合起來 (所以會做個merge)， =&amp;gt; 再upsample到目標大小 (v3+ decode則是會和1x1 conv組合&amp;quot;和feature map一樣大小&amp;quot;) =&amp;gt; 所以v3+是加強了decode的部份 &amp;lt;/td&amp;gt;   在实现目标检测的同时，把目标像素分割出来 =&amp;gt;所以可以比SSD這種類型,再多做一點 和Faster - RCNN 差異 1）将 Roi Pooling 层替换成了 RoiAlign； 2）添加并列的 FCN 层（mask 层）；=&amp;gt;所以叫作&amp;quot;Mask&amp;quot;RCNN FPN 在 MaskRCNN.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-self_supervised_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-self_supervised_learning/readme/</guid>
      <description>supervised learning (limitation: label) 自監督學習能突破現有深度學習方法的侷限。
比如說，最廣泛使用的監督式學習（supervised learning），
是由人給定標記好的資料，讓機器學習正確答案並作為推論根據。
但是，這種學習方法是立基於人的標記，不僅資料標記過程需要花費大量時間與資源，
機器也只能根據已標記的特徵來學習，完成指定的任務，如語音轉文字、分類圖像、物件辨識等。
reinforced learning (limitation: lots of trials) 強化學習（Reinforced Learning），是透過獎勵與懲罰的機制，
讓機器在虛擬情境中不斷試錯（trial and error），累積經驗來學習。
這種學習方式雖然在競技比賽裡表現良好、甚至能勝過人類，但學習效率極低。
舉例來說，人類在15分鐘內能領略的任一款Atari遊戲，機器卻平均要花83小時才能學會，
在臉書研發的虛擬圍棋遊戲ELF OpenGo中，更要用2000個GPU訓練14天，
更別提要訓練200年才學得會的星海爭霸遊戲（StarCraft）。
而且，強化學習並不能永遠在虛擬場景訓練，一旦進到真實世界，
所有試錯的過程將會帶來高成本的代價。比如說，在自駕車了解前面是懸崖要轉彎之前，
可能需要先掉下去幾百次，且不同於虛擬世界可以無間斷的循環訓練，
在真實世界中花費的訓練時間只會更長；更何況，人類學習過程只需極少數「試錯」的過程，
比如在看到前方的懸崖之後，常識就會使我們轉
self-supervised learning 自監督學習能解決這個問題。比起強化學習是從試錯的經驗中學習，
自監督學習是建構一個龐大的神經網絡，透過預測來認識世界。
換句話說，自監督學習所訓練的模型，能藉由觀察過去、當下所有的訓練資料，來預測下一刻會發生的事情 因此，在預測到車子將會摔落懸崖時，就能提前轉彎來避免。「就像人類是不斷透過已知的部分來預測未知，
看到一半的人臉會自動在腦海補足另一半畫面，所以自監督學習是更接近人類學習行為的方法。」
Facebook Yann LeCun
認為人腦是透過觀察來學習的
所以AI也是要透過類似的方式
Self-supervised learning可以做到的是由input來觀察,併進行自己思考,
像是BERT順練的方式,把input句子中的某一塊蓋掉,再訓練自己猜這個蓋掉的詞是什麼 =&amp;gt; 經驗學習
然後在inference的時候就可以input完整句子,併產生句子後接的詞 =&amp;gt; 預測未來</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-sparsely_gated_mixture-of-experts_layer/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-sparsely_gated_mixture-of-experts_layer/readme/</guid>
      <description>Concept &amp;ldquo;1370億個參數&amp;rdquo; 谷歌大脑的这项最新研究作者包括 Geoffrey Hinton 和 Jeff Dean，
论文提出了一个超大规模的神经网络——稀疏门控混合专家层（Sparsely-Gated Mixture-of-Experts layer，MoE）。
MoE 包含上万个子网络，每个网络的参数更是高达 1370 亿个之多
=&amp;gt; 超大型網路
通过灵活控制部分网络，新的技术在大规模语言建模和机器翻译基准测试中，
花费很小的计算力实现了性能的显著提升。这项工作是深度网络条件计算在产业实践中的首次成功，
有助于推广神经网络以及新应用的产生
conditional computation 神经网络吸收信息的能力受其参数数量的限制。有人在理论上提出了条件计算（conditional computation）的概念，
作为大幅提升模型容量而不会大幅增加计算力需求的一种方法。
在条件计算中，部分网络的活动以样本数量为基础（active on a per-example basis）。
然而在实践中，要实现条件计算，在算法和性能方面还存在很大的挑战。
在本次研究中，我们针对这些问题并最终在实践中发挥出条件计算的潜力，
在模型容量上得到超过 1000 倍的提升，同时让现代 GPU 集群的计算效率仅发生了微小的损失。
我们提出了一个稀疏门控混合专家层（Sparsely-Gated Mixture-of-Experts layer，MoE），
 (1) 由多达数千个前馈子网络组成 =&amp;gt; 很多網路 (2) 可训练的门控网络会决定这些专家层（expert）的稀疏组合，并将其用于每个样本 =&amp;gt; 一些控制邏輯(可訓練)被當作專家
我们将 MoE 应用于语言建模和机器翻译任务，在这些任务中模型性能（model capacity）对于吸收训练语料库中可用的大量知识至关重要。
我们提出的模型架构中，高达 1370 亿个参数被卷积地应用于堆叠的 LSTM 层当中。
在大型语言建模和机器翻译基准测试中，这些模型以更低的计算成本获了得比现有最好技术更好的结果。  利用训练数据和模型大小的规模是深度学习成功的关键。
当数据集足够大时，增加神经网络的容量（参数数量）可以得到更高的预测精度
这已在一系列研究领域的工作中得到证实，包括文本，图像，音频等领域。
对典型的深度学习模型，其中整个模型被激活用于每个示例，由于模型大小和训练样本的数量增加，
导致训练成本几乎二次方级地增加。但是计算力和分布式计算的进步不能满足这种需求。
为了提升模型能力，同时不会成比例地增加计算成本，
已经有前人研究提出了各种形式的条件计算（conditional computation）。
在这些设计中，网络的大部分在每个示例的基点上（on a per-example basis）可以是活动的（active）或者非活动的（inactive）。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-speech/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-speech/readme/</guid>
      <description>Speech- Speech應用也是屬於NLP(自然語言處理)其中一部份
https://github.com/evansin100/NLP/blob/master/README.md
Trend Unsuper-vised =&amp;gt; fine-tune for specific task 近年 NLP 界十分流行的兩階段遷移學習會先蒐集大量文本（無需任何標注數據）， 並以無監督的方式訓練一個通用 NLP 模型，
接著再微調（Fine-tune）該模型以符合特定任務的需求。
常見的 NLP 任務有文章分類、自然語言推論、問答以及閱讀理解等等。
這個包含GPT and BERT都是這樣做的
Comparison  (1) Multilyaer perception (2) 是一種分類的網路架構(和conv不同), (3) 中間可以有很多的hidden layers (4) 可以設計for M input and N output (5) 但不像是RNN有time stamp概念(了解順序),所以比較弱   (1) LSTM and RNN cell只是單純把數值不斷帶下去, (2) shape={batchsize,timestamp} batchsize等於一個cell的input timestamp等於有幾個cell(RNN block) 就是做一個判斷會需要經過多少個RNN block (3) https://github.com/evansin100/Keras/blob/ master/Example/addition_rnn/README.md 這邊有很完整的範例 (4) LSTM 是shared weight(因為是recurrent架構) (5) LSTM 的weight計算(因為是MLP,所以weight很大) a. 假設 num_units 是128 1,2,4 sigmoid and 3 tanh, 他們就分別有128個neuros =&amp;gt; neuron total 個數 = 128 x 4 b.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-stacked_capsule_autoencoders/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-stacked_capsule_autoencoders/readme/</guid>
      <description>Motivation 从2017年开始， Hinton等人研究的Capsule Network得到了深度学习社区的大量关注。
可以说Capsule Network在反思CNN的一些固有偏见，
比如CNN的学习过分强调不变性（invariant ）特征的学习
* 池化可以帮助输入近似不变（invariant），当输入发生少量偏移，输出的结果并不会改变
* =&amp;gt; 因為max pool假設說kernel size夠大,依然可以選到區域中比較大的值, 這樣就可以達到輸出不變 数据增强也服务于这一目的。而这样做，实际上，忽略了一个真实世界中的事实
 (1) 物体-部件 关系（Object-Part-relationship)  是视角不变的（viewpoint invariant）   (2) 物体-观察者（Object-Viewer-relationship）  是视角同变性（viewpoint equivariant）的
并且Capsule强调物体的存在是因为当部件以合理的关系组合才得以存在，所以进一步引出了routing的机制，来发掘part-whole关系。    这里想说的是: 如果有人问，当前深度学习的核心理念是什么，
我个人觉得，一个比较好的回答就是，学习目标形式化进而转换为参数的梯度学习
然而这样的同质研究越来越多，又越来越难以深入其内部，DL社区就开始自我反思。
而Capsule的理念里面，就尝试去摆脱D中L对梯度回传的过分依赖，对卷积结构的过分依赖
所以Capsule Network本身将一些自编码器、重构、混合高斯、注意力等机制引入其中。
其实读这篇论文会感觉作者用到的技术太多了，很容易忽略它背后的动机。我个人对它背后动机的理解为: 将图像中的实例的部件及属性从像素二维空间中以像素重建的方式抽取出出来；
再用重构的方式解释部件与整体的关系。这也是SACE的两个主要构成环
Concept 该胶囊网络全名 Stacked Capsule Autoencoder (SCAE)，具体可分为两个阶段：
该胶囊网络可以无监督地学习图像中的特征
 (1) Part Capsule Autoencoder (PCAE）  PCAE 负责将&amp;quot;图像分割成组件&amp;rdquo;，借此推断其姿势，并将图像像素重构为转换后的部件模板像素的混合产物
   (2) Object Capsule Autoencoder (OCAE)  OCAE 则试图将发现的部件及其姿势组成更小的一组对象，再结合针对每个部件的混合预测方案来解释部件的姿势。 每个物体胶囊通过将姿势-对象-视图-关系（OV）乘以相关的物体-部件-关系 (OP) 来为这些混合物提供组件。SCAE</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-structural_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-structural_learning/readme/</guid>
      <description>Concept https://www.tensorflow.org/neural_structured_learning Neural Structured Learning (NSL) is a new learning paradigm to train neural networks by leveraging structured signals in addition to feature inputs. Structure can be explicit as represented by a graph or implicit as induced by adversarial perturbation
Structured signals are commonly used to represent relations or similarity among samples that may be labeled or unlabeled. Therefore, leveraging these signals during neural network training harnesses both labeled and unlabeled data, which can improve model accuracy, particularly when the amount of labeled data is relatively small.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-super-resolution/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-super-resolution/readme/</guid>
      <description>Preliminary super resolution相關的model
和denoise (see in the dark - CAN or U-Net)
因為都是處理per pixel, 所以是類似的應用, 原理也都很像 https://github.com/evansin100/Denoise
History VDSR是在2016
Concept 超解析度技術（Super-Resolution）是指從觀測到的&amp;quot;低解析度影象&amp;quot;重建出相應的&amp;quot;高解析度影象&amp;rdquo; =&amp;gt; 所以user的感覺是給一張小圖(low resolution)會輸出一張大圖(high resolution),點變多了 在監控裝置、衛星影象和醫學影像等領域都有重要的應用價值。
SR可分為兩類:
 從&amp;quot;單張&amp;quot;低解析度影象重建出高解析度影象, 即Single Image Super-Resolution (SISR) SISR是一個逆問題，對於一個低解析度影象，可能存在許多不同的高解析度影象與之對應， 因此通常在求解高解析度影象時會加一個先驗資訊進行規範化約束。 在傳統的方法中，這個先驗資訊可以通過若干成對出現的低-高解析度影象的例項中學到。 而基於深度學習的SR通過神經網路直接學習解析度影象到高解析度影象的端到端的對映函式 較新的基於深度學習的SR方法，包括SRCNN，DRCN, ESPCN，VESPCN和SRGAN等 &amp;lt;/td&amp;gt;  Input 這邊可以看到各種model
他們的input image的設定
e.g., SRCNN: input 300x300, VGG19 192x192, SRGAN 512x512 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-transfer_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-transfer_learning/readme/</guid>
      <description>https://www.jianshu.com/p/bdb87070b780
Transfer_Learning e.g. 使用已經訓練好的model,再來新增新的分類
&amp;lsquo;我们需要重新训练顶层top layer来识别新的分类，这个函数将向graph添加一些操作，随着一些变量保存权重，然后为所有反向传播设置梯度变化。&amp;rsquo;
Tensorflow github https://github.com/tensorflow/hub
TensorFlow Hub is a library to foster the publication, discovery,
and consumption of &amp;ldquo;reusable parts&amp;rdquo; of machine learning models.
In particular, it provides modules, which are pre-trained pieces of TensorFlow models
that can be reused on new tasks.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-unsupervised-learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-unsupervised-learning/readme/</guid>
      <description>unsupervised-learning 明察秋毫、善於分類的女兒：非監督式學習 Unsupervised learning 喜歡將看到的物件依照結構分門別類、劃分成不同群組的它，當你將一箱動物玩具放在它面前時，
它很快就能夠從中分出不同的小圈子，讓你感到驚訝的是，你事先並沒有告訴它每隻動物是哪種分類，
它卻可以透過觀察分成有翅膀能夠飛的、能在水中生活的或只能在路上爬的動物，
有些甚至有它獨特的分類方法是你沒想到的，
這就是非監督式學習，你不必透過監督，就從玩具中透過觀察解析結構將資訊做好分類。
=&amp;gt; 所以重點是不用知道種類的意義,但可以透過學習知道怎麼做分類
非監督式學習可以只需要無標記( unlabeled)資料，便能實作。
換成現實中的例子，想到的是消費者喜好分析，一般在分類不同消費群組時，我們習慣根據性別、年紀等做分群，
但假如今天你發現有個會員會在白天購買化妝品、傍晚購買啤酒、晚上買電動，而且這族群數量還不算少，
若只看性別和年紀應該會滿頭霧水，覺得這個消費者是不是得了精神分裂症。
而這組會員其實就是家庭，全家爸爸、媽媽、小孩共用同一個帳號網購累積紅利，
若套用非監督式學習去分析消費者的行為，便有機會篩檢出這麼樣有相同屬性的客群。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-video_ai/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-video_ai/readme/</guid>
      <description>Concept Medical images like MRIs, CTs (3D images) are very similar to videos -
both of them encode 2D spatial information over a 3rd dimension.
Much like diagnosing abnormalities from 3D images, action recognition from videos would require capturing context
from entire video rather than just capturing information from each frame
Challenges Action recognition task involves the identification of different actions
from video clips (a sequence of 2D frames) where the action may or may not be</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-zero-shot-learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-zero-shot-learning/readme/</guid>
      <description>Concept 零樣本學習 Zero-Shot Learning 演算法
在訓練各種機器學習模型或是類神經網絡模型時，都需要倚靠大量的資料褲來協助訓練，
然而，並不可能對「所有」不同領域或是情況下的資料都蒐集完之後才來做模型的訓練。
為了讓模型可以更加 Generalize、同時因應各種不同領域的資料庫，
因而產生了 Zero-Shot Learning 的演算法。
而 Zero-Shot Learning 演算法的概念其實就是在模仿人類學習的方式， 人類在學習的時候其實會以概念的形式去做類推
Example ZSL就是希望能够模仿人类的这个推理过程，使得计算机具有识别新事物的能力
類如下面的例子
沒有拿斑馬的圖訓練過(也就是&amp;quot;0樣本&amp;rdquo;,沒有拿斑馬的圖) 只有拿馬 老虎 熊貓等等的圖順練 但有提到斑馬可能有的特徵
如果辨識到一個斑馬的圖 他就會知道這個是斑馬
Approaches 常用演算法介紹
各種常用的手法當中，其實都有一些基本的假設和概念存在： (1) 試圖找出共同的分布空間
(2) 對於目標資料庫進行 Adaptation
(3) 將萃取出來的特徵和已知的特徵做連結，進而達到預測未知的種類
(4) 選擇適當的樣本來做訓練
然而要進行以上的實驗都必須要達成一個最重要的前提，
也就是要找到共同的分布空間，否則以上的演算法也不會達到如此的成效
Issue (domain shift problem) 简单来说，就是同一种属性，在不同的类别中，视觉特征的表现可能很大。
如图所示，斑马和猪都有尾巴，因此在它的类别语义表示中，
“有尾巴”这一项都是非0值，但是两者尾巴的视觉特征却相差很远
Issue (semantic gap) 样本的特征往往是视觉特征，比如用深度网络提取到的特征，
而语义表示却是非视觉的，
这直接反应到数据上其实就是：
样本在特征空间中所构成的流型与语义空间中类别构成的流型是不一致的
=&amp;gt; 这使得直接学习两者之间的映射变得困难</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/compiler-concept/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/compiler-concept/readme/</guid>
      <description>Concept 簡言之，編譯器獲取原始碼，產生一個二進位制檔案。
因為從複雜的、人類可讀的程式碼直接轉化成0/1二進位制會很複雜， 所以編譯器在產生可執行程式之前有多個步驟：
Lexer 詞法分析
(1) 输入：一堆字符，即我们写的语法代码，一个文件，也就是一堆字符；
(2) 经过lexer处理；
(3) 输出：一系列的token，相当于一堆的变量或者说符号；
第一步是把輸入一個詞一個詞的拆分開。
這一步被叫做 詞法分析,或者說是分詞。
這一步的關鍵就在於 我們把字元組合成我們需要的單詞、識別符號、符號等等。 詞法分析大多都不需要處理邏輯運算像是算出 2+2 –
其實這個表示式只有三種 標記：一個數字：2,一個加號，另外一個數字：2
Parser(output AST) 解析器確實是語法解析的核心。
解析器提取由詞法分析器產生的標記，並嘗試判斷它們是否符合特定的模式，
然後把這些模式與函式呼叫，變數呼叫，數學運算之類的表示式關聯起來。 解析器逐詞地定義程式語言的語法
你可以寫好幾種不同型別的解析器。
最常見的解析器之一是從上到下的，遞迴降解的解析器。
遞迴降解的解析器是用起來最簡單也是最容易理解的解析器
解析器在解析時產生的樹狀結構被稱為 抽象的語法樹，或者稱之為 AST。
 AST 的全文是 Abstract Syntax Tree，中文大多翻作抽象語法樹， 主要是將我們 人類 所寫的程式語法，轉換成 程式 比較容易閱讀的語法結構，並以樹的資料結構來儲存
ast 中包含了所有要進行操作。解析器不會計算這些操作，它只是以正確的順序來收集其中的標記。
我之前補充了我們的詞法分析器程式碼，以便它與我們的語法想匹配，並且可以產生像圖表一樣的 AST  Optional: AST =&amp;gt; more IR 編譯器可能有 中間表示,或者簡稱 IR 。 IR 主要是為了在優化或者翻譯成另一門語言的時候，無損地表示原來的指令。
IR 不再是原來的程式碼；IR 是為了尋找程式碼中潛在的優化而進行的無損簡化。
迴圈展開 和 向量化 都是利用 IR 完成的 =&amp;gt; IR可以做更低階的優化</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/compiler-framework-glow/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/compiler-framework-glow/readme/</guid>
      <description>Introduction Glow is a machine learning compiler and execution engine for hardware accelerators.(nji It is designed to be used as a backend for high-level machine learning frameworks.
The compiler is designed to allow state of the art compiler optimizations
and code generation of neural network graphs. This library is in active development.
Glow是一個機器學習編譯器，可以用來加速不同硬體平臺上深度學習框架的效能，
同時也能讓硬體開發人員，更專注建構可支援PyTorch等深度學習框架的硬體加速器
所以目的也是為了推廣他們的pytorch
Note: 在PyTorch中，图架构是动态的，这意味着图是在运行时创建的。
而在TensorFlow中，图架构是静态的，这意味着先编译出图然后再运行
Note: PyTorch更适合于在研究中快速进行原型设计、业余爱好者和小型项目，
TensorFlow则更适合大规模的调度，尤其当考虑到跨平台和嵌入式调度操作时
IR Summary 所以是兩階段的IR - High and Low (low是instruction based的,可以用來優化memory相關的操作,還有指令級的排序), 最後才是machine code Glow lowers a traditional neural network dataflow graph into a two-phase strongly-typed intermediate representation (IR).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/compiler-framework-llvm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/compiler-framework-llvm/readme/</guid>
      <description>LLVM introduction (1) LLVM提供了完整編譯系統的中間層，
它會將中間語言（Intermediate Representation，IR）從編譯器取出與最佳化， 最佳化後的IR接著被轉換及鏈結到目標平台的汇编语言。
LLVM可以接受來自GCC工具鏈所編譯的IR，包含它底下現存的編譯器。 =&amp;gt; 所以代表LLVM可以當作是gcc的backend
(2) LLVM也可以在編譯時期、鏈結時期，甚至是執行時期產生可重新定位的程式碼（Relocatable Code）。 (3) LLVM支援與語言無關的指令集架構及類型系統[7]。
每個在静态单赋值形式（SSA）的指令集代表著 每個變數（被稱為具有型別的暫存器）僅被賦值一次，這簡化了變數間相依性的分析
(4) LLVM允許程式碼被靜態的編譯，
包含在傳統的GCC系統底下，或是類似JAVA等後期編譯才將IF編譯成機器碼所使用的即時編譯（JIT）技術。
它的型別系統包含基本型別（整數或是浮点数）及五個複合型別（指標、数组、向量、結構及函數），
在LLVM具體語言的型別建制可以以結合基本型別來表示，
舉例來說，C++所使用的class可以被表示為結構、函式及函数指针的陣列所組成。
(5) LLVM JIT編譯器可以最佳化在執行時期時程式所不需要的靜態分支，
這在一些部份求值（Partial Evaluation）的案例中相當有效，即當程式有許多選項，
而在特定環境下其中多數可被判斷為是不需要。這個特色被使用在Mac OS X Leopard（v10.5）底下OpenGL的管線，
當硬體不支援某個功能時依然可以被成功地運作[8]。OpenGL堆栈下的繪圖程式被編譯為IR，
接著在機器上執行時被編譯，當系統擁有高階GPU時，
這段程式會進行極少的修改並將傳遞指令給GPU，當系統擁有低階的GPU時，LLVM將會編譯更多的程序，
使這段GPU無法執行的指令在本地端的中央处理器執行。
LLVM增進了使用Intel GMA晶片等低端機器的效能。一個類似的系統發展於Gallium3D LLVMpipe，
它已被合併到GNOME，使其可運行在沒有GPU的環境
LLVM flow briefing 實驗了frontend=&amp;gt;optimizer=&amp;gt;backend的flow
LLVM 定義了一個通用的程式中介表示法，LLVM IR。LLVM IR 是一種類似機器語言，
但為了通用性以及給編譯器設計者方便而簡化的版本。在 LLVM 的世界裡，大家都講 LLVM IR：
Frontend 把原始語言的邏輯翻譯成 LLVM IR、
Optimizer 把 LLVM IR 整理成效率更好的 LLVM IR、
Backend 拿到 LLVM IR 來生成機器目標平台的機器語言。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/compiler-framework-mlir/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/compiler-framework-mlir/readme/</guid>
      <description>problem TensorFlow 生态系统包含许多编译器和优化器，可在多个级别的软硬件堆栈上运行。
作为 TensorFlow 的日常用户，在使用不同种类的硬件（GPU、TPU、移动设备）时，
这种多级别堆栈可能会表现出令人费解的编译器和运行时错误
所以可以從下圖看到,因為接的硬體不同,過程中經過的opt等等也都不同
如图中所示，TensorFlow 图 [1]能够以多种不同的方式运行。这包括：
(1) 将其发送至调用手写运算内核的 TensorFlow 执行器
(2) 将图转化为 XLA 高级优化器 (XLA HLO) 表示，反之，这种表示亦可调用适合 CPU 或 GPU 的 LLVM 编辑器，
或者继续使用适合 TPU 的 XLA。（或者将二者结合！）
(3) 将图转化为 TensorRT、nGraph 或另一种适合特定硬件指令集的编译器格式
(4) 将图转化为 TensorFlow Lite 格式，然后在 TensorFlow Lite 运行时内部执行此图，
或者通过 Android 神经网络 API (NNAPI) 或相关技术将其进一步转化，以在 GPU 或 DSP 上运行
虽然这些编译器和表示的大量实现可显著提升性能，但这种异构的环境可能会给最终用户带来问题，
例如在这些系统间的边界处产生令人困惑的错误消息。
此外，若需要构建新的软硬件堆栈生成器，则必须为每个新路径重新构建优化与转换传递=&amp;gt;代表opt不能reuse MLIR (Multi-Level Intermediate Representation Overview) MLIR（或称为多级别中介码）。这是一种表示格式和编译器实用工具库，
介于模型表示和低级编译器/执行器（二者皆可生成硬件特定代码）之间
MLIR 深受LLVM的影响，并不折不扣地重用其许多优秀理念。MLIR 拥有灵活的类型系统， 可在同一编译单元中表示、分析和转换结合多层抽象的图。
这些抽象包括 TensorFlow 运算、嵌套的多面循环区域乃至 LLVM 指令和固定的硬件操作及类型</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/compiler-ir-nir/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/compiler-ir-nir/readme/</guid>
      <description>Motivation  Software engineering principle
— break compiler into manageable pieces Simplifies retargeting to new host
— isolates back end from front end Simplifies support for multiple languages
— different languages can share IR and back end Enables machine-independent optimization
— general techniques, multiple passes  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-chiplet/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-chiplet/readme/</guid>
      <description>Concept chiplet 是什麼？
就是小晶片，首先將複雜功能進行分解，然後開發出多種具有單一特定功能，
英文的字尾（suffix）“let”，好比台語中名詞字尾加一陽平聲的「啊」字，
都帶有「小」的意思，像booklet是小書，piglet是小豬，而chiplet自然是小晶片
可相互進行模塊化組裝的「小晶片」（chiplet），
如實現數據存儲、計算、信號處理、數據流管理等功能， 並最終以此為基礎，建立一個「小晶片」的晶片網絡。
晶片網絡是什麼？
未來的電腦系統可能只包含一個CPU晶片(chiplet)和幾個GPU，
這些GPU都連接到這個chiplet晶片上，形成晶片網絡。 Example
Motivation 做chiplet的動機很簡單，是要在逐漸趨緩摩爾定律的大環境下，
持續提升產品的性能和價值。如果為了整合新功能模組入晶片而加大晶片面積，
於最先進製程上製造大晶片是很不划算的。而且晶片面積大了 由缺陷密度導致的良率損失也跟著增長 Chanllenges 發展Chiplet會遇到哪些挑戰？
既然實現Chiplets技術還存在比較多的技術難點，那麼下面具體了解下發展Chiplet會遇到以下幾方面的挑戰：
 (1) 首先當然是集成技術的挑戰。  Chiplet模式的基礎還是先進的封裝技術， 必須能夠做到低成本和高可靠性。此外，集成技術的挑戰還來自集成標準   (2 )互聯標準  首先，設計這樣一個異構集成系統需要統一的標準， 即die-to-die數據互聯標準。而且裸晶片到裸晶片的互連方案很昂貴   (3) 封裝技術  將多個模塊晶片集成在一個SiP中需要高密度的內部互連線。 可能的方案有矽interposers技術、矽橋技術和高密度Fan-Out技術， 不論採取那種技術，互連線（微凸）尺寸都將變得更小， 這要求互連線做到100%的無缺陷。因為互聯缺陷可能導致整個SiP晶片不工作。   (4) 測試技術  作為一個複雜的異構集成系統，保證SiPs晶片功能正常比SoC更困難。 SoC晶片通常需要採購IP，而目前關於IP的重用方法中，IP的測試和驗證已經很成熟，可以保證IP接入系統沒有問題。 採用Chiplet模式的SiPs晶片則不同，它採購或使用的是製造好的die，即模塊晶片。 這對單個die的良率要求非常高，因為在SiPs中一個die的功能影響了整體性能，一旦出了問題損失巨大。 同時在die設計中還需要植入滿足SiPs晶片的測試協議。而對於SiPs晶片， 由於管腳有限，如何單獨測試每個die的性能和整體SiP的性能也是一個難點。   (5) 開發工具  互聯、封裝和測試需要軟體工具的支持，對於EDA工具帶來巨大的需求。 例如在晶片設計中，30%-40%的成本是工具軟體。 DARPA的 CHIPS項目中一個工作重點就是設計工具。 Chiplet技術需要EDA工具從架構探索，到晶片實現，甚至到物理設計提供全面支持。   (6) 晶片網絡的交通死鎖與流量堵塞  儘管每個chiplet的晶片上routing system都可以很好地工作， 但是當它們全部連接在內插器的網絡上時，就出現了交通死鎖與流量堵塞問題。    </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-cim/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-cim/readme/</guid>
      <description>Concept ISSCC 2018，眾所矚目的焦點中，不可忽視的壓軸大戲——Machine Learning in Memory session
Machine Learning為什麼要CIM？ 簡而言之，就是邏輯計算的並行度不斷擴張時，
所需的資料傳輸頻寬（從儲存器到計算單元）的大小限制了計算速度，史稱馮諾依曼瓶頸。
並且，隨著摩爾定律的發展，計算單元的功耗越來越低，
而與之對應的儲存器讀寫功耗不斷上升，導致AI算力的功耗瓶頸。
所謂CIM（儲存器內的計算）就是為了克服這一馮諾依曼瓶頸，
即只要將資料扔到儲存器中，再讀出時的資料為計算完成時的結果
注意，此處的CIM和CS領域的in-memory computing是兩個概念。
（劃重點）CIM令人嚮往的潛力主要包括—— （1）資料傳輸頻寬需求下降， （2）用於資料傳輸的功耗下降 Example (DNN CIM) Architecture of binary-in ternary-weight RRAM in-memory computing macro </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-delta-sigma/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-delta-sigma/readme/</guid>
      <description>Concept Delta-Sigma（ΔΣ）調變（或稱Sigma-Delta（ΣΔ）調變、SDM，中文譯作積分-微分調變）
是一種數位類比互相轉換的實做方法， 它是把高位元解析度低頻率信号用,脉冲密度调制编码为, 低位元解析度高頻率信号的一种方法（PCM轉PWM），
可以將量化失真移往更高頻率、減少濾除時對目標頻率的影響，
推導自delta調變原理的類比至數位或是數位至類比轉換技術。ADC或是DAC可藉由低成本的CMOS製程實現此一技術，
也就是像數位IC一樣的製程。基於上述理由，即便本技術早在1960年代已經提出， 但是要到近年來由於半導體技術精進才得以普遍的使用。幾乎所有的類比IC製造商都有提供Sigma-Delta轉換器產品
ADC可被认为是一个压控震荡器，控制电压为被测量的电压，线性和比例性由负迴授决定。
振荡器输出为一个脉冲串，每个脉冲为已知，常量，幅度=V且持续时间为dt，
因此有一个已知的积分=Vdt但是变化的分离间隔。脉冲的间隔由迴授電路决定，
所以一个低输入电压产出一个脉冲间的长间隔；而一个高输入电压产生一个短间隔。
实际上，忽略开关错误，脉冲间的间隔与该间隔内输入电压的平均成反比，
因此在该间隔ts内，是一个平均输入电压的样本，与v/ts成正比。
最终的输出数是输入电压（该电压由脉冲计数决定）的数字化在一个固定加总间隔=Ndt产出一个计数，
Σ。脉冲串的积分为ΣVdt其在时间间隔Ndt内被生成，
因此输入电压在加总周期内的平均为VΣ/N，而且是平均的平均所以只遭受很小的变化。
达成的精度取决于已知V的精度和一个计数内N的精度及分辨率。
上面描述的脉冲可被认为是迪拉克方程的形式化分析，计数可被认为Σ。
在ADC的转化中，正是这些脉冲串被delta-sigma调制所传递。 ΔΣ架構主要是在對訊號的大小做一個粗略的估計，然後量測其誤差，
將其積分並補償之，最後輸出的平均值會等於輸入訊號的平均值（若誤差的積分為有限值）。
積分器的數量決定了ΔΣ調變電路的階數（Order），圖2中所示為二階ΔΣ電路；
階數越高時，noise shaping效果越好，但相對付出代價是穩定度必須妥善考量。
ΔΣ調變電路也可以用量化器的輸出位元數來分類，當使用N階的比較器時，輸出為log2N-bit；
基於線性度的考量，常見的ΔΣ電路為1-bit組態，也就是輸出僅有兩個位準：0或1。
Noise Shaping 由微分器、積分器構成的ΔΣ調變電路，會因其微分特性而對量化雜訊（Quantization noise）產生一種高通濾波的效果 一般線性PCM中產生的量化雜訊平均分布在各頻率上，基於前述特性，可以將量化雜訊推往高頻，而產生noise shaping功效 將取樣頻率設高，則人耳可聽到的頻段相對低頻，此時將已經被推往高頻的量化雜訊以低通濾波器濾除，則可以得到量化雜訊較少的原訊號。
當ΔΣ調變階數越多時，noise shaping效果也會越顯著，如圖所示為1~3階ΔΣ的noise shaping效果。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-general-hw-architecure/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-general-hw-architecure/readme/</guid>
      <description>General-HW-Architecure </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-hls_high_level_synthesis/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-hls_high_level_synthesis/readme/</guid>
      <description>Concept   高级综合（High-level Synthesis，縮寫 HLS），
 又譯高层次综合，另又稱C合成（C synthesis）、 電子系統層次合成（Electronic System Level synthesis，縮寫 ESL synthesis）， 是将电路设计规范的算法级或行为级描述在一定的约束条件下转化为电路结构描述的方法和过程。 =&amp;gt; 所以就是用C來寫硬體(高階)    高层次综合又称为行为级综合、算法级综合等。
 它使设计者能够在更高层次进行电子设计，更快速有效地在较高层次设计验证和仿真， 而较低层次的工作由工具来自动完成， 从而让数字电路系统设计工程师可以有更多的精力和更充分的条件去进行设计空间的搜索，寻求最佳的设计方案。    HLS 的过程通常
 (1) 基本包括预处理、编译、转换、调度、分配、控制器、综合、RTL 、生成、和反编译等几个部分。 (2) 编译、转换部分决定了软件的兼容性和易用性， (3) 调度（schedule）和分配（binding）主要决定了产生的 RTL 的性能、资源大小等。    History 高层次综合技术的发展经历了三个阶段。
尽管早在20世纪80年代初就开始了关于高层次综合技术的研究，并且在90年代推出了一些商业工具，
高层次综合设计方法直到最近两三年才取得了一定的成功，得到了集成电路设计公司更多的接受和应用。
  (phase 1) 第一阶段从80年代初到90年代初，
 关于 HLS 的研究主要集中在学术科研机构，他们完成了高层次综合的早期的许多研究工作， 包括许多关于 HLS 的基本的概念和技术。当时集成电路设计自动化系统开始了迅猛的发展， 但限于当时的技术水平和 IC 工业界提出的需求，多数研究努力都停留在较低的设计层次上。 因此高层次综合技术被认为是学院派的探索，主要限于在大学和一些基础性研究所中进行学术研究。    (phase 2) 第二阶段从90年代初到2000年初，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-power/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-power/readme/</guid>
      <description>Summary  電壓 (V)：單位帶電粒子所擁有的電能量。  標準單位是 V (伏)，1 J / 1 C = 1 V。   電功率 (P)：單位時間內提供或消耗的電能量,  標準單位是 W (瓦)，1 J / 1 s = 1 W。 所以可以再用來x時間,就可以算出最後的電能量   電能量：就&amp;hellip;電能量。  標準單位是 J (焦耳)。     Formula 瓦特-秒 = 焦耳 = V x I x t V 電壓, I 電流(ampere), t 時間 設某個烤箱功率1000瓦特，電壓100伏特(V)那麼它通過的電流為10安培(A) 反過來說100伏特的電壓通過10安培的電流，可以產生1000瓦特的功率。 若電壓為200伏特，功率不變(1000瓦)，則只需要5安培的電流
若電壓為200伏特，電流不變(10安培)則可以產生2000瓦特的功率。
電學公式, 直流或交流單相
P功率單位瓦特=V電壓 * I電流
歐姆定律=&amp;raquo;I電流=V電壓 / R電阻 ( 此公式帶入功率的公式)</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-simd_simt/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-simd_simt/readme/</guid>
      <description>Summary  (1) CPU就是SIMD(NEON instruction) &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; SIMT and SIMD &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; GPU (1) 一個execution engine/cluster內的threads可以執行同個OpenCL kernel指令 =&amp;gt; SIMT (3) 每個thread可以發一個FMA依然可以處理多個資料指令 =&amp;gt; SIMD &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; VLIW and SIMD &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; DSP (1) 通常DSP都是VLIW processor,一個instruction,可以有多筆的operation(slot) =&amp;gt; VLIW (2) 然後處理有個slot可以處理vector指令(SIMD) =&amp;gt; SIMD &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt;  SIMT Concept SIMT就是GPU,有multiple &amp;ldquo;thread&amp;rdquo;
SIMD就是一般的vector運算,還是只有一個thread但可以處理多筆data
SIMT中文译为单指令多线程，英文全称为Single Instruction Multiple Threads
GPU中的SIMT体系结构相对于CPU的SIMD中的概念。为了有效地管理和执行多个单线程，多处理器采用了SIMT架构。
不同于CPU中通过SIMD（单指令多数据）来处理矢量数据；
GPU则使用SIMT，SIMT的好处是无需开发者费力把数据凑成合适的矢量长度，
并且SIMT允许每个线程有不同的分支。 纯粹使用SIMD不能并行的执行有条件跳转的函数，
很显然条件跳转会根据输入数据不同在不同的线程中有不同表现， 这个只有利用SIMT才能做到。
SIMT/SIMD Comparison 不同於CPU中通過SIMD（單指令多資料）來處理向量資料；GPU則使用SIMT，
SIMT的好處是無需開發者費力把資料湊成合適的向量長度 =&amp;gt; 這個是優點，並且SIMT允許每個執行緒有不同的分支。 純粹使用SIMD不能並行的執行有條件跳轉的函式，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-info-systolic-array/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-info-systolic-array/readme/</guid>
      <description>Concept   Google TPU让“脉动阵列”（systolic&amp;quot;心臟收縮的&amp;rdquo; array）这项“古老”的技术又回到大家的视野当中
  (1) Simple and regular design
 简单和规则是脉动阵列的一个重要原则。而这样的设计主要是从“成本”的角度来考虑问题的 由于一个专用系统往往是功能有限的，因此它的成本必须足够低才能克服这一劣势。 而设计一个合理的架构（appropriate architectures）是降低设计成本的一个重要方法。 通过采用脉动阵列这个简单而规则的硬件架构，Google在很短的时间内完成了芯片的设计和实现。 从另一个角度来说，硬件设计相对简单，尽量发挥软件的能力，也是非常适合Google的一种策略    (2) Concurrency and communication
 这一点主要强调并行性和通信的重要    (3) Balancing computation with I/O
 平衡运算和I/O，应该说是脉动阵列最重要的设计目标 Issue  一个处理单元（PE）从存储器（memory）读取数据，进行处理，然后再写回到存储器。 这个系统的最大问题是：数据存取的速度往往大大低于数据处理的速度。 因此，整个系统的处理能力（MOPS，每秒完成的操作）很大程度受限于访存的能力。 这个问题也是多年来计算机体系结构研究的重要课题之一，可以说是推动处理器和存储器设计的一大动力。   Solution  而脉动架构用了一个很简单的方法：让数据尽量在处理单元中多流动一会儿   Example  正如图的下半部分所描述的， 第一个数据首先进入第一个PE，经过处理以后被传递到下一个PE，同时第二个数据进入第一个PE。以此类推， 当第一个数据到达最后一个PE，它已经被处理了多次。 所以，脉动架构实际上是多次重用了输入数据。 因此，它可以在消耗较小的memory带宽的情况下实现较高的运算吞吐率。 当然，脉动架构还有其它一些好处，比如模块化的设计容易扩展，简单和规则的数据和控制流程，
      Application 到这里不难看出，脉动架构是一种很特殊的设计，结构简单，实现成本低。
但它灵活性较差，只适合特定运算。而作者认为，卷积运算是展示脉动架构特点的理想应用，
Summary 总结起来，脉动架构有几个特征</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-ip-cpu/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-ip-cpu/readme/</guid>
      <description>Comparison  (1) 一般CPU vector width=128 or 64 bit(這個是硬體能力 (2) 128bit, 所以可以分 8(8bit) x 16, 16(fp16) x 8, 32(fp32) x 4 (3) 64bit, 所以可以分 8(8bit) x 8, 16(fp16) x 4, 32(fp32) x 2 (4) 注意fp16只有ARMv8.2才有指令集support &amp;lt;/td&amp;gt;   (0) Summary 8bit 硬體加速 =&amp;gt; X fp16 硬體加速 =&amp;gt; X fp32 硬體加速 =&amp;gt; O (FMLA instruction) (1) 支援FMLA for FP32運算, vector width=128 case =&amp;gt; 32(fp32)x4 等於一次處理4個資料 (2) FMA and FMLA 差異 兩個都是用來算floating point 相乘, 但FMA是包裝好的API(intrinsic),所以會被展開instruction　double fma(double a, double b, double c); 而FMLA是insruction是最底層的instruction (3) 沒有針對8bit and fp16 做硬體加速 &amp;lt;/td&amp;gt;   (0) Summary 8bit 硬體加速 =&amp;gt; O (UDOT/SDOT instruction) fp16 硬體加速 =&amp;gt; O (FMLA instruction) fp32 硬體加速 =&amp;gt; O (FMLA instruction) (1) 針對8bit做加速 引進UDOT,SDOT指令集(8bit pack在一起) (2) 新增FMLA for FP16運算, 要加compile option -march=armv8.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-ip-dsp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-ip-dsp/readme/</guid>
      <description>Concept DSP = VLIW and SIMD
 (1) SIMD  e.g., SIMD operation可以支援64-way(lane)(8bit), 就代表一次可以處理64筆資料 一般而言DSP都是用大量的vector能力來提高計算能力 例如, 可以一次宣告input 4x [64個signed char], coeff 4x [64個signed char] 然後只用一個指令, 就可以做到input and coeff相乘,並再把結果相加得到 result 4x [64個24bit] 只是要自己準備vector的data排序,是比較煩瑣點,但或許可以透過compiler來做auto vector   (2) VLIW  通常DSP都是VLIW processor (有多個slot) 所以一個instruction,可以有多筆的operation(slot) (一個operation放在一個slot) e.g., 4筆 vector SIMD operation可以同時發 (DSP會有定義每個slot可以發什麼operation) 因此這樣單位時間就可以處理64x4=256MACs, 所以通常DSP都會給MAC的能力(一筆VLIW最多可以算多少)
    Note VLIW vs issue multiple insturction per cycle 有些DSP會說 不是VLIW processor
但會說可以在同一個cycle發出多筆instruction
這樣其實也是達到VLIW的效果</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-ip-gpu-mali/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-ip-gpu-mali/readme/</guid>
      <description>Concept 重點還是看單位時間可以發幾個指令
所以可以直接看同時間可以發多少FMA即可(這個才是和AI相關的) 不用管shader core的thread這些定義
  GPU架構像是SIMT + SIMD
 (1) SIMT =&amp;gt; 就是同個kernel可以被很多人執行(以thread為單位) (2) SIMD =&amp;gt; 一個vector (FMA) 指令可以處理多筆data Core 至於有幾個Core,則是看Vendor決定要用幾顆e.g.,MC9, Mali G71/G72最多可支持32 Cores    Execution engine/Cluster 有自己專用的
 (1) 數據路徑控制邏輯 (2) 調度程序 (3) 指令緩存 (4) 寄存器文件(register file),所以多個thread要共用 (5) 消息傳遞塊    Warp and GPU Thread
 Warp是scheduling的單位 在Warp內的 每個thread執行同樣的同樣的指令 (SIMT) e.g., G77 最多可以有64個warp, 一個warp大小是16 (16個threads), 所以總共就有1024 threads    底下是用Winograd的(input transform的處理當作例子)
可以看到在input transform處理的時候會用多個threads</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-alibaba-mnn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-alibaba-mnn/readme/</guid>
      <description>Flow Concept (for inference) 摘要：MNN已经用于阿里手机淘宝、手机天猫、优酷等20多个应用之中， 覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等场景。 开源自家轻量级的深度神经网络推理引擎MNN（Mobile Neural Network） 用于在智能手机、IoT设备等端侧加载深度神经网络模型，进行推理预测
Android方面以小米6为例，MobileNet V2上耗费时间约为27毫秒， SqueezeNet V1.1上耗费约为25毫秒，领先业界至少30%；
Functionality MNN的两大功能与四大特点
(1) 模型转换部分帮助开发者兼容不同的训练框架
当前，MNN已经支持Tensorflow(Lite)、Caffe和ONNX，
PyTorch/MXNet的模型可先转为ONNX模型再转到MNN。
而且，也能通过算子融合、算子替代、布局调整等方式优化图
可以看到MNN有自己定義他的model format(.mnn)
(2) 计算推理部分致力于高效完成推理计算
为了更好地完成对模型的加载、计算图的调度，以及各计算设备下的内存分配、Op实现等任务。
他们在MNN中应用了多种优化方案，包括在卷积和反卷积中应用Winograd算法、
在矩阵乘法中应用Strassen算法、低精度计算、多线程优化、内存复用、异构计算等
=&amp;gt; 有vulkan ?
=&amp;gt; 有apple metal ?
=&amp;gt; 從下圖可以看到他的frontend可以接TF/Caffe/ONNX
然後再convert to MNN format
Features (1) 轻量性：针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。
Android platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB
(2) 通用性：支持Tensorflow、Caffe、ONNX等主流模型文件格式，支持CNN、RNN、GAN等常用网络。 Supports 86 Tensorflow ops, 34 Caffe ops; MNN ops: 71 for CPU, 55 for Metal, 29 for OpenCL, and 31 for Vulkan.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-android-aosp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-android-aosp/readme/</guid>
      <description>針對android 原本的AOSP的android code要怎麼download
https://source.android.com/setup/build/downloading
preparation Installing Repo Make sure you have a bin/ directory in your home directory and that it is included in your path:
mkdir ~/bin
PATH=~/bin:$PATH Download the Repo tool and ensure that it is executable:
curl https://storage.googleapis.com/git-repo-downloads/repo &amp;gt; ~/bin/repo
chmod a+x ~/bin/repo
Initializing a Repo client After installing Repo, set up your client to access the Android source repository:會需要填寫好user資訊 mkdir Android-AOSP
cd Android-AOSP
git config &amp;ndash;global user.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-android-nn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-android-nn/readme/</guid>
      <description>Download Android Framework ML git clone https://android.googlesource.com/platform/frameworks/ml
就可以看到ml folder底下有多common/driver/runtime/tools folder
並且ANN 是在frameworks中的ml folder
這些folder分別的意思如下
看起來比較重要的就是runtime folder
./runtime: Implementation of the NN API runtime.
Includes source code and internal header files.
./runtime/include: The header files that an external developer would use.
These will be packaged with the NDK. Includes a
C++ wrapper around the C API to make it easier to use.
./runtime/test: Test files.
./sample_driver: Sample driver that uses the CPU to execute queries.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-arm_compute_library/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-arm_compute_library/readme/</guid>
      <description>Concept ARM compute library有支援三種不同的實作 (1) NEON
(2) OpenCL
(3) GLES_COMPUTE
實作都會被封裝再主要兩個library
(1) libarm_compute-static.a
(2) libarm_compute.so
Graph Graph是后期添加的模块，上面的文档没有进行介绍。其命名空间为arm_compute::Graph。
其中包含了许多与arm_compute同名的类，需要注意区分。 Graph重载了operator&amp;laquo; ，可以用于添加Tensor和Node。
库调用的整体结构为：
Graph &amp;ndash;&amp;gt; Node
Node &amp;ndash;&amp;gt; Function
Function &amp;ndash;&amp;gt; Kernel
Scheduler 程序编译时在SConstruct会根据选项设置宏定义。schedule默认使用的是CPPSchedule。
ndk r16b中clang应该是支持openmp的，可以尝试一下。
=&amp;gt; 所以這個是指CPU的部份,要怎麼用multiple core
CPPScheduler::set_num_threads()可以设置调度程序用来运行内核的线程数。
/** C++11 implementation of a pool of threads to automatically split a kernel&amp;rsquo;s execution among several threads. */
class CPPScheduler : public IScheduler
如果参数num_threads设置为0，则将使用C++ 11支持的最大线程数，否则将使用指定的线程数。
scheduler::get根据Type返回不同类实例。
scheduler::get &amp;ndash;&amp;gt; SingleThreadScheduler::get
scheduler::get &amp;ndash;&amp;gt; CPPScheduler::get</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-caffe2/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-caffe2/readme/</guid>
      <description>Concept Gemfield得承认，“PyTorch的Android编译”应该是“caffe2的Android编译”， 只不过caffe2现在被合并到PyTorch仓库里了，所以这么写。所以本文中， 如果说的是Android上的PyTorch，那么就等价于Android上的caffe2</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-coreml/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-coreml/readme/</guid>
      <description>Concept Core ML 3：支援進階神經網路 WWDC 2019 蘋果介紹了 Core ML 3，這是蘋果機器學習模型框架的最新版本。 Core ML 是能在蘋果產品使用的高效能機器學習框架， 幫助開發者快速將多種機器學習模型融合到 App。 2017 年推出，2018 年升級為 Core ML 2，處理速度提升了 30%。
如今 Core ML 升級為 Core ML 3，Core ML 3 將首次培訓裝置的機器學習。 由於模型可使用裝置用戶資料更新，因此 Core ML 3 能幫助模型不損隱私的情況下與用戶行為保持關聯。 不僅如此，Core ML 3 還支援進階神經網路，支援超過 100 種層類別，在影像、音響辨識有更好表現。 另外，能無縫利用 CPU、GPU 和神經引擎提供最大效能和效率。
 On-device model personalisation:  models bundled into apps can be updated with user data on-device, helping personalise the models to the user while maintaining privacy.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-darknet/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-darknet/readme/</guid>
      <description>Introducion darknet深度学习框架源码分析：详细中文注释，涵盖框架原理与实现语法分析
https://github.com/hgpvision/darknet https://pjreddie.com/darknet/
darknet是一个较为轻型的完全基于C与CUDA的开源深度学习框架， 其主要特点就是容易安装，没有任何依赖项（OpenCV都可以不用）， 移植性非常好，支持CPU与GPU两种计算方式 Feature NVIDIA would like you to buy their new DIGITS Devbox, but priced at $15,000 with a 8-10 week lag time I&amp;rsquo;m not sure why anyone would want it. For about $6,000 after tax you can build your own 4 GPU box that ships in just a few days from Newegg.Comparison w/ Tensorflow 相比于TensorFlow来说，darknet并没有那么强大，但这也成了darknet的优势：
darknet完全由C语言实现，没有任何依赖项， 当然可以使用OpenCV，但只是用其来显示图片、为了更好的可视化； darknet支持CPU（所以没有GPU也不用紧的）与GPU（CUDA/cuDNN，使用GPU当然更块更好了）；
正是因为其较为轻型，没有像TensorFlow那般强大的API，所以给我的感觉就是有另一种味道的灵活性， 适合用来研究底层，可以更为方便的从底层对其进行改进与扩展；
darknet的实现与caffe的实现存在相似的地方，熟悉了darknet，相信对上手caffe有帮助
可以看到裡面有很多OP實作,都是用C or cuda寫的 Details darknet 中最重要的三个struct定义是 network_state, network, layer； 新版本network_state 已经并入到 network 里去了。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-ebpf/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-ebpf/readme/</guid>
      <description>Motivation 大部分情況，其實我們已經可以通過top，pidstat等命令定位到具體是哪一個服務出的問題。
當然重啟服務可以解決60%以上的服務異常問題，但是重啟後會丟失現場。
重啟一時爽，一直重啟就不爽了。還是需要定位到具體的問題。我還是希望知道底病根在哪， 最好直接告訴我哪個具體函數，哪條語句導致的問題或者bug。最差也得知道是大致什麼節點的什麼類型故障
很多人可能會想到GDB。雖然這些工具很偉大，但是這應該不適合我們sre在已經服務已經發病的情況下使用，
因為線上的服務不能被中止。GDB在調試過程中設置斷點會發出SIGSTOP信號， 這會讓被調試進程進入T (TASK_STOPPED or TASK_TRACED)暫停狀態或跟蹤狀態。 同時 GDB 所基於的 ptrace 這種很古老的系統調用，其中的坑和問題也非常多。
比如 ptrace 需要改變目標調試進程的父親，還不允許多個調試者同時分析同一個進程， 而且不太熟悉GDB的人可能會把程序調試掛了，這種交互式的追蹤過程通常不考慮生產安全性， 也不在乎性能損耗。另外提一下，strace也是基於ptrace的，所以strace也是對性能不友好的
那麼就要提到動態追蹤技術了，動態追蹤技術通常通過探針這樣的機制發起查詢。 動態追蹤一般來說是不需要應用目標來配合的，隨時隨地，按需採集
而且它非常大的優勢為性能消耗極小（通常5%或者更低的水平)
dynamic tracing 動態追蹤技術（dynamic tracing）是現代軟體的進階除錯和追蹤機制
在動態追蹤的實作中，一般是通過探針 (probe) 這樣的機制來發起查詢。
我們會在軟體系統的某個層次，或者某幾個層次上面，安置一些探針，然後我們會自己定義這些探針所關聯的處理程式
動態追蹤機制如果內建於作業系統，那麼使用者層級的程式即可隨時採集資訊，
構建出一幅完整的軟體樣貌，從而有效地指導我們做一些很複雜的分析。這裡非常關鍵的一點是，它是非侵入式的
動態追蹤的工具很多，
 (1) systemtap (2) perf (3) ftrace (4) sysdig (5) dtrace (6) eBPF =&amp;gt; 但eBPF裡面也有support static tracing  eBPF introduction Berkeley Packet Filter (BPF) 最初的動機的確是封包過濾機制，
 但擴充為 eBPF (Extended BPF) 後，就變成 Linux 核心內建的內部行為分析工具包含以下:   (1) 動態追蹤 (dynamic tracing);</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-keras/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-keras/readme/</guid>
      <description>Concept Keras是一款建立在Theano或Tensorflow上的高層神經網路API。
相較於TensorFlow與pytorch等套件，keras更適合想快速體驗機器學習的初學者
因為keras是高層的神經網路API，較不適合做一些底層的更動，也較難深度的去學習神經網路背後的原理，
若是已有深度學習的基礎，又或者是時間較為充裕者，可以前往Tensorflow又或者是pytorch的教學
以Keras作為Tensorflow的接口，可以說是大大的降低了神經網路的入門門檻，
對於初學者來說可以大大的降低疊出一個神經網路的時間，使用上極為方便
Install 安裝keras
sudo pip3 install keras
安裝存儲model的套件
sudo pip3 install h5py
=&amp;gt; 所以Keras的model檔案
Keras models Keras有两种类型的模型，序贯模型（Sequential）和函数式模型（Model），函数式模型应用更为广泛，
=&amp;gt; 序贯模型是函数式模型的一种特殊情况
序贯模型
=&amp;gt; 序贯模型是多个网络层的线性堆叠，也就是“一条路走到黑”。
可以通过向Sequential模型传递一个layer的list来构造该模型
也可以通过.add()方法一个个的将layer加入模型中：
model= Sequential()
model.add(Dense(32, input_shape=(784,)))
model.add(Activation(&amp;lsquo;relu&amp;rsquo;))
Keras layer building block Dense就是fully connected
然後在建構model,的確會比tensorflow簡單一些</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-mmsr/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-mmsr/readme/</guid>
      <description>Concept MMSR is an open source image and video super-resolution toolbox based on PyTorch. It is a part of the open-mmlab project developed by Multimedia Laboratory, CUHK. MMSR is based on our previous projects:
 (1) BasicSR, (2) ESRGAN (PIRM18) (3) EDVR (NTIRE19)  Dependencies and Installation deformable conv是使用某一個的implementaiton
Python 3 (Recommend to use Anaconda)
PyTorch &amp;gt;= 1.1
For now, we mainly support pytorch 1.1-1.4. We will support Pytorch 1.5 in the future ASAP</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-mxnet/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-mxnet/readme/</guid>
      <description>Concept MXNet是一種高度可擴展的深度學習工具，可用於各種設備。 雖然與TensorFlow相比，它似乎沒有被廣泛使用，
但MXNet的增長可能會因為成為一個Apache項目而得到提升。
MXNet基本特性：
(1) 該框架支持多種語言，如C ++，Python，R，Julia，JavaScript，Scala，Go，甚至Perl。
(2) 可以在多個GPU和許多機器上非常有效地並行計算。
MXNet優點： (1) 支持多個GPU（具有優化的計算和快速上下文切換）
(2) 清晰且易於維護的代碼（Python，R，Scala和其他API）
(3) 快速解決問題的能力（對於像我這樣的深度學習新手至關重要） (4) 雖然它不像TF那麼受歡迎，但MXNet具有詳細的文檔並且易於使用， 能夠在命令式和符號式編程風格之間進行選擇，使其成為初學者和經驗豐富的工程師的理想選擇。
Other
(1) 其開發者之一李沐，是中國人，在MXNet的推廣中具有語言優勢（漢語）
(2) MXNet的高層介面是Gluon，Gluon同時支援靈活的動態圖和高效的靜態圖， 既保留動態圖的易用性，也具有靜態圖的高效能
Note
https://github.com/PaddlePaddle/X2Paddle
Paddle裡面的pretrained model -&amp;gt; paddle model
也只有支援
TensorFlow
Caffe
ONNX
Design mxnet::engine 主要包括如下实现： (1) function 并行执行过程中的参数依赖问题
(2) 精确到 device 的多线程调度控制
(3) 除了具体实现之外，可以借鉴的设计思想：
每个 device 分配自己的任务队列和线程池，function 分配到具体 device 执行
便于更可控的性能调度
普通任务通过设置 device id 分配到具体的 device 上执行
设立 high priority 专用线程池，不区分 device，所有 device 资源优先执行高优先任务
CPU/GPU 间的拷贝操作单独拆开，用 IO 专用线程池专门负责，保证与计算任务间并发</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-ncnn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-ncnn/readme/</guid>
      <description>Concept (for inference) https://github.com/Tencent/ncnn ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。
ncnn 从设计之初深刻考虑手机端的部署和使用。
无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。
基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，
开发出人工智能 APP，将 AI 带到你的指尖。
ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。
所以這個應該專做device端inference
功能
•支持卷积神经网络，支持多输入和多分支结构，可计算部分分支 •无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架
•纯 C++ 实现，跨平台，支持 android ios 等
•ARM NEON 汇编级良心优化，计算速度极快 •精细的内存管理和数据结构设计，内存占用极低
•支持多核并行计算加速，ARM big.LITTLE cpu 调度优化
•整体库体积小于 500K，并可轻松精简到小于 300K
•可扩展的模型设计，支持 8bit 量化和半精度浮点存储，可导入 caffe 模型
•支持直接内存零拷贝引用加载网络模型
•可注册自定义层实现并扩展
=&amp;gt; 所以特點是很小,用c++實作,對ARM CPU有優化</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-onnx/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-onnx/readme/</guid>
      <description>Concept onnx全称Open Neural Network Exchange，本身设计的目的就是用来进行模型之间的相互转换</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-openai/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-openai/readme/</guid>
      <description>OpenAI </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-opencv/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-opencv/readme/</guid>
      <description>OpenCV 如果是在VM環境想用Camera, 會需要開啟VM前先設定, attach host端的webcam如下 C:\Program Files\Oracle\VirtualBox&amp;gt;VBoxManage.exe controlvm &amp;ldquo;Ubuntu&amp;rdquo; webcam attach .1
issue : undefined reference to `cv::String::allocate(unsigned long)&amp;rsquo; C++也有OpenCV
但是簡單的範例 確compile不過
#include &amp;lt;opencv2/core/core.hpp&amp;gt;
#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;
#include using namespace cv;
using namespace std;
int main( int argc, char** argv )
{ Mat image;
image = imread(argv[1], CV_LOAD_IMAGE_COLOR);
透過這個方式可以安裝OpenCV for C++ and python
sudo apt-get install libopencv-dev python-opencv
若用 C++ 開發 OpenCV 的程式，官方是建議搭配 CMake 來編譯，而 CMake 的 CMakeLists.txt 設定檔內容如下：
要把cpp改成你的名子
cmake_minimum_required(VERSION 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-openvino/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-openvino/readme/</guid>
      <description>Overview ====================================
NN part特點 (1) 有支援Caffe/Kaldi/MXNet/ONNX/TF(尤其是Kaldi)
(2) 還有一個demo folder,來做安裝好的驗證,script會自己下載caffe model然後做convert
(3) Model optimizer只有支援 a. Cutting off parts of the model, 例如將只有訓練用的部份砍掉 e.g. dropout b. OP fusion (4) OpenVINO IE inference engine主要針對HW有做OP優化(e.g. CPU/GPU/VPU-Movidius) (5) 可以在inference API指定batch
(6) inference的時後有支援 async call (API:StartAsync),評估建議是throughput - FPS
(7) inference的時後有支援 sync call (API:Infer),評估建議是throughput - latency
CV part特點 (1) 此外OpenVINO OpenCV部份也有針對一些CV function做硬體加速
integration特點 (1) OpenCV 3.3以上本來就支援DNN 至少有支援Caffe and Tensorflow model,API的用法如下圖
cv::dnn::readNetFromTensorflow(weights, prototxt); (2) 但OpenVINO OpenCV NN部份有連結到 IE(inference engine),所以有做優化,這樣就可以用同一套串CV and NN了</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-paddlepaddle/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-paddlepaddle/readme/</guid>
      <description>Concept (for training and inference) PaddlePaddle Keys (1) 同时支持动态图和静态图，兼顾灵活性和高性能
桨同时为用户提供动态图和静态图两种计算图。动态图组网更加灵活、
调试网络便捷，实现AI 想法更快速；静态图部署方便、运行速度快，应用落地更高效 (2) 源于实际业务淬炼，提供应用效果领先的官方模型
飞桨提供的80+官方模型，全部经过真实应用场景的有效验证。
不仅包含“更懂中文”的NLP 模型，同时开源多个视觉领域国际竞赛冠军算法
(3) 源于产业实践，输出业界领先的超大规模并行深度学习平台能力 (4) 追求极致速度体验，推理引擎一体化设计实现训练到多端推理的无缝对接
飞桨完整支持多框架、多硬件和多操作系统，为用户提供高兼容性、高性能的多端部署能力。
依托业界领先的底层加速库，利用 Paddle Lite和 Paddle Serving 分别实现端侧和服务器上的部署
飞桨提供高效的自动化模型压缩库 PaddleSlim，实现高精度的模型体积优化，
并提供业界领先的轻量级模型结构自动搜索Light-NAS，
对比MobileNet v2在ImageNet 1000类分类任务上精度无损情况下FLOPS 减少17%
PaddlePaddle Repo (1) Paddle
飞桨』核心框架，高性能单机、分布式训练和跨平台部署
https://github.com/PaddlePaddle/Paddle
(2) Paddle-Lite
https://github.com/PaddlePaddle/Paddle-Lite Paddle Lite为Paddle-Mobile的升级版， 定位支持包括手机移动端在内更多场景的轻量化高效预测，
支持更广泛的硬件和平台，是一个高性能、轻量级的深度学习预测引擎 (3) Paddle model
https://github.com/PaddlePaddle/models
官方模型库，包含多种学术前沿和工业场景验证的深度学习模型 (4) PaddleHub
PaddleHub是基于PaddlePaddle生态下的预训练模型管理和迁移学习工具</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-pytorch/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-pytorch/readme/</guid>
      <description>Concept 但 2017 年初由 Facebook 開源的另一套建立在 Torch 之上的深度學習框架 PyTorch 因其語法簡潔優雅、概念直觀和易上手的特性
且標榜 Python First ，為量身替 Python 語言所打造，
使用起來就跟寫一般 Python 專案沒兩樣，也能和其他 Python 套件無痛整合 PyTorch 的優勢在於其概念相當直觀且語法簡潔優雅，
因此視為新手入門的一個好選項；再來其輕量架構讓模型得以快速訓練且有效運用資源
Practice 這邊有使用Pytorch的範例
https://github.com/evansin100/SW-FRAMEWORK-MMSR
basic element：Tensor 一個 Tensor（張量）類似一個高維度向量，也是深度學習裡進行運算的基本元素。
這裡比數學上的意義還要廣義，所以可以把它當成任意維度的資料向量。 既然此文假設讀者已有基本神經網絡知識，那為什麼 Tensor 會是基本元素應該不難理解吧。
Computational Graph Computational graph 讓你定義 data 要怎麼銜接組合才能取得 output、
其中有哪些 parameter、有哪些 activation function 等等，
總之你的 model 要運算導數（derivative）及梯度（gradient）需要的資訊都在裡頭
Functions 神經網絡需要用到很多 function，例如 activation function、loss function 等等
torch.nn 提供了很多 neural network 需要的功能和元件，
而 torch.nn.functional 也提供了很多常用 function。兩者差別在於， torch.nn.functional 提供的是純函數，
而 torch.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-slim/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-slim/readme/</guid>
      <description>Concept slim库是tensorflow中的一个高层封装， 它将原来很多tf中复杂的函数进一步封装，省去了很多重复的参数，
以及平时不会考虑到的参数。可以理解为tensorflow的升级版</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-tensorflow-hub/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-tensorflow-hub/readme/</guid>
      <description>Concept TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.
 Transfer learning can:  Train a model with a smaller dataset, Improve generalization, and Speed up training.
目前總共有468個pre-trained model
可以做完transfer learning</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-tensorflow/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-tensorflow/readme/</guid>
      <description>Tensorflow build from source git clone https://github.com/tensorflow/tensorflow.git
git branch -a
然後選擇要的版本
cd tensorflow # cd to the top-level directory created
=&amp;gt; ./configure
因為build這個會需要bazel但是tensorflow會指定　bazel的版本
解決的方是install from script
先去官網
(使用舊的版本比較不會有問題）
For TF1.12, you need to downgrade Bazel to 0.15.0. For TF1.13.1, you need to use Bazel 0.19.2. Thanks! https://github.com/bazelbuild/bazel/releases/tag/0.27.1 下載對應的版本
chmod +x bazel--installer-linux-x86_64.sh
./bazel--installer-linux-x86_64.sh &amp;ndash;user
The &amp;ndash;user flag installs Bazel to the $HOME/bin directory on your system 所以會被安裝在home/bin
然後再指定目錄即可
export PATH=&amp;quot;$PATH:$HOME/bin&amp;rdquo;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-tensorflow2.0/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-tensorflow2.0/readme/</guid>
      <description>Concept Tensorflow 2.0 : Eager + Keras API
用法和Tensorflow1.x完全不同 等於重頭再來
Tensorflow1.0 使用 tensorflow 會覺得 coding 好像是給計算機而不是給人看的。
要先 init session and global variables; input variable 要使用 placeholder 先佔位。 要 session run 才能執行程式。用 operators/operands 建造一個深度學習網絡非常複雜。
主因是 tensorflow 需要先建構 (靜態) computation graph. 即使是執行一些簡單的運算也要歷經這種麻煩的過程。
完全不像 python programming 的清晰明白，也就是 Pythonic way!
一般人(e.g. me) 可能會偏愛用更高階的 keras 或是 pytorch 開發深度學習專案，
而避免直接使用 tensorflow 開發。除非是非常重視效能的 edge device 應用才必須用 TF or TFLite 開發。
[4] summarize tensorflow 1.x 的幾個問題：
 TensorFlow 的代码比较像是給計算機看的 TensorFlow 不容易 debug Computation graph 模式很难描述  Tensorflow1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-tflite/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-tflite/readme/</guid>
      <description>TFLite </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-xiaomi-mace/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-xiaomi-mace/readme/</guid>
      <description>Concept Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。 =&amp;gt; 所以主要就是做inference MACE覆盖了常见的移动端计算设备（CPU、GPU、Hexagon DSP、Hexagon HTA、MTK APU）， 并且提供了完整的工具链和文档，用户借助MACE能够 很方便地在移动端部署深度学习模型。 MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性
下图描述了MACE的基本框架
MACE version commit d763bc2cf1d2559798bbf7931b4bf4990ac3945f (HEAD -&amp;gt; master, origin/master, origin/HEAD) Author: elswork &amp;lt;elswork@gmail.com&amp;gt; Date: Thu Apr 30 04:00:41 2020 +0200 Update README.md (#632) Fix broken link Prepare 0 (python) conda activate mace
還有要安裝mace要的requirement
Prepare 1 (bazel) 要安裝bazel 0.13 (原本是0.26)
注意把bazel用user版本安裝,這樣在build mace的時候,才可以不用root build,用root會有問題
=&amp;gt; sudo ./bazel-0.13.1-installer-linux-x86_64.sh &amp;ndash;user &amp;ndash;bin=/home/evan/bin-user
export BAZEL_VERSION=0.13.1 mkdir bazel-user &amp;amp;&amp;amp; \ cd bazel-user &amp;amp;&amp;amp; \ wget https://github.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-android-ion/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-android-ion/readme/</guid>
      <description>Motivation 为了避免内存碎片化，或者为一些有着特殊内存需求的硬件，
比如GPUs、display controller以及camera等，在系统启动的时候，
会为他们预留一些memory pools，这些memory pools就由ION来管理。
通过ION就可以在硬件以及user space之间实现zero-copy的内存share
=&amp;gt; 所以就是想要做kernelspace and user space的data sharing是zero copy =&amp;gt; 然後手法就是先預留memory pool
ION implmentation ION通过ION heaps来展示presents它对应的memory pools。 不同的Android硬件可能会要求不同的ION heaps实现，
默认的ION驱动会提供如下三种不同的ION heaps实现 (1) ION_HEAP_TYPE_SYSTEM: memory allocated via vmalloc_user()
(2) ION_HEAP_TYPE_SYSTEM_CONTIG: memory allocated via kzalloc
(3) ION_HEAP_TYPE_CARVEOUT: carveout memory is physically contiguous and set aside at boot.
开发者可以自己实现更多的ION heaps。比如NVIDIA就提交了一种ION_HEAP_TYPE_IOMMU的heap，这种heap带有IOMMU功能。
不管哪一种ION heaps实现，他们都必须实现如下接口：
 struct ion_heap_ops { int (*allocate) (struct ion_heap *heap, struct ion_buffer *buffer, unsigned long len, unsigned long align, unsigned long flags); void (*free) (struct ion_buffer *buffer); int (*phys) (struct ion_heap *heap, struct ion_buffer *buffer, ion_phys_addr_t *addr, size_t *len); struct scatterlist *(*map_dma) (struct ion_heap *heap, struct ion_buffer *buffer); void (*unmap_dma) (struct ion_heap *heap, struct ion_buffer *buffer); void * (*map_kernel) (struct ion_heap *heap, struct ion_buffer *buffer); void (*unmap_kernel) (struct ion_heap *heap, struct ion_buffer *buffer); int (*map_user) (struct ion_heap *heap, struct ion_buffer *buffer, struct vm_area_struct *vma); }; 简单来说，接口的各个函数功能如下：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-data_analysis/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-data_analysis/readme/</guid>
      <description>Concept 資料對於deep learning的重要性
(1)資料的品質、(2)特徵的選取決定了機器學習的上限， =&amp;gt; 模型(Model)只是逼近這個上限
雖然在學術界總是以Model為主要討論對象，
但實際上在業界80%的時間都是在對資料進行前處理，
包含了資料獲取、清理、特徵選擇、特徵處理… 到這裡我們可以稍微了解資料前處理的重要性
Data pre-process summary 常見的資料前處理如下所示：
(1) 缺失值的處理(Missing Value) (2) 去除極端值(outlier)或是雜訊(noisy data)
(3) 類別資料的處理（有序、無序）(categorical variable) (4) 資料特徵縮放 (feature scaling) 還有很多進階的資料前處理，比方說去掉一些不重要的欄位或是利用某些欄位去組合產生出新的欄位
Data understanding (1) clustering</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-distributed/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-distributed/readme/</guid>
      <description>Basic - Session 通過多 GPU 並行的方式可以有很好的加速效果，然而一臺機器上所支持的 GPU 是有限的
分佈式 TensorFlow 允許我們在多臺機器上運行一個模型，所以訓練速度或加速效果能顯著地提升
每次調用 tf.Session() 都會創建一個單獨的「執行引擎」，然後將會話句柄連接到執行引擎。執行引擎是實際存儲變量值並運行操作的東西。
且 Python 天生是面向對象的編程，它裏面的元素都是類或對象，因此更正式地說，tf.Seesio() 是 TensorFlow 中的一個方法，它會打開一個會話並運行計算圖
同進程中的執行引擎是不相關的。在一個會話中更改變量（在一個執行引擎上）不會影響其他會話中的變量。
print(&amp;ldquo;Initial value of var in session 1:&amp;quot;, sess1.run(var))
print(&amp;ldquo;Initial value of var in session 2:&amp;quot;, sess2.run(var))
sess1.run(var.assign_add(1.0)) // 這邊更改sess1的變數,對sess2不會有影響
print(&amp;ldquo;Incremented var in session 1&amp;rdquo;)
print(&amp;ldquo;Value of var in session 1:&amp;quot;, sess1.run(var))
print(&amp;ldquo;Value of var in session 2:&amp;quot;, sess2.run(var))
Server tf.train.Server.create_local_server 在本地創建一個只有一臺機器的 TensorFlow 集羣。然後在集羣上生成一個會話，通過該對話，我們可以將創建的計算圖運行在 TensorFlow 集羣上。
雖然這只是一個單機集羣，但它基本上反映了 TensorFlow 集羣的工作流程
task/job TensorFlow 集羣會通過一系列任務（task）來執行計算圖中的運算，一般來說不同的任務會在不同的機器上運行。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-euler/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-euler/readme/</guid>
      <description>Concept e，又叫歐拉常數 Euler’s number
歐拉常數e是與(1)極限和(2)無限有關
e定義上是和變化率(rate of change)有關，而且它其實起源於一個和日常生活息息相關的經典經濟學問題
歐拉常數有甚麼用？ 相信大家小學、初中應該都學過甚麼是利息，尤其是複利率 (compound interest)。
複利率這種計息方法就是指除了本身本金外，本金所衍生出來的利息一樣會計息，「利疊利」就好像滾雪球，
電影《食神》發達大計「上市集資，然後分拆公司再上市再集資」般。
Example 假設銀行年利率是100% (小弟都想有)，你存入一元本金，
那在單利息的情況下一年後你會得到的金錢包括本金就是2元；但在複利率計息下，
如果計息期拆做半年，而利率改為50%，咁你最後得到的金錢就會是 (1+0.5)²=2.25。說了這麼久，
Example (e) 那究竟e與複利率有甚麼關係呢？聰明的你/妳或許由e的數學定義已經看出來了吧！
 Question: 試想像複利率中的計息期越縮越短，  例如由一年減至一個月，一日甚至無限小，但收息年期不變，那你最終得到的利息是多少？無限大？   Anwser: 不對，這問題答案其實就是e。  (1) 雖然計息期無限小代表你收息次數有無限多次， (2) 但同一時間每次計息時的利率都會變得無限小 (3) 兩者某程度上會抵消，所以雖然計息期不斷分拆縮短會增加年期完結後最終的利息收入，但增幅是有極限，而這個極限就是e    </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-float/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-float/readme/</guid>
      <description>IEEE-754 曾出現過各種不同的浮點數表示法，但目前最通用的是IEEE二進制運算標準 =&amp;gt; Mantissa就是數字小數部份的表示 (IEEE Standard for Binary Floating-Point Arithmetic , 簡稱IEEE-754)
(1) float32
(2) float16
IEEE 754 标准指定了一个 binary16 要有如下的格式：
Sign bit（符号位）： 1 bit
Exponent width（指数位宽）： 5 bits
Significand precision(Mantissa)（尾数精度）： 11 bits （有10位被显式存储） =&amp;gt; 所以和float32相比就是exponent and mantissa可以用的個數比較少
Example1 13.125 轉浮點數為例 : (1) 由於13.125為正，所以符號(S) = 0
(2) 先將數值轉成二進位並正規化 13.125 = 1101.001 = 1.101001 x 2^3 =&amp;gt; 所以重點是要將數值變成1.xxxxxx x 2^x 的表示
(3) 計算指數(Exponent) = 127 + 3 (原本的次方數加上127，因為次方數有可能是負的) = 01111111 + 11 = 10000010 (4) 計算尾數(Mantissa) = 101001，因為正規化後一定是1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-general-deep-learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-general-deep-learning/readme/</guid>
      <description>Basic 1.dimention 維度
2.layer 縱軸的總維度
3.neurons 神經元，這裡指其中一個輸出與輸入中間的函式
線好像才是計算,圓圈(Neurons,hidden units,神經元)代表的好像是一個一個資料
可以看到這個例子
線(edge)都是計算,
所以全部的data變成一個點(neuro,activation),
但如果要產生多個channel,那就要好幾個Neurons
4.function set 函式集成
5.input 輸入參數
6.output 輸出參數
7.Loss 輸出結果與目標結果的差距
8.Learning rate n，取w值所踏出的步伐長度
9.gradient descent 梯度下降法
10.w 權重
11.Backpropagation 一種計算斜率的方式
12.batch_size 代表 一個batch 有幾個函式
Feature Gategory 網路上有個分類不錯
 Face identification (CPU and GPU) &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Camera stills (靜止的，不動的) &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; type Object detection (CPU and GPU) Noise reduction (NPU) Super resolution (CPU and GPU) Semantic segmentation (CPU and GPU) &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Video &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; type (這個一定是全系列NPU) Super resolution (NPU) Semantic segmentation (NPU) &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Voice assistants &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; type Keyword spotting (CPU and GPU) Natural language processing (CPU and GPU) Text to speech (CPU and GPU) &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Driving &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; type Autonomous driving (NPU) Driver assistance (CPU and GPU) &amp;lt;/td&amp;gt;  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-isp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-isp/readme/</guid>
      <description>ISP </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-leetcode/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-leetcode/readme/</guid>
      <description>Concept 先刷热题 HOT 100，再刷精选 TOP 面试题，之后刷其他的题。
如果你时间比较紧迫，为了找工作而刷题，我建议你先刷热门推荐，一共两百多道题
如果你时间比较充裕，那我建议你：
按从低到高的难度分组刷
按 tag 分类刷
定期复习，重做之前刷过的题
如果你在刷题的时候发现怎么也写不出来，别担心，这是正常的。
如果你还发现，之前明明刷过的题，过段时间再做的时候，自己还是不会。别担心，这也是正常的。
刷题方法：
第一遍：可以先思考，之后看参考答案刷，结合其他人的题解刷。思考、总结并掌握本题的类型，思考方式，最优题解。
第二遍：先思考，回忆最优解法，并与之前自己写过的解答作比对，总结问题和方法。
第三遍：提升刷题速度，拿出一个题，就能够知道其考察重点，解题方法，在短时间内写出解答。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-rounding/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-rounding/readme/</guid>
      <description>IEE754 Definition IEEE有5個定義　 (0) RTE - 一般在code裡面看到的 (1) 因為direct rounding很單純　(2) 所以只要看round to nearest/ties to even的部份即可　基本上還是follow四捨五入　(3) 只是如果發現是模擬兩可的狀態下(tie e.g, xxx.5) 就看進位會不會變成偶數,是的話就進位　&amp;lt;/td&amp;gt;   (0) RTZ - 一般在code裡面看到的 (1) 就是直接砍掉(truncate) (2) for conversion to integers or the default rounding mode for conversion to floating-point types. &amp;lt;/td&amp;gt;   (0) RTP - 一般在code裡面看到的 (1) the result of the rounding is never smaller than the argument. (2) 有點像是無條件進位　+6.0 will be rounded to +6, while +6.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-similarity/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-similarity/readme/</guid>
      <description>Similarity </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-info-vision_spec/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-info-vision_spec/readme/</guid>
      <description>Concept 視聽系統都在強調高清二個字，但什麼是高清，就是我們常常會在電視的左上角看
到幾個英文字母HD，或HDTV還是FULL HD。
  (1) p and i
 先從簡單的來說，720P 1080i 和 1080P，它們的P和i到底代表什麼意思？ i 隔行掃描(interlaced scan)  i代表Interlace指的是傳統的交錯式掃瞄，也就是它是隔行掃描顯示， 我們會先看到135, 再看到246， =&amp;gt; 等於一個畫面跑出來時，它只會先跑畫面的一半頻寬， 所以很快的另一半也會馬上出現，視覺暫留的效果，1080i就會很清楚， 不過如果畫面是高速移動，就會有一 點點晃動的效果，可是不仔細看是看不出來的。   p 逐行掃描 (Progressive scan)  P是使用循序式掃瞄的畫面顯示方式， 也就是逐行掃瞄是123456 循序式掃瞄的優點是畫面穩定、不會閃爍， 如果我們長時間看電視，也不容易疲累，但是缺點是傳送畫面所需的頻寬比以前多出一倍。      (2) 1920x1080 =&amp;gt; why naming 1080 only
 為什麼都要用高來稱解析度 「1080」表示&amp;quot;垂直&amp;quot;方向有1080條水平掃描線    (3) Comparison
 4K: 2160 x 4096 (代表寬4K) 2K: 1152 x 2048 (代表寬2K) 1080p: 1080(高,代表水平有幾條線) x 1920(寬) 720p: 720 x 1280 DV: 480 x 720     </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-lang-c/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-lang-c/readme/</guid>
      <description>C-_and_C </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-lang-octave/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-lang-octave/readme/</guid>
      <description>Concept Octave 可以看作是商业语言 MATLAB 的 GNU 版本，它是一种脚本矩阵语言（scripting matrix language），
其语法有大约 95% 可与 MATLAB 兼容。Octave 由工程师设计，
因此预装了工程师常用的程序，其中很多时间序列分析程序、统计程序、文件命令和绘图命令与 MATLAB 语言相同。
=&amp;gt; 所以不用錢
 优点  (1) 首先，目前没有可用的鲁棒性 Octave 编译器，且没有必要有，因为该软件可以免费安装。 (2) Octave 和 Matlab 的语言元素相同，除了一些个例，如嵌套函数。  Octave 仍然处于积极开发的状态，每一个偏离 Matlab 语法之处都被视为 bug 或者至少是待解决问题。   (3) Octave 有很多可用工具箱，只要程序不要求图输出，那么在不进行大量更改的前提下，使用 Octave 运行和使用 Matlab 运行差不多。 (4) 图方面的能力是 Matlab 的优势。Matlab 最新版本包括 GUI 设计器，包含大量很棒的可视化特征。 (5) Octave 使用 GNU Plot 或 JHandles 作为图程序包，JHandles 与 Matlab 中的图程序包更接近一些。  但是，Octave 不具备类似 GUI 设计器的组件，其可视化机制很受限且不与 Matlab 兼容。   (6) 集成开发环境也是类似的情况：Octave 有一个 QTOctave 项目，但仍处于早期阶段。 (7) Octave 社区的合作很可能帮助该软件很快提供更好、更兼容的图以及 GUI 能力。   缺点：  它只是 MATLAB 的免费开源版本，无法带给用户新的东西    Install sudo add-apt-repository &amp;ndash;yes https://adoptopenjdk.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-lang-opencl/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-lang-opencl/readme/</guid>
      <description>Key 建議看底下link了解GPU(OpenCL-&amp;gt;NDrange-&amp;gt;GPU thread的整個處理流程) https://github.com/evansin100/OpenCL/tree/master/Core
可以看這邊有
(1) GPU arch. workgroup (kernel-fiber,wave,L2 cache,L1 cache,texture unit) (2) GPU memory 優化還有架構 local memory,global memory
(3) GPU openCL 優化技巧(一個kernel計算多個output..etc.) https://github.com/evansin100/OpenCL/tree/master/whitepaper/qcom</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-lang-opengl/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-lang-opengl/readme/</guid>
      <description>Comparison 计算机图形程序接口
 Vulkan不再使用OpenGL的状态机设计，内部也不保存全局状态变量。 显示资源全然由应用层负责管理。包括内存管理、线程管理、多线程绘制命令产生、渲染队列提交等。 应用程序能够充分利用CPU的多核多线程的计算资源，降低CPU等待，降低延迟。 带来的问题是。 线程间的同步问题也由应用程序负责，从而对开发人员的要求也更高。 &amp;lt;/td&amp;gt;  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-lang-python/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-lang-python/readme/</guid>
      <description>可以用簡單的python API
就可以做到deep learning的API
不用真的跑起來tensorflow or model
可以快速做實驗</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-activation/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-activation/readme/</guid>
      <description>Purpose 使用&amp;quot;激勵函數&amp;quot;的目的
懶人包：激勵函數主要作用是引入非線性
在類神經網路中如果不使用激勵函數，那麼在類神經網路中皆是以上層輸入的線性組合作為這一層的輸出
（也就是矩陣相乘），輸出和輸入依然脫離不了線性關係，做深度類神經網路便失去意義 =&amp;gt; 所以就是要讓數值比較亂一點就是了
Requirement (1)非線性(not y=ax linear function),(2)可微分 (因為要可以訓練)
只要滿足這兩個就算activation function
Comparison  sigmoid函数也叫Logistic函数 然而Sigmoid存在著三大缺點: A. 容易出現梯度消失gradient vanishing (上面有介紹) B. 函數輸出並不是zero-centered : =&amp;gt; 因為產生的值 當x=0(input)的時候,output不是0,而是約等於0.4多 C. 指數運算較為耗時 &amp;lt;/td&amp;gt;   這也是個常用於分類問題的activation function，輸出範圍介於[-1, 1] ，輸出範圍會有正有負，也是個嚴格遞增函數，他的微分是f&#39;(x) = 1 - f^2(x)。 實際應用上跟sigmoid function差不多，tanh收斂到1跟-1的速度比較快， 所以容易學的比較慢一些，相對sigmoid function的學習表現也不是非常好 &amp;lt;/td&amp;gt;   1. 梯度消失問題 (vanishing gradient problem) 對使用反向傳播訓練的類神經網絡來說，梯度的問題是最重要的， 使用 sigmoid 和 tanh 函數容易發生梯度消失問題，是類神經網絡加深時主要的訓練障礙。 具體的原因是這兩者函數在接近飽和區 (如sigmoid函數在 [-4, +4] 之外)， 求導後趨近於0，也就是所謂梯度消失， 造成更新的訊息無法藉由反向傳播傳遞 2. 類神經網路的稀疏性（奧卡姆剃刀原則） Relu會使部分神經元的輸出為0，可以讓神經網路變得稀疏，緩解過度擬合的問題 3.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-conv-block/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-conv-block/readme/</guid>
      <description>Conv-Block </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-deconv/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-deconv/readme/</guid>
      <description>Concept  deconvolution并不是个好名字，因为它存在歧义： before (感覺有從模糊變回清楚的感覺)  deconvolution最初被定义为“inverse of convolution”或者“inverse filter”或者“解卷积”，是指消除先前滤波作用的方法。 比如，我们认为原始图像是清晰的，但是通过透镜观测到的图像却变得模糊，如果假设透镜的作用相当于以某个kernel作用在原始图像上， 由此导致图像变得模糊，那么根据模糊的图像估计这个kernel或者根据模糊图像恢复原始清晰图像的过程就叫deconvolution   after (correct naming), 只是說反像操作  又重新定义了deconvolution，实际上与transposed convolution、sub-pixel or fractional convolution指代相同。 transposed convolution是一个更好的名字，sub-pixel or fractional convolution可以看成是transposed convolution的一个特例。 对一个常规的卷积层而言，前向传播时是convolution，将input feature map映射为output feature map， 反向传播时则是transposed convolution，根据output feature map的梯度计算出input feature map的梯度，梯度图的尺寸与feature map的尺寸相同。    Comparison  tensorflow =&amp;gt; input padding caffe =&amp;gt; GEMM + col2im 傳統deconv and ncnn =&amp;gt; 有人說是 direct conv
=&amp;gt; 看起來不同的實作算出來的答案會不同 e.g.,padding or direct conv  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-depthwise-conv/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-depthwise-conv/readme/</guid>
      <description>Traditional CONV 舉個例子
input = [224,224,3,1] RGB的圖
output = [224,224,10,1] 變成有10個channel
這樣的話kernel 可以是[5,5,3,10]
如下圖(1), input channel 一定要等於 kernel channel
如下圖(2), kernel個數 一定要等於 output channel
如下圖(3), output每個點的計算等於 input channel 和對應的 filter channel做計算再加起來
以範例來看就是
input channel 1 &amp;lt;=&amp;gt; kernel 1 + input channel 2 &amp;lt;=&amp;gt; kernel 2 + input channel 3 &amp;lt;=&amp;gt; kernel 3
W_in = 224
H_in = 224 Nch = 3
k = 5
Nk = 10
可以這樣想,output = [224,224,10,1]</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-fully-connected/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-fully-connected/readme/</guid>
      <description>Concept 全連結層：全連結層（fully connected layer）用於將一層裡的各神經元連接到另一層裡的所有神經元。
=&amp;gt; fully connected layer就是没有weights share的!和一般的3x3 conv不同 =&amp;gt; fully-connected and 1x1 and 3x3,conv ..是兩種類型的應用(因為沒有shared weight,1x1有) =&amp;gt; case 1:input 10x10x1, 1x1 conv, 這樣kernel大小只有1x1
=&amp;gt; case 2:input 10x10x1, 1x1 conv, 這樣kernel大小暴增為10x10
所以可以看出fully connected,每個input都會有一個kernel參數(所以就是全連接層)
=&amp;gt; 應用: 1x1 conv,通常用來調整維度
e.g.,10x10x3,我想把他調整為10x10x5,那就需要[1x1x3]=&amp;gt;可以產出ouput channel(1),有5個那就需要 [1x1x3]x5 =&amp;gt; 應用: fully connected,通常用來針對每個input做訓練for next softmax e.g.,10x10x3,我想把他調整為10x10x5,那就需要[10x10x3]=&amp;gt;可以產出ouput channel(1),有5個那就需要 [10x10x3]x5 e.g.,1x1x300,我想把他調整為1x1x10,那就需要[1x1x300]=&amp;gt;可以產出ouput channel(1),有10個那就需要 [1x1x300]x10
==
透過fully-connected,可以做到平坦化, 基本上全連接層的部分就是將之前的結果平坦化之後接到最基本的神經網絡了 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。
如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，
全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。
在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；
而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽
Full example conv+pooling+fully connected
最后的两列小圆球就是两个全连接层，在最后一层卷积结束后，进行了最后一次池化，
输出了20个12 x 12的图像，然后通过了一个全连接层变成了1 x 100的向量。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-gemm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-gemm/readme/</guid>
      <description>Matrix </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-octave/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-octave/readme/</guid>
      <description>Paper https://blog.csdn.net/weixin_37993251/article/details/89333099
Facebook和新加坡国立大学联手提出了新一代替代品：OctConv（Octave Convolution），
效果惊艳，用起来还非常方便。
OctConv就如同卷积神经网络（CNN）的“压缩器”。 用它替代传统卷积，能在提升效果的同时，节约计算资源的消耗
OctConv即插即用 (所以可以直接替換conv)，无需修改原来的网络架构，也不用调整超参数，方便到家。 就是这个新一代的卷积，让GAN的主要创造者、AI大牛Ian Goodfellow迫不及待，
不仅转发力荐，还表示要持续关注进展，开源时再发推告诉大家
Abstract 在自然图像中，信息以不同的频率传递，其中较高的频率通常用精细的细节编码，较低的频率通常用全局结构编码。
同样，卷积层的输出特征图也可以看作是不同频率下信息的混合。
在这项工作中，我们提出将混合特征图按其频率分解，
并设计一种新的Octave Convolution(OctConv)操作来存储和处理空间分辨率较低且空间变化较慢的特征图，
从而降低了内存和计算成本。与现有的多尺度方法不同，OctConv被表示为一个单一的、通用的、即插即用的卷积单元，
可以直接替换(普通的)卷积，而无需对网络架构进行任何调整。
它也正交和互补的方法，建议更好的拓扑或减少像组或深度卷积信道冗余。
实验表明，通过简单地用OctConv替换卷积，我们可以不断提高图像和视频识别任务的准确性，
同时降低内存和计算成本。
一个装备了八重卷积网络(octconvo)的ResNet-152仅用22.2 GFLOPs就能在ImageNet上实现82.9%的top-1分类精度。
Introduction 如图1(a)所示，自然图像可以分解为描述平稳变化结构的低空间频率分量和描述快速变化精细细节的高空间频率分量[1,12]。
同样，我们认为卷积层的输出特征映射也可以分解为不同空间频率的特征，
并提出了一种新的多频特征表示方法，将高频和低频特征映射存储到不同的组中
因此，通过相邻位置间的信息共享，可以安全降低低频组的空间分辨率，减少空间冗余，如图1(c)所示。适应新的特征表示，我们推广了vanilla convolution，并提出Octave Convolution(OctConv)将张量特征图包含两个频率和一个octave部分，频率和提取信息直接从低频地图不需要解码的高频如图1所示(d)。作为普通卷积的替代品，OctConv消耗的内存和计算资源大大减少。此外，OctConv对低频信息进行相应的(低频)卷积处理，有效地扩大了原始像素空间的接收域，从而提高了识别性能。
我们以一种通用的方式设计了OctConv，使它成为即插即用的卷积的替代品。
OctConv以来主要集中在加工特征图谱在多个空间频率和减少空间冗余、正交和补充现有的方法， 专注于构建更好的CNN拓扑[24, 38, 36, 34, 30]，
减少channel-wise冗余卷积特征图谱[45, 10, 35, 33, 23]
和减少冗余在浓密的模型参数[40, 16, 32]。我们还将进一步讨论OctConv在群、深度和三维卷积情况下的积分。
此外，与利用多尺度信息的方法[4, 41, 14]不同的是，
OctConv可以很容易地作为即插即用单元部署来替代卷积，而不需要改变网络架构或进行超参数调优
我们的实验证明，
只需用OctConv代替vanilla卷积， 我们始终可以提高受欢迎的2D CNN backbones的性能 包括ResNet [18, 19]， ResNeXt [45], DenseNet [24], MobileNet[20, 35]和SE-Net[21] 在2D图像识别ImageNet[13]，以及3D CNN backbones C2D[42]和I3D[42]视频行动识别动力学[26, 3, 2]。 配备OctConv的Oct-ResNet-152能够以更低的内存和计算成本匹配或超过最先进的手工设计网络[33, 21]。 我们的贡献可以总结如下：</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-pooling/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-pooling/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-op-region-proposal/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-op-region-proposal/readme/</guid>
      <description>Region Finding Comparison  作法是將 Segment 的結果先各自畫出 bounding box，然後以一個迴圈， =&amp;gt; 每次合併相似度最高的兩個 box，直到整張圖合併成單一個 box 為止， 在這過程中的所有 box 便是 selective search 出來的 region proposals &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; (3) Selective Search + ROI pooling (Fast R-CNN) &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; Fast RCNN 一樣要預選 Region proposals， (1) 和R-CNN差異是只做一次 CNN。 (2) 输入特征图尺寸不固定，但是输出特征图尺寸固定, 所以將box用max pooling map到固定大小後再做判斷 e.g., 其中一個30x30(box) to 7x7 e.g., 其中一個20x20(box) to 7x7 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; (4) region proposals(RPN) + ROI pooling (Faster R-CNN) &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; 不管是 R-CNN 還是 Fast R-CNN 都還是要先透過 selective search 預選 region proposals Faster R-CNN 的想法也很直覺，與其預先篩選 region proposals, 到不如從 CNN 的 feature map 上選出 region proposals RPN (Region Proposal Network) 也是一個 Convolution Network (a) &amp;quot;Input 是之前 CNN 輸出的 feature map&amp;quot; (b) 輸出是多個 &amp;quot;bounding box&amp;quot; 以及該 bounding box 包含一個&amp;quot;物體的機率&amp;quot;(這個像是SSD multi-box) (c) 就和SSD mutlti-box一樣,沒有用到softmax(主力是conv) 但是RPN也有缺点，最大的问题就是对小物体检测效果很差， 假设输入为512*512，经过网络后得到的feature map是32*32， 那么feature map上的一个点就要负责周围至少是16*16的一个区域的特征表达， 那对于在原图上很小的物体它的特征就难以得到充分的表示， 因此检测效果比较差。 去年年底的工作SSD: Single Shot MultiBox Detector很好的解决了这个问题 SSD可以理解为multi-scale版本的RPN 而SSD允许从CNN各个level的feature map预测检测结果， 这样就能很好地适应不同scale的物体，对于小物体可以由更底层的feature map做预测 &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; (5) region proposals(RPN) + ROI align (Mask R-CNN) &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; refine ROI pooling在mapping box 到固定大小,會使用max pooling 這樣比較不準,所以改用 &amp;quot;双线性插值法进行计算&amp;quot; =&amp;gt; 所以 7(現在的特徵圖)x32(conv縮小的比例)x2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-ai-playground/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-ai-playground/readme/</guid>
      <description>Concept https://leemeng.tw/deep-learning-resources.html#courses
這邊有紀錄各種的tool可以方便大家了解AI並且透過一些demo app了解AI的功能
列舉了一些透過瀏覽器就能馬上開始遊玩 / 體驗深度學習的應用。 作為這些應用的使用者，你可以先高層次、 直觀地了解深度學習能做些什麼。之後有興趣再進一步了解背後原理。
這小節最適合：
想要快速體會深度學習如何被應用在真實世界的好奇寶寶
想要直觀理解類神經網路（Artifical Neural Network）運作方式的人
想從別人的深度學習應用取得一些靈感的開發者
 (1) https://playground.tensorflow.org/ (2) 由 Tensorflow 團隊推出，模擬訓練一個類神經網路的過程並了解其運作原理 (3) 可以搭配這篇 Introduction to Neural Networks: Playground Exercises 學習 &amp;lt;/td&amp;gt;   (1) https://magenta.tensorflow.org/ (2) 一個利用機器學習來協助人們進行音樂以及藝術創作的開源專案 (3) 可以在網站上的 Demo 頁面嘗試各種由深度學習驅動的音樂 / 繪畫應用（如彈奏鋼琴、擊鼓）   (1) https://experiments.withgoogle.com/collection/ai (2) 這邊展示了接近 40 個利用圖片、 語言以及音樂來與使用者產生互動的機器學習 Apps，值得慢慢探索 (3) 知名例子有 Quick Draw 以及 Teachable Machine，將在下方介紹）   (1) https://quickdraw.withgoogle.com/ (2) 由 Google 推出的知名手寫塗鴉辨識， 使用的神經網路架構有常見的卷積神經網路 CNN 以及循環神經網路 RNN (3) 該深度學習模型會不斷將最新的筆觸當作輸入來預測使用者想畫的物件。 你會驚嘆於她精準且即時的判斷   (1) https://teachablemachine.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-anaconda/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-anaconda/readme/</guid>
      <description>Concept Anaconda是目前最受歡迎的Python數據科學(Data Science)平台， 除了有眾多使用者及企業用戶外，目前也超過1000種的Data Science Packages可使用(包括250多種流行的packages)，
適用於Windows、Linux和MacOS 不同作業系統環境下的conda軟件包(package)和虛擬環境管理器，
對於在安裝、執行及升級複雜的數據科學(Data Science)及機器學習(Machine Learning)環境上變得簡單快速
Conda vs Anaconda 而 Anaconda 中有自己的套件管理系統, 叫做 conda 還沒有安裝的套件我們可以用 conda 裝
=&amp;gt; 所以再安裝Anaconda後,就default有conda來安裝套件了
(1) Conda是一个包管理器；
=&amp;gt; 自动化软件安装，更新，卸载的一种工具
=&amp;gt; 有命令”conda install”, “conda update”, “conda remove”
(2) Anaconda才是一个python发行版。
=&amp;gt; 软件发行版是在系统上提前编译和配置好的软件包集合， 装好了后就可以直接用
虽然conda是用Anaconda打包的， 但是它们两个的目标是完全不同的 conda和Anaconda没有必然关系， 你可以不安装Anaconda的同时， 使用conda安装和管理软件
Remove Anaconda conda install anaconda-clean
anaconda-clean
接著手動刪除anaconda資料夾
rm -rf anaconda3
更改$PATH
sudo vi ~/.bashrc
把anaconda加的都砍掉
Install Anaconda curl -O https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh
sh Anaconda3-5.2.0-Linux-x86_64.sh export PATH=/home/evan/anaconda3/bin:$PATH
Suggestion  所以都先 update conda 自己。 conda update conda 有一個不一定非做不可, 但你開心想把 Anaconda 整個更新, 可以這樣做: conda update anaconda &amp;lt;/td&amp;gt;   更酷的是, 我們做新的環境, 你可以指定 Python 啦, 某某套件的版本。比方說你一直在想要用 Python 2 還是 Python 3, 現在不用管啦, 就各開一個獨立環境就好。 現在我們假設要開一個叫 py3 的 Python 3 環境。 conda create -n py3 python=3 anaconda &amp;lt;/td&amp;gt;   source activate py3 如果要離開的話,只要打上 deactivate即可 &amp;lt;/td&amp;gt;   conda install xxx 若要安裝特定版本的 直接在後面打上版本號即可 $ conda install python=3.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-bazel/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-bazel/readme/</guid>
      <description>Concept 是google的一個軟體構建工具，而他具有四項特性：
(1) 加快構建和測試：
bazel只重構必要的項目，採用先進的本地以及分散式緩存，
優化依賴分析和平行執行，可以更快速構建更多項目
(2) 支援多種語言：
可以在Java、C++、Android、iOS、GO以及其他各領域的語言平台進行構建和測試，
再作業系統也可以再windows,macOS,Linux
(3) 可延展性：
Bazel可以幫助延展你的組織、代碼庫以及不斷的整合系統，
可以處理在多個儲存庫或是一個大的monorepo裡的任意大小代碼庫
(4) 擴展你的需要：
可以用Bazel熟悉的語法簡單的新增對新語言和平台的輔助，
也可以從Bazel的溝通社群將編寫的語言規則分享和重新使用</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-colab/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-colab/readme/</guid>
      <description>Concept Google Colab (Colaboratory)是Google提供的雲端Jupyter Notebook開發環境， 主要作為Python/機器學習/深度學習教學的輔助工具，
最大賣點是提供了免費的GPU(型號為Tesla K80 GPU)， 雖然虛擬化過後運行速度比直接裝一顆K80還慢(IO時間長)， 但是已經比大多數配有中低階GPU的電腦還來的快了 (筆者筆電有顆GTX950M，運行上感覺Colab在某些專案還會快上一些)。 不過每次開啟有12小時的連續使用時間限制，12小時過後虛擬機需要重新開啟才能繼續運行， 也就是說不能在Colab上運行一個需要跑超過12小時的程式。
Usage 1 若是開啟Google Drive上分享的檔案，
大概都會引導你使用Colab功能(建議第三方應用程式)，
或是也可以從你的Google Drive中開啟一份全新檔案。
其實檔案操作方法就很像Google Drive其他類型的檔案 選取GPU作為運算單元
開啟檔案後預設的運行單元為CPU，若要改成GPU則可以按下面兩圖操作
Usage 2 若要開啟的是一份存放在GitHub上的Notebook，
可以直接更改網址便能自動以Colab開啟。例如Notebook連結為:
https://github.com/&amp;lt;一大串東西&amp;gt;.ipynb
能用以下網址開啟：
https://colab.research.google.com/github/&amp;lt;一大串東西&amp;gt;.ipynb</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-compression/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-compression/readme/</guid>
      <description>Summary https://arxiv.org/pdf/1710.09282.pdf
有summarize幾種model compression方法
但其實還可以再補充
 (1) Concept: Reducing redundant parameters which are not sensitive to the performance (2) APP: Convolutional layer and fully connected layer (3) Details: Robust to various settings, can achieve good performance, can support both train from scratch and pre-trained model &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Pruning - Weight sparcity &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) skip 掉0的 filter,把某些filter砍掉, 這個也是 pruning &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Activation sparcity &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) skip 掉0的 activation (因為Relu會產生大量的0) Accelerating Convolutional Neural Networks via Activation Map Compression &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Low-rank factorization &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Using matrix/tensor decomposition to estimate the informative parameters (2) APP: Convolutional layer and fully connected layer (3) Details: Standardized pipeline, easily to be implemented, can support both train from scratch and pre-trained model &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Transferred/compact convolutional filters &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Designing special structural convolutional filters to save parameters (2) APP: Convolutional layer only (3) Details: Algorithms are dependent on applications usually achieve good performance, only support train from scratch &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Knowledge distillation &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Training a compact neural network with distilled knowledge of a large model (2) APP: Convolutional layer and fully connected layer (3) Details: Model performances are sensitive to applications and network structure only support train from scratch &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Quantization &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) model的量化 或者是mix-precision &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt;  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-docker/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-docker/readme/</guid>
      <description>Docker 一般的nn sdk, 都會配置一個docker image, 來方便大家使用sdk來開發
docker裡面其實就是一個virtual image裡面已經有所有配置好的tool e.g. python 以及相關的套件
Prepapation 要使用的話, 就先下載toolbox,安裝的同時也會自動安裝virtualbox
https://docs.docker.com/toolbox/toolbox_install_windows/
然後點Docker QuickStart
接下來就會在virtualbox create一個default
你可以把它想做是docker的一個daemon,用來和host端溝通的
NN SDK 這邊用Xiaomin mace SDK為例子 在docker shell中
Pull lite edition docker image 這樣就可以不用自己準備一個OS,並安裝相關的套件 docker pull registry.cn-hangzhou.aliyuncs.com/xiaomimace/mace-dev
Execute an interactive bash shell on the container 關係圖就是host =&amp;gt; docker daemon =&amp;gt; NN SDK image
docker exec -it mace-dev /bin/bash
如果是重開機後
可以打底下的command來restart docker
docker restart mace-dev
docker exec -it mace-dev /bin/bash
Download SDK and model 在mace-dev的shell中 git clone https://github.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-github_page/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-github_page/readme/</guid>
      <description>Hugo+Github Pages Hugo在Github Pages上建置Blog，比起Hexo簡單不少，而且效能也好， 跟Hexo一樣是使用Markdown來撰寫文章，非常適合程式設計師使用
Hugo本身是使用Go語言編寫，可以讓使用者快速建置一個靜態網站
安裝 Hugo sudo apt install linuxbrew-wrapper
sudo apt-get install build-essential
brew install hugo
export PATH=/home/linuxbrew/.linuxbrew/bin/:$PATH
初始化 Blog (private repo) 先create一個private repo &amp;ldquo;evansin100.github.io.blog&amp;rdquo;
hugo new site 目標資料夾路徑
=&amp;gt; hugo new site /home/evan/evansin-github/evansin.github.io.blog/
成功的話就會在底下出現一堆訊息，該資料夾可以視為根目錄
安裝主題  Hugo有許多主題可以選擇，選擇一個自己看得順眼的主題  https://themes.gohugo.io/   step 1: 先在cmd移動到themes資料夾(資料夾folder中)  =&amp;gt; ~/evansin-github/evansin.github.io.blog/themes git clone https://github.com/ertuil/ertuil.git   step 2: 所以可以下載很多的theme,然後透過指定的方式,隨意切換theme step 3: Hugo 在生成 SSG 時會使用到 config.toml 檔案的設定內容。  我們可以將之前 Mainroad theme 中的 config.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-google-cloud-ml/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-google-cloud-ml/readme/</guid>
      <description>https://blog.gcp.expert/cloud-machine-learning-beginning/
Cloud ML Engine 也是一個您客製化機器學習模型的一個工具，
您可以在上面執行任何一種 TensorFlow 架構，像是 Google 相簿和和 Google Cloud Speech 都是 ML Engine 運用下的成功範例。
Cloud ML Engine 產品特色
HyperTune
ML Engine 結合了 HyperTune，您不用再手動找出所需數據，HyperTune 具備自動調整高等參數的功能，讓您能更有效的建立您的模型。
可攜式模型
您可以透過下載 Cloud Machine Learning 訓練過的模型，提供本機執行或行動整合。
可擴充的代管服務 =&amp;gt; 這個才是我想要用的 Google 利用支援 CPU 和 GPU 的代管型分散式訓練基礎架構，讓您能同時支援以 TB 等級的資料和上千位的使用者，您也不需擔心基礎架構問題。
整合性高
跟過往的 Google 產品相同的是：Cloud ML Engine 能與其他 Google 產品整合，您可以使用來自像是 Cloud Storage 等其他來源的資料、搭配 Cloud Dataflow 的功能處理、Cloud Datalab 的模型建立。 Cloud Machine Learning 是 Google Cloud Platform 對於深度學習提供的管理服務。
它可以讓您建立作用於任何大小、任何資料的學習模型，並藉由 TensorFlow 框架打造您的服務。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-ndk/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-ndk/readme/</guid>
      <description>Android NDK 工具链的使用方法（Standalone Toolchain） gcc 的sysroot 选项 sysroot 选项设定 gcc 在编译源码的时候，寻找头文件和库文件的根目录。
可以这样调用 gcc &amp;ndash;sysroot=/tmp/gcc-arm (及其他选项)。
NDK 根目录下的 platforms 目录中的各个子目录的路径都可以直接传给 gcc &amp;ndash;sysroot=。
为了简化操作，可以在linux系统的命令终端执行以下命令，设置SYSROOT环境变量，$NDK是ndk的根目录。
$ SYSROOT=$NDK/platforms/android-8/arch-arm
=&amp;gt; 所以sysroot就是要來找header file 還有library的路徑
调用 NDK gcc（第1种方法） 设置 SYSROOT之后，要把它传给 gcc 的 &amp;ndash;sysroot 选项。
由于unix/linux自带的gcc并非交叉编译工具，而我们需要使用的是ndk中提供的交叉编译工具（也是gcc），
所以需要想办法让编译脚本找到ndk中的gcc，而不要去寻找系统中的gcc。
而 unix/linux 系统的编译脚本常常会用 CC 环境变量来引用编译器，
所以通过把 CC 设置为ndk中的gcc的路径，就能帮助编译脚本找到正确的gcc（我们还能顺便加上&amp;ndash;sysroot选项）
=&amp;gt; 所以就是要用NDK內的gcc就對了
将CC 按如下设置
$ export CC=&amp;quot;$NDK/toolchains//prebuilt//bin/gcc &amp;ndash;sysroot=$SYSROOT&amp;rdquo;
$ $CC -o foo.o -c foo.c (不必执行这一行，这条命令是调用gcc编译程序）
调用NDK编译器（第2种方法，更简单） android ndk 提供脚本，允许自己定制一套工具链。例如：
$NDK/build/tools/make-standalone-toolchain.sh &amp;ndash;platform=android-5 &amp;ndash;install-dir=/tmp/my-android-toolchain [ &amp;ndash;arch=x86 ]</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-normalization/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-normalization/readme/</guid>
      <description>Normalization Category  (1) 針對數值正規化 (either input(batch/只在同一個layer or weight or 算式) 來解決Internal Covariate Shift (ICS), (2) ICS的問題在於前面的layer的W會影響到後面的layer的input 因為後面layer input = 前面的data x W &amp;lt;/td&amp;gt;  Normalization vs Regularization normalization和standardization是差不多的， 都是把数据进行前处理，从而使数值都落入到统一的数值范围
normalization一般是把数据限定在需要的范围，比如一般都是【0，1】 standardization 一般是指将数据正态化，使平均值1方差为0
=&amp;gt; 因此normalization和standardization 是针对数据而言的
消除一些数值差异带来的特种重要性偏见。经过归一化的数据，能加快训练速度，促进算法的收敛
regularization是在cost function里面加惩罚项，增加建模的模糊性 从而把捕捉到的趋势从局部细微趋势，调整到整体大概趋势
虽然一定程度上的放宽了建模要求，但是能有效防止over-fitting的问题，增加模型准确性。 =&amp;gt; regularization是针对模型而言
=&amp;gt; 所以regularization不是normalization的方式一種
Normalization vs Standardization  首先归一化是为了后面数据处理的方便， 其次是保正程序运行时收敛加快。一般指将数据限制在[0 1]之间。 (1)把数变为（0,1）之间的数，主要是为了数据处理方便提出来的， 把数据映射到0-1之间处理，更便携快速； (2)把有量纲表达式变为无量纲表达式，成为纯量； (3) 一般采用最大-最小规范化对原始数据进行线性变换：X*=（X-Xmin）/(Xmax-Xmin) &amp;lt;/td&amp;gt;   标准化：对原始数据进行缩放处理，限制在一定的范围内。一般指正态化， 即均值为0，方差为1。即使数据不符合正态分布， 也可以采用这种方式方法，标准化后的数据有正有负。 由于信用指标体系的各个指标度量单位是不同的， 为了能够将指标参与评价计算，需要对指标进行规范化处理， 通过函数变换将其数值映射到某个数值区间 =&amp;gt; 所以不會將數值限縮在某個區間(不一定在0-1) （1）数据同趋化处理：解决不同性质数据问题， 对不同性质指标直接加总不能正确反映不同作用力的综合结果， 须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化， 再加总才能得出正确结果； （2）无量纲化处理：要解决数据的可比性； （3）一般采用Z-score规范化：即均值为0，方差为1的正态分布； &amp;lt;/td&amp;gt;  Why normalization (1) 独立同分布与白化</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-or-tool/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-or-tool/readme/</guid>
      <description>Google Optimization Tools(OR-Tools) 是一款专门快速而便携地解决组合优化问题的套件。它包含了：
(1) 约束编程求解器。
(2) 简单而统一的接口，用于多种线性规划和混合整数规划求解 CBC、CLP、GLOP、GLPK、Gurobi、CPLEX 和SCIP。
The primary OR-Tools &amp;ldquo;linear optimization&amp;rdquo; solver is Glop, Google&amp;rsquo;s linear programming system. (3) 图算法 (最短路径、最小成本、最大流量、线性求和分配)。
(4) 经典旅行推销员问题和车辆路径问题的算法。
(5) 经典装箱和背包算法。
Google使用C++开发了OR-Tools库，但支持Python，C#，或Java语言调用。
Concept Like all optimization problems, this problem has the following elements:
任何問題都會被分為目標&amp;amp;限制
The objective —the quantity you want to optimize. In the example above, the objective is to minimize cost.
To set up an optimization problem, you need to define a function that calculates the value of the objective</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-pruning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-pruning/readme/</guid>
      <description>Table Concept 模型剪枝（Model Pruning）是一种模型压缩方法，
对深度神经网络的稠密连接引入稀疏性，
通过将“不重要”的权值直接置零 =&amp;gt; 来减少非零权值数量
最近比较流行基于幅度的权值剪枝方法【4】，该方法将权值取绝对值，
与设定的 threshhold 值进行比较，低于门限的权值被置零。
基于幅度的权值剪枝算法计算高效，可以应用到大部分模型和数据集。
TensorFlow 也使用了基于幅度的权值剪枝算法
模型训练时剪枝，只需选定需要剪枝的层，对于选中做剪枝的层增加一个二进制掩模（mask）变量，
形状和该层的权值张量形状完全相同。该掩模决定了哪些权值参与前向计算。
掩模更新算法则需要为 TensorFlow 训练计算图注入特殊运算符，对当前层权值按绝对值大小排序，
对幅度小于一定门限的权值将其对应掩模值设为 0。
反向传播梯度也经过掩模，被屏蔽的权值（mask 为 0）在反向传播步骤中无法获得更新量
因為有些weight變成0了,所以會再用一個稀疏矩陣index來存對應的位置 TensorFlow 代码目录 tensorflow/contrib/model_pruning/ 提供了对 TensorFlow 框架的扩展，
=&amp;gt;可在模型训练时实现剪枝。
Concept 2 剪枝就是利用某一个准则对某一组或某一个权值置0
从而达到将网络神经元置0以达到稀疏化网络连接从
而加快整个推理过程及缩小模型大小的迭代过程，这个准则有暴力穷尽组合排忧、
使用对角 Hessian 逼近计算每个权值的重要性、基于一阶泰勒展开的模型代价函数来对权值排序、
基于L1绝对值的权值参数大小进行排序、基于在小验证集上的影响进行分值分配排序等方法，
而某一组或某一个网络权值则可以是整个卷积核、全连接层、卷积核或全连接层上的某个权重参数，
剪枝的目的是将冗余的神经元参数置0减小模型大小(需要特殊的模型存储方式)
减少计算参数（需要某种特殊的硬件计算方式）
稀疏化网络连接加快推理速度，剪枝前后的网络连接对比图如下
Implementation 对每个被选中做剪枝的层增加一个二进制掩模（mask）变量，
形状和该层的权值张量形状完全相同。该掩模决定了哪些权值参与前向计算。
掩模更新算法则需要为 TensorFlow 训练计算图注入特殊运算符，
对当前层权值按绝对值大小排序，对幅度小于一定门限的权值将其对应掩模值设为 0。
反向传播梯度也经过掩模，被屏蔽的权值（mask 为 0）在反向传播步骤中无法获得更新量。
研究发现稀疏度不宜从一开始就设置最大，这样容易将重要的权值剪掉造成无法挽回的准确率损失，
更好的方法是渐进稀疏度，从初始稀疏度 （一般为 0 ）开始，逐步增大到最终稀疏度 ，
这期间二进制掩模变量 mask 经历了 n 次更新，每次更新时的门限由当时的稀疏度决定，稀疏度由如下公式计算得到
初始时刻，稀疏度提升较快，而越到后面，稀疏度提升速度会逐渐放缓，
这个比较符合直觉，因为初始时有大量冗余的权值，而越到后面保留的权值数量越少，</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-python_pandas_data_analysis/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-python_pandas_data_analysis/readme/</guid>
      <description>Concept Pandas 一樣是 python 的 libary，pandas 很像 excel，
但沒有 excel 來得直覺，同樣可以將資料呈現像試算表，在資料分析的角色中。
數據分析大多數時間都在做資料整理，基本上用 Pandas 整理資料，用 Numpy、SkLearn 等…計算分析
Pandas 重要的兩個概念:
 Series (像是list、array，只是每欄有名稱) DataFrame (像是Excel的試算表表格)  Step 1: Read File (CSV) 既然要處理資料，必定要有資料來源，Pandas 提供讀入 CSV 的好方法，
相較於使用 Python 原生的讀入方式更便利許多，隨後會介紹差異。 以下程式可以讀入範例 CSV，由於此範例有 100(行) * 6(欄) 資料，
相當的多，所以使用 df.head() 僅讀出前 5 個資料內容。
df = pd.read_csv(&amp;ldquo;weather_2012.csv&amp;rdquo;) print(df.head())
Step 2: Get Data (part of CSV) CSV中有這些欄位
 Date/Time Temp (C) Dew Point Temp (C) : 露點溫度是在固定氣壓之下，空氣中所含的氣態水達到飽和而凝結成液態水所需要降至的溫度 Rel Hum (%) Wind Spd (km/h) Visibility (km) Stn Press (kPa) Weather 可以只dump出特定欄位的資料</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-quantize/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-quantize/readme/</guid>
      <description>Summary  重點還是fakequant 但有分(1) by script -&amp;gt; 可以進行run inference,更新activation min/max 或者是(2) by tool -&amp;gt; by default range直接轉成fakequant_pb https://github.com/evansin100/Quantize/blob/master/ direct%20quant/README.md &amp;lt;/td&amp;gt;   (0) 依然是透過fake quant機制 (1) fakequant會在re-train過程中,更新min/max ..etc (2) 然後weight也可以做對應的調整來提高accuracy =&amp;gt; range auto update https://github.com/evansin100/Quantize/blob/master/ fake_quantization/default_ranges_min,max/README.md =&amp;gt; fake quant細節 https://github.com/evansin100/Quantize/tree/master/fake_quantization &amp;lt;/td&amp;gt;  Key Example 建議必看這個link
 (1) 來了解training產生checkpoint (2) 然後自己建構eval graph(避免有多餘的training node)  quant model除了原本的model圖之外 還要再透過tf.contrib.quantize.create_eval_graph()把min/max加進去   (3) 在freeze pb file (4) 最後再優化產生 tflite https://github.com/evansin100/SW-FRAMEWORK-Tensorflow/tree/master/Training/Training-with-Slim/Example/mobilenetv2_quant  Quantization Concept TensorFlow的量化是通過將&amp;quot;預測的操作轉換成等價的8位版本&amp;quot;的操作來實現</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-regularization/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-regularization/readme/</guid>
      <description>Concept  Normalisation adjusts the data regularisation adjusts the prediction function. =&amp;gt; 所以regularisation不是要處理data,而是改動loss function增加他的模糊性  Regularisation imposes some control on this by rewarding simpler fitting functions over complex ones
=&amp;gt; 所以通常就是在loss function後面加個penality function,來解決overfitting問題 For instance, it can promote that a simple log function with a RMS error of x is preferable to a 15th-degree polynomial with an error of x/2. Tuning the trade-off is up to the model developer: if you know that your data are reasonably smooth in reality, you can look at the output functions and fitting errors, and choose your own balance.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-view_model/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-view_model/readme/</guid>
      <description>Summary  (1) tensorflow/tensorflow/python/tools/inspect_checkpoint.py (2) 檔案列印輸出模型中所有張量(tensor)和操作(op)的名稱 &amp;lt;/td&amp;gt;   (1) pywrap_tensorflow.NewCheckpointReader(&amp;quot;example.ckpt&amp;quot;) (2) 範例https://github.com/evansin100/Tensorflow/tree/ master/Training/Re-training-from-ckpt &amp;lt;/td&amp;gt;   (0) https://github.com/evansin100/Tensorflow/tree/master/summarize_graph (1) 可以找到input tensor (2) 可以找到可能的output tensor (3) 可以印出有多少參數(weight)-const parameter,以及是否有control edge (4) 使用到那些OPs &amp;lt;/td&amp;gt;   (0) https://github.com/evansin100/Tensorflow/tree/master/ Example/regression_tensorboard/view_frozen_graph (1) 需要event(可以透過script由frozen model轉出來) (2) 可以看到很細的內容 &amp;lt;/td&amp;gt;   (0) https://github.com/evansin100/TFLite/tree/master/Benchmark-Tool (1) 可以用來測試model(gen input) (2) --enable_op_profiling=true 可以印出OP的執行速度 and op summary (3) 因為是tflite,所以有Android version and desktop version &amp;lt;/td&amp;gt;   (0) https://github.com/evansin100/TFLite/tree/master/view_frozen_graph(tflite) (1) sudo bazel run tensorflow/lite/tools:visualize (2) 因為tflite不能使用tensorboard,所以只能用這個才能達到類似功能 (3) 可以看每個中間buffer的大小 (4) 這個網頁是互動式的,只要點了input他還會和你說input的名稱 (5) 也可以印出是那個版本的TOCO &amp;lt;/td&amp;gt;  </description>
    </item>
    
  </channel>
</rss>