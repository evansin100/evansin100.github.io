<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TOOL-Compression on Memo</title>
    <link>https://evansin100.github.io/categories/tool-compression/</link>
    <description>Recent content in TOOL-Compression on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/tool-compression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/tool-compression/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/tool-compression/readme/</guid>
      <description>Summary https://arxiv.org/pdf/1710.09282.pdf
有summarize幾種model compression方法
但其實還可以再補充
 (1) Concept: Reducing redundant parameters which are not sensitive to the performance (2) APP: Convolutional layer and fully connected layer (3) Details: Robust to various settings, can achieve good performance, can support both train from scratch and pre-trained model &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Pruning - Weight sparcity &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) skip 掉0的 filter,把某些filter砍掉, 這個也是 pruning &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Activation sparcity &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) skip 掉0的 activation (因為Relu會產生大量的0) Accelerating Convolutional Neural Networks via Activation Map Compression &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Low-rank factorization &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Using matrix/tensor decomposition to estimate the informative parameters (2) APP: Convolutional layer and fully connected layer (3) Details: Standardized pipeline, easily to be implemented, can support both train from scratch and pre-trained model &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Transferred/compact convolutional filters &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Designing special structural convolutional filters to save parameters (2) APP: Convolutional layer only (3) Details: Algorithms are dependent on applications usually achieve good performance, only support train from scratch &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Knowledge distillation &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) Concept: Training a compact neural network with distilled knowledge of a large model (2) APP: Convolutional layer and fully connected layer (3) Details: Model performances are sensitive to applications and network structure only support train from scratch &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt; Quantization &amp;lt;/td&amp;gt; &amp;lt;td&amp;gt; note (1) model的量化 或者是mix-precision &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt;  </description>
    </item>
    
  </channel>
</rss>