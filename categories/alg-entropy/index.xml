<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ALG-Entropy on Memo</title>
    <link>https://evansin100.github.io/categories/alg-entropy/</link>
    <description>Recent content in ALG-Entropy on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/alg-entropy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-entropy/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-entropy/readme/</guid>
      <description>熵 - Entropy 词源 - 最初来源于热力学
Entropy来源于希腊语，原意：内向，
即：一个系统不受外部干扰时往内部稳定状态发展的特性。
定义的其实是一个热力学的系统变化的趋势 1923年，德国科学家普朗克来中国讲学用到entropy这个词， 胡刚复教授看到这个公式，创造了“熵”字，因为“火”和热量有关，定义式又是热量比温度，相当自洽
是接受的每条消息中包含的信息的平均值。又被称为信息熵、信源熵、平均自信息量。
可以被理解为不确定性的度量，熵越大，信源的分布越随机
广义的定义
熵是描述一个系统的无序程度的变量；
同样的表述还有，熵是系统混乱度的度量，
一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行
Summary  (1) 信息熵(information entropy) 是衡量随机变量分布的混乱程度，  是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。 当随机分布为均匀分布时，熵最大；信息熵推广到多维领域，则可得到联合信息熵；条件熵表示的是在 X 给定条件下，Y 的条件概率分布的熵对 X的期望。   (2) 相对熵(relative entropy - KLD) 可以用来衡量两个概率分布之间的差异。 (3) 交叉熵(cross entropy)  可以来衡量在给定的&amp;quot;真实分布&amp;rdquo;(NN的golden)下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。    </description>
    </item>
    
  </channel>
</rss>