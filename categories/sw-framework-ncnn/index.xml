<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SW-FRAMEWORK-ncnn on Memo</title>
    <link>https://evansin100.github.io/categories/sw-framework-ncnn/</link>
    <description>Recent content in SW-FRAMEWORK-ncnn on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/sw-framework-ncnn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/sw-framework-ncnn/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/sw-framework-ncnn/readme/</guid>
      <description>Concept (for inference) https://github.com/Tencent/ncnn ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。
ncnn 从设计之初深刻考虑手机端的部署和使用。
无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。
基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，
开发出人工智能 APP，将 AI 带到你的指尖。
ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。
所以這個應該專做device端inference
功能
•支持卷积神经网络，支持多输入和多分支结构，可计算部分分支 •无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架
•纯 C++ 实现，跨平台，支持 android ios 等
•ARM NEON 汇编级良心优化，计算速度极快 •精细的内存管理和数据结构设计，内存占用极低
•支持多核并行计算加速，ARM big.LITTLE cpu 调度优化
•整体库体积小于 500K，并可轻松精简到小于 300K
•可扩展的模型设计，支持 8bit 量化和半精度浮点存储，可导入 caffe 模型
•支持直接内存零拷贝引用加载网络模型
•可注册自定义层实现并扩展
=&amp;gt; 所以特點是很小,用c++實作,對ARM CPU有優化</description>
    </item>
    
  </channel>
</rss>