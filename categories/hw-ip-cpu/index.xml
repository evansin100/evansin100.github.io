<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HW-IP-CPU on Memo</title>
    <link>https://evansin100.github.io/categories/hw-ip-cpu/</link>
    <description>Recent content in HW-IP-CPU on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/hw-ip-cpu/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/hw-ip-cpu/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/hw-ip-cpu/readme/</guid>
      <description>Comparison  (1) 一般CPU vector width=128 or 64 bit(這個是硬體能力 (2) 128bit, 所以可以分 8(8bit) x 16, 16(fp16) x 8, 32(fp32) x 4 (3) 64bit, 所以可以分 8(8bit) x 8, 16(fp16) x 4, 32(fp32) x 2 (4) 注意fp16只有ARMv8.2才有指令集support   (0) Summary 8bit 硬體加速 =&amp;gt; X fp16 硬體加速 =&amp;gt; X fp32 硬體加速 =&amp;gt; O (FMLA instruction) (1) 支援FMLA for FP32運算, vector width=128 case =&amp;gt; 32(fp32)x4 等於一次處理4個資料 (2) FMA and FMLA 差異 兩個都是用來算floating point 相乘, 但FMA是包裝好的API(intrinsic),所以會被展開instruction　double fma(double a, double b, double c); 而FMLA是insruction是最底層的instruction (3) 沒有針對8bit and fp16 做硬體加速   (0) Summary 8bit 硬體加速 =&amp;gt; O (UDOT/SDOT instruction) fp16 硬體加速 =&amp;gt; O (FMLA instruction) fp32 硬體加速 =&amp;gt; O (FMLA instruction) (1) 針對8bit做加速 引進UDOT,SDOT指令集(8bit pack在一起) (2) 新增FMLA for FP16運算, 要加compile option -march=armv8.</description>
    </item>
    
  </channel>
</rss>