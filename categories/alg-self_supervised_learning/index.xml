<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ALG-Self_supervised_learning on Memo</title>
    <link>https://evansin100.github.io/categories/alg-self_supervised_learning/</link>
    <description>Recent content in ALG-Self_supervised_learning on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/alg-self_supervised_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-self_supervised_learning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-self_supervised_learning/readme/</guid>
      <description>supervised learning (limitation: label) 自監督學習能突破現有深度學習方法的侷限。
比如說，最廣泛使用的監督式學習（supervised learning），
是由人給定標記好的資料，讓機器學習正確答案並作為推論根據。
但是，這種學習方法是立基於人的標記，不僅資料標記過程需要花費大量時間與資源，
機器也只能根據已標記的特徵來學習，完成指定的任務，如語音轉文字、分類圖像、物件辨識等。
reinforced learning (limitation: lots of trials) 強化學習（Reinforced Learning），是透過獎勵與懲罰的機制，
讓機器在虛擬情境中不斷試錯（trial and error），累積經驗來學習。
這種學習方式雖然在競技比賽裡表現良好、甚至能勝過人類，但學習效率極低。
舉例來說，人類在15分鐘內能領略的任一款Atari遊戲，機器卻平均要花83小時才能學會，
在臉書研發的虛擬圍棋遊戲ELF OpenGo中，更要用2000個GPU訓練14天，
更別提要訓練200年才學得會的星海爭霸遊戲（StarCraft）。
而且，強化學習並不能永遠在虛擬場景訓練，一旦進到真實世界，
所有試錯的過程將會帶來高成本的代價。比如說，在自駕車了解前面是懸崖要轉彎之前，
可能需要先掉下去幾百次，且不同於虛擬世界可以無間斷的循環訓練，
在真實世界中花費的訓練時間只會更長；更何況，人類學習過程只需極少數「試錯」的過程，
比如在看到前方的懸崖之後，常識就會使我們轉
self-supervised learning 自監督學習能解決這個問題。比起強化學習是從試錯的經驗中學習，
自監督學習是建構一個龐大的神經網絡，透過預測來認識世界。
換句話說，自監督學習所訓練的模型，能藉由觀察過去、當下所有的訓練資料，來預測下一刻會發生的事情 因此，在預測到車子將會摔落懸崖時，就能提前轉彎來避免。「就像人類是不斷透過已知的部分來預測未知，
看到一半的人臉會自動在腦海補足另一半畫面，所以自監督學習是更接近人類學習行為的方法。」
Facebook Yann LeCun
認為人腦是透過觀察來學習的
所以AI也是要透過類似的方式
Self-supervised learning可以做到的是由input來觀察,併進行自己思考,
像是BERT順練的方式,把input句子中的某一塊蓋掉,再訓練自己猜這個蓋掉的詞是什麼 =&amp;gt; 經驗學習
然後在inference的時候就可以input完整句子,併產生句子後接的詞 =&amp;gt; 預測未來</description>
    </item>
    
  </channel>
</rss>