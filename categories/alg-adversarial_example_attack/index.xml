<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ALG-adversarial_example_attack on Memo</title>
    <link>https://evansin100.github.io/categories/alg-adversarial_example_attack/</link>
    <description>Recent content in ALG-adversarial_example_attack on Memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	<atom:link href="https://evansin100.github.io/categories/alg-adversarial_example_attack/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://evansin100.github.io/post/alg-adversarial_example_attack/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://evansin100.github.io/post/alg-adversarial_example_attack/readme/</guid>
      <description>Concept 利用對抗例（adversarial example）造成深度學習模型判斷錯誤這種攻擊手法
所謂對抗例，是一種刻意製造的、讓機器學習模型判斷錯誤的輸入資料。
最早是 Szegedy et al（2013）[2]發現對於用 ImageNet、AlexNet 等資料集訓練出來的影像辨識模型，
常常只需要輸入端的微小的變動，就可以讓輸出結果有大幅度的改變。
例如取一張卡車的照片，可以被模型正確辨識，但只要改變影像中的少數像素，
就可以讓模型辨識錯誤，而且前後對影像的改變非常少，對肉眼而言根本分不出差異
=&amp;gt; 也可以讓accuracy大幅度提升
這樣的例子除了顯現深度學習模型可能結果「不穩定」，而且有可能被惡意利用而有安全疑慮，
因此近年來已經成為機器學習熱門的研究議題。2017 年這方面發表的研究成果，
主要有加強防禦，以及更快速簡單地產生對抗例兩個方向。
Definition Adversarial examples are inputs to a neural network that result in an incorrect output from the network.
This is because neural networks are extremely susceptible to something called adversarial examples.
Example 1 It’s probably best to show an example.
You can start with an image of a panda on the left which some network thinks with 57.</description>
    </item>
    
  </channel>
</rss>