<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - Concept Medical images like MRIs, CTs (3D images) are very similar to videos -
both of them encode 2D spatial information over a 3rd dimension.
Much like diagnosing abnormalities from 3D images, action recognition from videos would require capturing context
from entire video rather than just capturing information from each frame
Challenges Action recognition task involves the identification of different actions
from video clips (a sequence of 2D frames) where the action may or may not be" name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/alg-video_ai/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">ALG-Video_AI</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="concept">Concept</h1>
<p>Medical images like MRIs, CTs (3D images) are very similar to videos -<br>
both of them encode 2D spatial information over a 3rd dimension.<br>
Much like diagnosing abnormalities from 3D images, <br>
action recognition from videos would require capturing context<br>
from entire video rather than just capturing information from each frame<br>
<img src="Selection_497.png" alt="image"></p>
<h1 id="challenges">Challenges</h1>
<p>Action recognition task involves the identification of different actions<br>
from video clips (<strong>a sequence of 2D frames</strong>) where the action may or may not be<br>
performed throughout the entire duration of the video.</p>
<p>This seems like a natural extension of image classification tasks to multiple frames<br>
and then aggregating the predictions from each frame. <br>
Despite the stratospheric success of deep learning architectures in image classification (ImageNet), <br>
progress in architectures for video classification and representation learning has been slower.</p>
<ul>
<li>Huge Computational Cost
<ul>
<li>A simple convolution 2D net for classifying 101 classes has just ~5M parameters</li>
<li>whereas the same architecture when inflated to a 3D structure results in ~33M parameters</li>
</ul>
</li>
<li>Capturing long context
<ul>
<li>Action recognition involves capturing spatiotemporal(時空的) context across frames.</li>
<li>Additionally, the spatial information captured has to be compensated for camera movement.</li>
<li>Even having strong spatial object detection doesn’t suffice as the motion information also carries finer details.</li>
<li>There’s a local as well as global context w.r.t. motion information which needs to be captured for robust predictions.</li>
<li>For example, consider the video representations shown in Figure 2.</li>
<li>A strong image classifier can identify human,</li>
<li>water body in both the videos but the nature of temporal periodic action differentiates front crawl from breast stroke</li>
</ul>
</li>
<li>Designing classification architectures
<ul>
<li>Designing architectures that can capture spatiotemporal information</li>
<li>involve multiple options which are non-trivial and expensive to evaluate.</li>
<li>For example, some possible strategies could be</li>
<li>(1) One network for capturing spatiotemporal information vs. two separate ones for each spatial and temporal</li>
<li>(2) Fusing predictions across multiple clips</li>
<li>(3) End-to-end training vs. feature extraction and classifying separately</li>
</ul>
</li>
</ul>
<h1 id="approach-1-single-stream-network">Approach 1: Single Stream Network</h1>
<p>the consecutive frames of the video are presented as input in all setups<br>
=&gt; The learnt spatiotemporal(空間時間的來源) features didn’t capture motion features<br>
<img src="Selection_498.png" alt="image"></p>
<h1 id="approach-2-two-stream-networks">Approach 2: Two Stream Networks</h1>
<p>one for spatial context (pre-trained), one for motion context <br>
=&gt; The method involved pre-computing <strong>optical flow</strong> vectors <br>
<img src="Selection_499.png" alt="image"></p>
<h1 id="approach-3-3d-convnet">Approach 3: 3D ConvNet</h1>
<p>Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification<br>
和approach1的差異是多了TTL layer(Temporal Transition Layer), 所以還是有時間概念<br>
三维卷积用于行为识别<br>
=&gt; 所以是輸入一串圖片然後做辨識<br>
该文章提出了一种基于densenet改进得到的三维卷积网络结构（Temporal 3D ConvNets），<br>
同时提出了一种迁移学习方法使三维卷积网络能够得到更好的初始化</p>
<p><strong>Model</strong><br>
实现方式</p>
<ul>
<li>(1) 三维卷积主要是在densenet的基础上改进得到，
<ul>
<li>具体来说就是将原始网络中二维卷积修改为三维卷积，</li>
<li>二维pooling修改为三维pooling；</li>
</ul>
</li>
<li>(2) 自主要的创新还是来源于Temporal Transition Layer（TTL）层，
<ul>
<li>TTL层包含几个不同大小和时间域深度的卷积kernel和三维pooling层构成，</li>
<li>所希望达到的效果是能够对短、中、长三个不同的时间长度的序列信息进行建模。<br>
<img src="Selection_500.png" alt="image"></li>
</ul>
</li>
</ul>
<p><strong>Training</strong>
考虑到三维卷积网络的参数较多，训练难度大，<br>
作者提出了一种迁移学习方法来将二维卷积网络的知识迁移到三维卷积网络中，具体实现方式如下：<br>
二维卷积网络使用的是在ImageNet上pre-train之后的DenseNet模型，三维卷积网络的权值是完全随机初始化的；
两个网络输入的都是视频序列图片，二维网络是将视频序列的单帧图片依次输入，将网络是最后的2D的fc特征取平均，得到1024-d的特征；<br>
同时三维卷积网络直接送入序列图片得到3D的fc特征，也同样是1024-d的特征；<br>
将以上两个特征concantenate之后送入两层全连接层进行分类，判断这两个网络输入的视频是否一致<br>
训练过程中二维网络权值不更新，这样在训练过程中不需要得到视频的label，只需要知道送入两个网络的视频是不是同一个即可，<br>
也即希望三维网络在训练过程中不断具备对序列视频的特征提取能力<br>
=&gt; 所以是用已經訓練好的2D網路來訓練3D網路    <br>
<img src="Selection_501.png" alt="image"></p>
<h1 id="another-comparison-for-video-understanding">Another Comparison for video understanding</h1>
<!-- raw HTML omitted -->
<pre><code>        dominant approach for video understanding
        (1) highly successful two-stream networks
        (2) 3D convolutional networks
</code></pre>
<!-- raw HTML omitted -->
<pre><code>        看時間關係            
        include RNNs that model the evolution of video frames
        and multilayer perceptrons that model ordered frame features 
</code></pre>
<!-- raw HTML omitted -->
<pre><code>        with modern CNNs is
        less explored, in part due to GPU memory constraints
        解法 (1) use precomputed features without end-to-end training
        解法 (2) aggressive subsampling or large striding (就是點少一點)
</code></pre>
<!-- raw HTML omitted -->
<pre><code>        就是延伸原本的2D object detection方式
        (1) propose tubelets/boxes in a short clip/frame
        (2) classify the tubelets/boxes into action classes 
</code></pre>
<!-- raw HTML omitted -->
<pre><code>        資料庫的概念
        object bank   
        detection bank 
        memory networks
</code></pre>
<!-- raw HTML omitted -->
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>
</body>
</html>
