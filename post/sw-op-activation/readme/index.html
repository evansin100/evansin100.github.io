<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-167528382-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - Purpose 使用&quot;激勵函數&quot;的目的
懶人包：激勵函數主要作用是引入非線性
在類神經網路中如果不使用激勵函數，那麼在類神經網路中皆是以上層輸入的線性組合作為這一層的輸出
（也就是矩陣相乘），輸出和輸入依然脫離不了線性關係，做深度類神經網路便失去意義 =&gt; 所以就是要讓數值比較亂一點就是了
Requirement (1)非線性(not y=ax linear function),(2)可微分 (因為要可以訓練)
只要滿足這兩個就算activation function
Comparison  sigmoid函数也叫Logistic函数 然而Sigmoid存在著三大缺點: A. 容易出現梯度消失gradient vanishing (上面有介紹) B. 函數輸出並不是zero-centered : =&gt; 因為產生的值 當x=0(input)的時候,output不是0,而是約等於0.4多 C. 指數運算較為耗時 &lt;/td&gt;   這也是個常用於分類問題的activation function，輸出範圍介於[-1, 1] ，輸出範圍會有正有負，也是個嚴格遞增函數，他的微分是f&#39;(x) = 1 - f^2(x)。 實際應用上跟sigmoid function差不多，tanh收斂到1跟-1的速度比較快， 所以容易學的比較慢一些，相對sigmoid function的學習表現也不是非常好 &lt;/td&gt;   1. 梯度消失問題 (vanishing gradient problem) 對使用反向傳播訓練的類神經網絡來說，梯度的問題是最重要的， 使用 sigmoid 和 tanh 函數容易發生梯度消失問題，是類神經網絡加深時主要的訓練障礙。 具體的原因是這兩者函數在接近飽和區 (如sigmoid函數在 [-4, &#43;4] 之外)， 求導後趨近於0，也就是所謂梯度消失， 造成更新的訊息無法藉由反向傳播傳遞 2. 類神經網路的稀疏性（奧卡姆剃刀原則） Relu會使部分神經元的輸出為0，可以讓神經網路變得稀疏，緩解過度擬合的問題 3." name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167528382-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());
          gtag('config', 'UA-167528382-1');
        </script>
    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<h5 id="wc" style="font-size: 1rem;text-align: center;">200 Words|Read in about 1 Min|total read<span id="busuanzi_value_page_pv"></span></h5>

<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/sw-op-activation/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">SW-OP-Activation</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="purpose">Purpose</h1>
<p>使用&quot;激勵函數&quot;的目的<br>
懶人包：激勵函數主要作用是引入非線性<br>
在類神經網路中如果不使用激勵函數，那麼在類神經網路中皆是以上層輸入的線性組合作為這一層的輸出<br>
（也就是矩陣相乘），輸出和輸入依然脫離不了線性關係，做深度類神經網路便失去意義 <br>
=&gt; 所以就是要讓數值比較亂一點就是了</p>
<h1 id="requirement">Requirement</h1>
<p>(1)非線性(not y=ax linear function),(2)可微分 (因為要可以訓練)<br>
只要滿足這兩個就算activation function</p>
<h1 id="comparison">Comparison</h1>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>   sigmoid函数也叫Logistic函数   
   
   然而Sigmoid存在著三大缺點:
   A. 容易出現梯度消失gradient vanishing (上面有介紹)
   B. 函數輸出並不是zero-centered : 
   =&gt; 因為產生的值 當x=0(input)的時候,output不是0,而是約等於0.4多      
   C. 指數運算較為耗時
 &lt;/td&gt;    
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>     這也是個常用於分類問題的activation function，輸出範圍介於[-1, 1]
     ，輸出範圍會有正有負，也是個嚴格遞增函數，他的微分是f'(x) = 1 - f^2(x)。
     實際應用上跟sigmoid function差不多，tanh收斂到1跟-1的速度比較快，
     所以容易學的比較慢一些，相對sigmoid function的學習表現也不是非常好  
 &lt;/td&gt;    
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>     1. 梯度消失問題 (vanishing gradient problem)
     對使用反向傳播訓練的類神經網絡來說，梯度的問題是最重要的，  
     使用 sigmoid 和 tanh 函數容易發生梯度消失問題，是類神經網絡加深時主要的訓練障礙。
     具體的原因是這兩者函數在接近飽和區 (如sigmoid函數在 [-4, +4] 之外)，
     求導後趨近於0，也就是所謂梯度消失，
     造成更新的訊息無法藉由反向傳播傳遞
     
     2. 類神經網路的稀疏性（奧卡姆剃刀原則）
     Relu會使部分神經元的輸出為0，可以讓神經網路變得稀疏，緩解過度擬合的問題
     
     3. 生物事實：全有全無律 (all or none law)
     在神經生理方面，當刺激未達一定的強度時，神經元不會興奮，因此不會產生神經衝動。
     如果超過某個強度，才會引起神經衝動。Relu比較好的捕捉了這個生物神經元的特徵 
     =&gt; 所以就是小於0的其實不用太管他   
     
     4. Relu 計算量小，只需要判斷輸入是否大於0，不用指數運算
     
     
     gradient特性不同。sigmoid和tanh的gradient在饱和区域非常平缓，接近于0，
     很容易造成vanishing gradient的问题，减缓收敛速度。
     vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。
     =&gt;所以訓練會慢  
     相反，Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题。
     Relu的另一个优势是在生物上的合理性，它是单边的，相比sigmoid和tanh，
     更符合生物神经元的特征。而提出sigmoid和tanh，主要是因为它们全程可导。
     还有表达区间问题，sigmoid和tanh区间是0到1，或着-1到1，
     在表达上，尤其是输出层的表达上有优势
     =&gt; Relu產生數值變義大,也變化性高,訓練快    
 &lt;/td&gt;    
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>     有兩種實作方式
     (1) 人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为 0.01x而非0。
     (2) 另外一种直观的想法是基于参数的方法，即Parametric ReLU:f(x) = max(ax,x)  
         其中a可由back propagation学出来。
     理论上来讲，Leaky ReLU有ReLU的所有优点，
     外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU    
 &lt;/td&gt;    
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>     GELUs正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，
     直观上更符合自然的认识，同时实验效果要比Relus与ELUs都要好
     GELUs其实是 dropout、zoneout、Relus的综合，
     GELUs对于输入乘以一个0,1组成的mask，
     而该mask的生成则是依概率随机的依赖于输入。假设输入为X, mask为m
 &lt;/td&gt;    
</code></pre>
<!-- raw HTML omitted -->
<p><img src="Selection_013.png" alt="image"></p>
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <span id="busuanzi_container_site_pv">
        total vistor：<span id="busuanzi_value_site_pv"></span>
    </span>
    &nbsp;
    <span id="busuanzi_container_site_uv">
        you are <span id="busuanzi_value_site_uv"></span> th visitor
    </span>

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>

</body>
</html>
