<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - problem TensorFlow 生态系统包含许多编译器和优化器，可在多个级别的软硬件堆栈上运行。
作为 TensorFlow 的日常用户，在使用不同种类的硬件（GPU、TPU、移动设备）时，
这种多级别堆栈可能会表现出令人费解的编译器和运行时错误
所以可以從下圖看到,因為接的硬體不同,過程中經過的opt等等也都不同
如图中所示，TensorFlow 图 [1]能够以多种不同的方式运行。这包括：
(1) 将其发送至调用手写运算内核的 TensorFlow 执行器
(2) 将图转化为 XLA 高级优化器 (XLA HLO) 表示，反之，这种表示亦可调用适合 CPU 或 GPU 的 LLVM 编辑器，
或者继续使用适合 TPU 的 XLA。（或者将二者结合！）
(3) 将图转化为 TensorRT、nGraph 或另一种适合特定硬件指令集的编译器格式
(4) 将图转化为 TensorFlow Lite 格式，然后在 TensorFlow Lite 运行时内部执行此图，
或者通过 Android 神经网络 API (NNAPI) 或相关技术将其进一步转化，以在 GPU 或 DSP 上运行
虽然这些编译器和表示的大量实现可显著提升性能，但这种异构的环境可能会给最终用户带来问题，
例如在这些系统间的边界处产生令人困惑的错误消息。
此外，若需要构建新的软硬件堆栈生成器，则必须为每个新路径重新构建优化与转换传递=&gt;代表opt不能reuse MLIR (Multi-Level Intermediate Representation Overview) MLIR（或称为多级别中介码）。这是一种表示格式和编译器实用工具库，
介于模型表示和低级编译器/执行器（二者皆可生成硬件特定代码）之间
MLIR 深受LLVM的影响，并不折不扣地重用其许多优秀理念。MLIR 拥有灵活的类型系统， 可在同一编译单元中表示、分析和转换结合多层抽象的图。
这些抽象包括 TensorFlow 运算、嵌套的多面循环区域乃至 LLVM 指令和固定的硬件操作及类型" name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/compiler-framework-mlir/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">COMPILER-FRAMEWORK-MLIR</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="problem">problem</h1>
<p>TensorFlow 生态系统包含许多编译器和优化器，可在多个级别的软硬件堆栈上运行。<br>
作为 TensorFlow 的日常用户，在使用不同种类的硬件（GPU、TPU、移动设备）时，<br>
这种多级别堆栈可能会表现出令人费解的编译器和运行时错误</p>
<p>所以可以從下圖看到,因為接的硬體不同,過程中經過的opt等等也都不同<br>
<img src="https://github.com/evansin100/MLIR/blob/master/Selection_002.png" alt="image"></p>
<p>如图中所示，TensorFlow 图 [1]能够以多种不同的方式运行。这包括：<br>
(1) 将其发送至调用手写运算内核的 TensorFlow 执行器<br>
(2) 将图转化为 XLA 高级优化器 (XLA HLO) 表示，反之，这种表示亦可调用适合 CPU 或 GPU 的 LLVM 编辑器，<br>
或者继续使用适合 TPU 的 XLA。（或者将二者结合！）<br>
(3) 将图转化为 TensorRT、nGraph 或另一种适合特定硬件指令集的编译器格式<br>
(4) 将图转化为 TensorFlow Lite 格式，然后在 TensorFlow Lite 运行时内部执行此图，<br>
或者通过 Android 神经网络 API (NNAPI) 或相关技术将其进一步转化，以在 GPU 或 DSP 上运行<br>
虽然这些编译器和表示的大量实现可显著提升性能，但这种异构的环境可能会给最终用户带来问题，<br>
例如在这些系统间的边界处产生令人困惑的错误消息。</p>
<h4 id="此外若需要构建新的软硬件堆栈生成器则必须为每个新路径重新构建优化与转换传递代表opt不能reuse">此外，若需要构建新的软硬件堆栈生成器，则必须为每个新路径重新构建优化与转换传递=&gt;代表opt不能reuse</h4>
<h1 id="mlir-multi-level-intermediate-representation-overview">MLIR (Multi-Level Intermediate Representation Overview)</h1>
<p>MLIR（或称为多级别中介码）。这是一种表示格式和编译器实用工具库，<br>
介于模型表示和低级编译器/执行器（二者皆可生成硬件特定代码）之间</p>
<h5 id="mlir-深受llvm的影响并不折不扣地重用其许多优秀理念mlir-拥有灵活的类型系统">MLIR 深受LLVM的影响，并不折不扣地重用其许多优秀理念。MLIR 拥有灵活的类型系统，</h5>
<p>可在同一编译单元中表示、分析和转换结合多层抽象的图。<br>
这些抽象包括 TensorFlow 运算、嵌套的多面循环区域乃至 LLVM 指令和固定的硬件操作及类型</p>
<p>The MLIR project aims to define a common intermediate representation (IR)<br>
that will unify the infrastructure required to execute high performance machine learning models <br>
in TensorFlow and similar ML frameworks.</p>
<p>This project will include the application of HPC techniques,<br>
along with integration of search algorithms like reinforcement learning.<br>
This project aims to reduce the cost to bring up new hardware,<br>
and improve usability for existing TensorFlow users.</p>
<p>Note that this repository contains the core of the MLIR framework.<br>
The TensorFlow compilers we are building on top of MLIR will be part of the main TensorFlow repository soon</p>
<h1 id="more-resources">More resources</h1>
<p>For more information on MLIR, please see:<br>
The MLIR draft specification, which describes the IR itself.<br>
The MLIR rationale document, covering motivation behind some decisions.<br>
Previous external talks.<br>
Join the MLIR mailing list to hear about announcements and discussions. <br>
Please be mindful of the TensorFlow Code of Conduct, which pledges to foster an open and welcoming environment.</p>
<h1 id="what-is-mlir-for">What is MLIR for?</h1>
<p>MLIR is intended to be a hybrid IR which can support multiple different requirements <br>
in a unified infrastructure. For example, this includes:</p>
<h5 id="1-the-ability-to-represent-all-tensorflow-graphs">(1) The ability to represent all TensorFlow graphs,</h5>
<h5 id="including-dynamic-shapes-the-user-extensible-op-ecosystem-tensorflow-variables-etc">including dynamic shapes, the user-extensible op ecosystem, TensorFlow variables, etc.</h5>
<p>=&gt; 所以是graph-based IR</p>
<h5 id="2-optimizations-and-transformations-typically-done-on-a-tensorflow-graph-eg-in-grappler">(2) Optimizations and transformations typically done on a TensorFlow graph, e.g. in Grappler.</h5>
<h5 id="3-quantization-and-other-graph-transformations-done-on-a-tensorflow-graph-or-the-tf-lite-representation">(3) Quantization and other graph transformations done on a TensorFlow graph or the TF Lite representation.</h5>
<h5 id="4-representation-of-kernels-for-ml-operations-in-a-form-suitable-for-optimization">(4) Representation of kernels for ML operations in a form suitable for optimization.</h5>
<h5 id="5-ability-to-host-high-performance-computing-style-loop-optimizations-across-kernels">(5) Ability to host high-performance-computing-style loop optimizations across kernels</h5>
<h5 id="fusion-loop-interchange-tiling-etc-and-to-transform-memory-layouts-of-data">(fusion, loop interchange, tiling, etc) and to transform memory layouts of data.</h5>
<h5 id="6-code-generation-lowering-transformations-such-as-dma-insertion">(6) Code generation &ldquo;lowering&rdquo; transformations such as DMA insertion,</h5>
<h5 id="explicit-cache-management-memory-tiling-and-vectorization-for-1d-and-2d-register-architectures">explicit cache management, memory tiling, and vectorization for 1D and 2D register architectures.</h5>
<h5 id="7-ability-to-represent-target-specific-operations-eg-the-mxu-on-tpus">(7) Ability to represent target-specific operations, e.g. the MXU on TPUs.</h5>
<p>MLIR is a common IR that also supports hardware specific operations.<br>
Thus, any investment into the infrastructure surrounding MLIR<br>
(e.g. the compiler passes that work on it) should yield good returns;<br>
many targets can use that infrastructure and will benefit from it.</p>
<p>MLIR is a powerful representation, but it also has non-goals.<br>
We do not try to support low level machine code generation algorithms<br>
(like register allocation and instruction scheduling).<br>
They are a better fit for lower level optimizers (such as LLVM).<br>
Also, we do not intend MLIR to be a source language that<br>
end-users would themselves write kernels in (analogous to CUDA C++).<br>
While we would love to see a kernel language happen someday,<br>
that will be an independent project that compiles down to MLIR.</p>
<h1 id="mlir-with-vendor">MLIR with vendor</h1>
<p>为区分不同的硬件与软件受众，MLIR 提供 “方言(IR)”，其中包括：<br>
(1) TensorFlow IR，代表 TensorFlow 图中可能存在的一切<br>
(2) XLA HLO IR，旨在利用 XLA 的编译功能（输出到 TPU 等）<br>
(3) 实验性仿射方言，侧重于多面表示与优化<br>
(4) LLVM IR，与 LLVM 自我表示之间存在 1:1 映射，可使 MLIR 通过 LLVM 发出 GPU 与 CPU 代码<br>
(5) TensorFlow Lite，将会转换以在移动平台上运行代码<br>
每种方言均由一组存在不变性的已定义操作组成，如：“这是一个二进制运算符，输入与输出拥有相同类型<br>
=&gt; 所以這邊還是有定義一些轉換的方言(IR)</p>
<h4 id="mlir-没有众所周知的固定或内置的操作列表无-内联函数方言dialect可完全定义自定义类型">MLIR 没有众所周知的固定或内置的操作列表（无 “内联函数”）。方言(dialect)可完全定义自定义类型</h4>
<h4 id="所以vendor可以自己建構因此可以extend並保有彈性">所以vendor可以自己建構,因此可以extend,並保有彈性</h4>
<p>即 MLIR 如何对 LLVM IR 类型系统（拥有一流汇总）、域抽象（对量化类型等经机器学习 (ML) 优化的加速器有着重要意义），<br>
乃至未来的 Swift 或 Clang 类型系统（围绕 Swift 或 Clang 声明节点而构建）进行建模。</p>
<h4 id="如果您想要连接新的低级编译器则需要创建新方言以及-tensorflow-图方言与您的方言ir之间的降阶">如果您想要连接新的低级编译器，则需要创建新方言，以及 TensorFlow 图方言与您的方言(IR)之间的降阶。</h4>
<h4 id="如此一来硬件及编译器制造商便可一路畅行">如此一来，硬件及编译器制造商便可一路畅行。</h4>
<h4 id="-白話-就是tensorflow--mlir處理high-level--to你的方言low-evel降階--low-level-compile">=&gt; 白話: 就是Tensorflow =&gt; MLIR處理(high-level) =&gt; to你的方言(low-evel)&ldquo;降階&rdquo; =&gt; low-level compile</h4>
<p>您甚至可以在同一个模型中定位不同级别的方言；<br>
高级优化器将尊重 IR 中不熟悉的部分，并等待较低级别的优化器来处理此类部分。</p>
<p>如果您是编译器研究者和框架制造者，则可以借助 MLIR 在每个级别进行转换，<br>
甚至是在 IR 中定义自己的操作和抽象，从而针对您试图解决的问题领域构建最佳模型。<br>
由此看来，MLIR 比 LLVM 更像是纯编译器基础设施。</p>
<p>虽然 MLIR 充当 ML 的编译器，但我们也看到，MLIR 同样支持在编译器内部使用机器学习技术！<br>
这一点尤为重要，因为在进行扩展时，开发数字库的工程师无法跟上 ML 模型或硬件的多样化速度。<br>
MLIR 的扩展性有助于探索代码降阶策略，并在抽象之间执行逐步降阶</p>
<h1 id="compiler-infrastructure">Compiler infrastructure</h1>
<p>We benefitted from experience gained from building other IRs<br>
(HLO, LLVM and SIL) when building MLIR.<br>
We will directly adopt existing best practices,<br>
e.g. writing and maintaining an IR spec, building an IR verifier,<br>
providing the ability to dump and parse MLIR files to text,<br>
writing extensive unit tests with the FileCheck tool,<br>
and building the infrastructure as a set of modular libraries that can be combined in new ways.<br>
We plan to use the infrastructure developed by the XLA team for performance analysis and benchmarking.</p>
<p>Other lessons have been incorporated and integrated into the design in subtle ways. <br>
For example, LLVM has non-obvious design mistakes that prevent a multithreaded compiler<br>
from working on multiple functions in an LLVM module at the same time.<br>
MLIR solves these problems by having per-function constant pools<br>
and by making references explicit with function_ref.</p>
<h1 id="getting-started-with-mlir">Getting started with MLIR</h1>
<p>The following instructions assume that you have git, ninja, and a working C++ toolchain. <br>
In the future, we aim to align on the same level of platform support as LLVM. <br>
For now, MLIR has been tested on Linux and macOS, with recent versions of clang and with gcc 7.</p>
<p>git clone <a href="https://github.com/llvm/llvm-project.git">https://github.com/llvm/llvm-project.git</a><br>
cd llvm-project/llvm/projects/<br>
所以這個會和LLVM有關聯,下載完會長這樣<br>
llvm這個folder是llvm相關code的核心的地方<br>
<img src="https://github.com/evansin100/MLIR/blob/master/Selection_003.png" alt="image"></p>
<p>git clone <a href="https://github.com/tensorflow/mlir">https://github.com/tensorflow/mlir</a><br>
MLIR的folder如下<br>
<img src="https://github.com/evansin100/MLIR/blob/master/Selection_001.png" alt="image"></p>
<p>cd ../..<br>
mkdir build<br>
cd build<br>
cmake -G Ninja ../llvm -DLLVM_BUILD_EXAMPLES=ON -DLLVM_ENABLE_CXX1Y=Y</p>
<p>CMake was unable to find a build program corresponding to &ldquo;Ninja&rdquo;<br>
有人是說沒有裝Ninja<br>
sudo apt-get install ninja-build <br>
<img src="https://github.com/evansin100/MLIR/blob/master/Selection_024.png" alt="image"><br>
<img src="https://github.com/evansin100/MLIR/blob/master/Selection_023.png" alt="image"></p>
<p>Run Build Command:&quot;/usr/bin/ninja&rdquo; &ldquo;cmTC_95634&rdquo;<br>
[1/2] Building C object CMakeFiles/cmTC_95634.dir/CheckIncludeFile.c.o<br>
FAILED: CMakeFiles/cmTC_95634.dir/CheckIncludeFile.c.o <br>
/usr/bin/cc    -o CMakeFiles/cmTC_95634.dir/CheckIncludeFile.c.o   -c CheckIncludeFile.c<br>
CheckIncludeFile.c:1:10: fatal error: malloc/malloc.h: No such file or directory<br>
#include &lt;malloc/malloc.h&gt;<br>
^~~~~~~~~~~~~~~~~<br>
compilation terminated.<br>
ninja: build stopped: subcommand failed.</p>
<p>Issue : c++: error: unrecognized command line option &lsquo;-Wshorten-64-to-32&rsquo;<br>
To install them add the ppa repository and update the APT database:<br>
$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test<br>
$ sudo apt-get update</p>
<h3 id="issue--could-not-find-z3-found-unsuitable-version-000-but-required-is-at-least-471-found-z3_libraries-notfound">issue : Could NOT find Z3: Found unsuitable version &ldquo;0.0.0&rdquo;, but required is at least &ldquo;4.7.1&rdquo; (found Z3_LIBRARIES-NOTFOUND)</h3>
<p>在一開始的,MLIR會做確認<br>
sudo apt install libz3-4 libz3-dev<br>
Found unsuitable version &ldquo;4.4.1&rdquo;, but required is at least &ldquo;4.7.1&rdquo;  <br>
Z3 is a state-of-the art theorem prover from Microsoft Research. It can be<br>
used to check the satisfiability of logical formulas over one or more<br>
theories. Z3 offers a compelling match for software analysis and verification<br>
tools, since several common software constructs map directly into supported<br>
theories.<br>
<a href="https://github.com/Z3Prover/z3/releases">https://github.com/Z3Prover/z3/releases</a> <br>
z3-4.8.4.d6df51951f4c-x64-ubuntu-16.04.zip<br>
好像都沒有ubuntu18.04的版本</p>
<h3 id="issue--could-not-find-libxml2-missing-libxml2_library-libxml2_include_dir">issue : Could NOT find LibXml2 (missing: LIBXML2_LIBRARY LIBXML2_INCLUDE_DIR)</h3>
<p>ninja check-mlir</p>
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>
</body>
</html>
