<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-167528382-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - Flow Concept (for inference) 摘要：MNN已经用于阿里手机淘宝、手机天猫、优酷等20多个应用之中， 覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等场景。 开源自家轻量级的深度神经网络推理引擎MNN（Mobile Neural Network） 用于在智能手机、IoT设备等端侧加载深度神经网络模型，进行推理预测
Android方面以小米6为例，MobileNet V2上耗费时间约为27毫秒， SqueezeNet V1.1上耗费约为25毫秒，领先业界至少30%；
Functionality MNN的两大功能与四大特点
(1) 模型转换部分帮助开发者兼容不同的训练框架
当前，MNN已经支持Tensorflow(Lite)、Caffe和ONNX，
PyTorch/MXNet的模型可先转为ONNX模型再转到MNN。
而且，也能通过算子融合、算子替代、布局调整等方式优化图
可以看到MNN有自己定義他的model format(.mnn)
(2) 计算推理部分致力于高效完成推理计算
为了更好地完成对模型的加载、计算图的调度，以及各计算设备下的内存分配、Op实现等任务。
他们在MNN中应用了多种优化方案，包括在卷积和反卷积中应用Winograd算法、
在矩阵乘法中应用Strassen算法、低精度计算、多线程优化、内存复用、异构计算等
=&gt; 有vulkan ?
=&gt; 有apple metal ?
=&gt; 從下圖可以看到他的frontend可以接TF/Caffe/ONNX
然後再convert to MNN format
Features (1) 轻量性：针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。
Android platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB
(2) 通用性：支持Tensorflow、Caffe、ONNX等主流模型文件格式，支持CNN、RNN、GAN等常用网络。 Supports 86 Tensorflow ops, 34 Caffe ops; MNN ops: 71 for CPU, 55 for Metal, 29 for OpenCL, and 31 for Vulkan." name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167528382-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());
          gtag('config', 'UA-167528382-1');
        </script>
    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<h5 id="wc" style="font-size: 1rem;text-align: center;">200 Words|Read in about 1 Min|total read<span id="busuanzi_value_page_pv"></span></h5>

<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/sw-framework-alibaba-mnn/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">SW-FRAMEWORK-Alibaba-MNN</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="flow">Flow</h1>
<p><img src="../Selection_062.png" alt="image"></p>
<h1 id="concept-for-inference">Concept (for inference)</h1>
<p>摘要：MNN已经用于阿里手机淘宝、手机天猫、优酷等20多个应用之中， <br>
覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等场景。  <br>
开源自家轻量级的深度神经网络推理引擎MNN（Mobile Neural Network） <br>
用于在智能手机、IoT设备等端侧加载深度神经网络模型，进行推理预测</p>
<p>Android方面以小米6为例，MobileNet V2上耗费时间约为27毫秒，  <br>
SqueezeNet V1.1上耗费约为25毫秒，领先业界至少30%；</p>
<p><img src="../Selection_706.png" alt="image"></p>
<h1 id="functionality">Functionality</h1>
<p>MNN的两大功能与四大特点<br>
(1) 模型转换部分帮助开发者兼容不同的训练框架<br>
当前，MNN已经支持Tensorflow(Lite)、Caffe和ONNX，<br>
PyTorch/MXNet的模型可先转为ONNX模型再转到MNN。<br>
而且，也能通过算子融合、算子替代、布局调整等方式优化图<br>
可以看到MNN有自己定義他的model format(.mnn)<br>
<img src="../https://github.com/evansin100/Alibaba-MNN/blob/master/Selection_006.png" alt="image"></p>
<p>(2) 计算推理部分致力于高效完成推理计算<br>
为了更好地完成对模型的加载、计算图的调度，以及各计算设备下的内存分配、Op实现等任务。<br>
他们在MNN中应用了多种优化方案，包括在卷积和反卷积中应用Winograd算法、<br>
在矩阵乘法中应用Strassen算法、低精度计算、多线程优化、内存复用、异构计算等<br>
=&gt; 有vulkan ?<br>
=&gt; 有apple metal ?<br>
=&gt; 從下圖可以看到他的frontend可以接TF/Caffe/ONNX<br>
然後再convert to MNN format</p>
<p><img src="../Selection_707.png" alt="image"></p>
<h1 id="features">Features</h1>
<p>(1) 轻量性：针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。<br>
Android platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB</p>
<p>(2) 通用性：支持Tensorflow、Caffe、ONNX等主流模型文件格式，支持CNN、RNN、GAN等常用网络。  <br>
Supports 86 Tensorflow ops, 34 Caffe ops; MNN ops: 71 for CPU, <br>
55 for Metal, 29 for OpenCL, and 31 for Vulkan.<br>
Supports hybrid computing on multiple devices. <br>
Currently supports CPU and GPU. GPU op plugin can be loaded dynamically <br>
to replace default (CPU) op implementation.<br>
=&gt; 所以有做GPU custom OP</p>
<p>(3) 高性能：不依赖任何第三方计算库，依靠大量手写汇编实现核心运算，充分发挥ARM CPU的算力。<br>
比如，在iOS设备上，可以开启GPU加速（Metal），常用模型上快于苹果原生的CoreML。<br>
Convolution and transposition convolution algorithms are efficient and stable.<br>
The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3 -&gt; 7x7<br>
Additional optimizations for the new architecture ARM v8.2 with half-precision calculation support <br>
=&gt; 有對ARMv8.2 FP16優化?</p>
<p>(4) 易用性：有高效的图像处理模块，覆盖常见的形变、转换等需求，一般情况下，无需额外引入libyuv或opencv库处理图像。
=&gt; 有CV function ?</p>
<p>MNN不仅支持回调机制，可以在网络运行中插入回调，提取数据或者控制运行走向；<br>
还支持只运行网络中的一部分，或者指定CPU和GPU间并行运行。</p>
<h1 id="example">Example</h1>
<p>手机淘宝春节活动：扫年货，集五福<br>
2019年春节期间，<br>
=&gt; 淘宝通过扫年货的方式加入到了阿里“集五福”活动之中。<br>
具体的使用场景是，<br>
=&gt; 通过扫一扫商品识别能力，来识别红色年货，并分析&quot;照片中有年货&quot;的概率，根据概率来发放相关权益<br>
<img src="../Selection_710.png" alt="image"></p>
<p>首先，为了扫描年货，淘宝在服务端用百万张年货图片训练出了一个可以鉴别年货的深度神经网络模型。<br>
接下来，就需要MNN发挥作用了。<br>
用户通过相机扫描年货时，淘宝会获取相机中的照片数据。然后，对照片做预处理，包括图片的缩放、颜色空间的转换等。<br>
离青说，扫年货是一个基于相机的应用场景，使用云端AI会消耗用户大量的流量去传输逐帧照片、<br>
服务端的计算资源，同时响应速度也会取决于网络状况。<br>
而MNN，可以通过端侧AI，避免了网络开销，使整体体验流畅、稳定</p>
<h1 id="comment">Comment</h1>
<p>今年3月份正式宣布加入阿里的框架大牛贾扬清，在MNN项目开源评审时也给出了自己的建议。<br>
在贾扬清看来，与Tensorflow、Caffe2等同时覆盖训练和推理的通用框架相比， <br>
MNN更注重在推理时的加速和优化，解决在模型部署的阶段的效率问题，从而在移动端更高效地实现模型背后的业务。<br>
这和服务器端TensorRT等推理引擎的想法不谋而合。 <br>
离青说，这样的转变，让MNN有了更切合使用场景的定位，对于其进一步发展，和为开发者服务，都有很大的帮助<br>
=&gt; 所以就是走inference</p>
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <span id="busuanzi_container_site_pv">
        total vistor：<span id="busuanzi_value_site_pv"></span>
    </span>
    &nbsp;
    <span id="busuanzi_container_site_uv">
        you are <span id="busuanzi_value_site_uv"></span> th visitor
    </span>

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>

</body>
</html>
