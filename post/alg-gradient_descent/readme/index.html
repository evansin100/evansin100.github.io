<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-167528382-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - Concept 梯度下降法(gradient&quot;梯度&rdquo; descent)是最佳化理論裡面的一個一階找最佳解的一種方法 =&gt; 所以他是一種求解的方式,只是被拿到deep learning使用
主要是希望用梯度下降法找到函數(剛剛舉例的式子)的局部最小值，
因為梯度的方向是走向局部最大的方向，所以在梯度下降法中是往梯度的反方向走。
梯度下降法就好比『我們在山頂，但不知道要下山的路，
於是，我們就沿路找向下坡度最大的叉路走，直到下到平地為止』。要找到向下坡度最大， 在數學上常使用**『偏微分』(Partial Differential)，求取斜率**，
一步步的逼近，直到沒有顯著改善為止，這時我們就認為是最佳解了，過程可參考下圖說明。
這邊我們先大概說一下梯度， 要算一個函數J(w) &ldquo;loss function&quot;的梯度有一個前提，就是這個函數要是任意可微分函數，
這也是深度學習為什麼都要找可微分函數出來當激活函數(activation function)
=&gt; 因為loss function可以微分才有梯度 底下是一個類似NN找weight併讓loss可以最低的範例 gradient descent example function 1 剛有提到我們需要先設定一個初始化的「解」，此例我設定x(0)=20(故意跟最佳值有差距)
紅色的點是每一次更新找到的解
紅色線是法線，藍色線是切線，法線和切線這兩條線是垂直的，但因為x軸和y軸scale不一樣，所以看不出來它是垂直的。 learning rate = 0.01
一開始loss下降很快,因為斜率很大,所以w的一點點改動就會影響到loss的值
一開始loss下降很慢,因為斜率變很小了
function 2
如果使這個case, learning rate太小
就會找到loss的local optmization解,找不到最低點
因為不會振動到那邊
Forward Propagation and Backpropagation 依據上圖，我們先建構好模型，決定要做幾層的隱藏層，
接著，Neural Network 就會利用 Forward Propagation 及 Backpropagation 機制，如下圖，
幫我們求算模型中最重要的參數 &ndash; 『權重』(Weight)，這個過程就稱為『最佳化』(Optimization)， 最常用的技巧就是『梯度下降』(Gradient Descent)。
接著，我們就來用圖說故事，這部份主要參考 DataCamp Deep Learning in Python 的投影片，" name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167528382-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());
          gtag('config', 'UA-167528382-1');
        </script>
    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<h5 id="wc" style="font-size: 1rem;text-align: center;">200 Words|Read in about 1 Min|total read<span id="busuanzi_value_page_pv"></span></h5>

<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/alg-gradient_descent/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">ALG-gradient_descent</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="concept">Concept</h1>
<p>梯度下降法(gradient&quot;梯度&rdquo; descent)是<strong>最佳化理論裡面的一個一階找最佳解的一種方法</strong>   <br>
=&gt; 所以他是一種求解的方式,只是被拿到deep learning使用<br>
主要是希望用梯度下降法找到函數(剛剛舉例的式子)的局部最小值，<br>
因為梯度的方向是走向局部最大的方向，所以在梯度下降法中是往梯度的反方向走。</p>
<p>梯度下降法就好比『我們在山頂，但不知道要下山的路，<br>
於是，我們就沿路找向<strong>下坡度最大的叉路走，直到下到平地為止</strong>』。要找到向下坡度最大， <br>
在數學上常使用**『偏微分』(Partial Differential)，求取斜率**，<br>
一步步的逼近，直到沒有顯著改善為止，這時我們就認為是最佳解了，過程可參考下圖說明。<br>
<img src="Selection_509.png" alt="image"></p>
<p>這邊我們先大概說一下梯度， 要算一個函數J(w) &ldquo;loss function&quot;的梯度有一個前提，就是這個函數要是任意可微分函數，<br>
這也是深度學習為什麼都要找可微分函數出來當激活函數(activation function)<br>
=&gt; 因為loss function可以微分才有梯度 <br>
底下是一個類似NN找weight併讓loss可以最低的範例 <br>
<img src="Selection_510.png" alt="image"><br>
<img src="Selection_512.png" alt="image"></p>
<h1 id="gradient-descent-example">gradient descent example</h1>
<p>function 1
<img src="Selection_513.png" alt="image"><br>
<img src="Selection_514.png" alt="image"></p>
<p>剛有提到我們需要先設定一個初始化的「解」，此例我設定x(0)=20(故意跟最佳值有差距)<br>
紅色的點是每一次更新找到的解<br>
紅色線是法線，藍色線是切線，法線和切線這兩條線是垂直的，但因為x軸和y軸scale不一樣，所以看不出來它是垂直的。   <br>
learning rate = 0.01<br>
一開始loss下降很快,因為斜率很大,所以w的一點點改動就會影響到loss的值<br>
<img src="Selection_517.png" alt="image"><br>
<img src="Selection_516.png" alt="image"><br>
一開始loss下降很慢,因為斜率變很小了<br>
<img src="Selection_515.png" alt="image"></p>
<p>function 2<br>
如果使這個case, learning rate太小<br>
就會找到loss的local optmization解,找不到最低點<br>
因為不會振動到那邊<br>
<img src="Selection_519.png" alt="image"></p>
<h1 id="forward-propagation-and-backpropagation">Forward Propagation and Backpropagation</h1>
<p>依據上圖，我們先建構好模型，決定要做幾層的隱藏層，<br>
接著，Neural Network 就會利用 Forward Propagation 及 Backpropagation 機制，如下圖，<br>
幫我們求算模型中最重要的參數 &ndash; 『權重』(Weight)，這個過程就稱為『最佳化』(Optimization)， <br>
最常用的技巧就是『梯度下降』(Gradient Descent)。<br>
<img src="Selection_523.png" alt="image"></p>
<p>接著，我們就來用圖說故事，這部份主要參考 DataCamp Deep Learning in Python 的投影片，<br>
先假設一個簡單模型如下圖，只有一層隱藏層，兩個 Input 變數值。<br>
先任意假設一組權重(W)，如圖中紅圈內數字。<br>
每一層的 output 就等於 前一層的 input 乘以權重(W)，<br>
即 y = sum(w * x)，這個過程就是 Forward Propagation。<br>
舉例來說，隱藏層的第一個節點(Node)就等於 2 * 1 + 3 * 1 = 5, <br>
第二個節點就等於 2 * -1 + 3 * 1 = 1，output層就等於 5 * 2 + 1 * -1 = 9</p>
<p><img src="Selection_520.png" alt="image"></p>
<p>接著，我們就反推回去(Backpropagation)，<br>
<strong>用『梯度下降法』，逐步調整權重(W)</strong>，慢慢逼近最佳解，以達到『損失函數最小化』。<br>
如圖一，我們先求算梯度(Gradient)，再依梯度往下走，損失就會越來越小，<br>
梯度的公式等於 -2 * input * (y實際值 - y 預測值)，後續會證明。再接續上圖，<br>
假設已知資料的ouput實際值為 13，那損失就等於 13-9 = 4，<br>
這時的output對隱藏層兩個節點的梯度就分別等於 -2 * 5 * 4 = -40 及 -2 * 1 * 4 = -8，<br>
因此，我們，就可以設定下降的步幅，即學習率(Learning Rate)，譬如 0.01，<br>
所以，調整新權重 = 原權重 - (學習率 * 梯度)，推算出所有的權重後，<br>
再回到步驟2，不斷循環，直到損失的縮小已經不顯著了，我們就認定那一組權重是最佳解了。公式整理如下：<br>
=&gt; <strong>可以看到W的更新公式裏面有X(所以會和input有關係)</strong><br>
=&gt; <strong>所以可以是(a)看過一筆x就更新W, (b)或者是多個X(batch)就更新W, (c)或者是all的X就更新W</strong><br>
=&gt; GD要看全部的X, SGD/Adam/Momentum..etc.只要看1筆or多筆(batch)<br>
<img src="Selection_525.png" alt="image"></p>
<p>底下這個link有用到的optimize方式來使用gradient descent<br>
例如要不要看歷史資料,還有learning rate可不可以變動 等等<br>
這些都是optimizer的方式<br>
<a href="https://github.com/evansin100/SW-FRAMEWORK-Tensorflow/tree/master/Training/Training-Design/Optimizer">https://github.com/evansin100/SW-FRAMEWORK-Tensorflow/tree/master/Training/Training-Design/Optimizer</a></p>
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <span id="busuanzi_container_site_pv">
        total vistor：<span id="busuanzi_value_site_pv"></span>
    </span>
    &nbsp;
    <span id="busuanzi_container_site_uv">
        you are <span id="busuanzi_value_site_uv"></span> th visitor
    </span>

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>

</body>
</html>
