<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-167528382-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <title>Memo</title>
    
    
    <meta content="Memo" name="keywords">
    
    <meta content="Memo - Overview ====================================
NN part特點 (1) 有支援Caffe/Kaldi/MXNet/ONNX/TF(尤其是Kaldi)
(2) 還有一個demo folder,來做安裝好的驗證,script會自己下載caffe model然後做convert
(3) Model optimizer只有支援 a. Cutting off parts of the model, 例如將只有訓練用的部份砍掉 e.g. dropout b. OP fusion (4) OpenVINO IE inference engine主要針對HW有做OP優化(e.g. CPU/GPU/VPU-Movidius) (5) 可以在inference API指定batch
(6) inference的時後有支援 async call (API:StartAsync),評估建議是throughput - FPS
(7) inference的時後有支援 sync call (API:Infer),評估建議是throughput - latency
CV part特點 (1) 此外OpenVINO OpenCV部份也有針對一些CV function做硬體加速
integration特點 (1) OpenCV 3.3以上本來就支援DNN 至少有支援Caffe and Tensorflow model,API的用法如下圖
cv::dnn::readNetFromTensorflow(weights, prototxt); (2) 但OpenVINO OpenCV NN部份有連結到 IE(inference engine),所以有做優化,這樣就可以用同一套串CV and NN了" name="description">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    

    

    
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167528382-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());
          gtag('config', 'UA-167528382-1');
        </script>
    

    
    
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>
    

    <link rel="stylesheet" href="/layui/css/layui.css">
    <link rel="stylesheet" href="/self/css/default.css">
    <script async src="/layui/layui.js"></script>

    <link rel="stylesheet" async href="/self/css/markdown.min.css">
    <link rel="stylesheet" async href="/self/css/gallery.css">
    
    
    

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
    <script async src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin="anonymous"></script></head>

<body>
    
    <header class="layui-header layui-bg-cyan">

    
    
    <a class="nav-self-logo" href="/">
        Memo
    </a>

    <ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter="">
        
        
        <li class="layui-nav-item" id="nav_big"><a href="/post/">Posts</a></li>
        

        
            
                <li class="layui-nav-item" id="nav_big"><a href="/about/">About</a></li>
            
        

        
        <li class="layui-nav-item" id="nav_small">
            <a href="javascript:;">
                <i class="layui-icon layui-icon-app" style="font-size: 24px;"></i>
            </a>

            <dl class="layui-nav-child">
                
                <dd><a href="/post/">Posts</a></dd>
                

                
                    
                        <dd><a href="/about/">About</a></dd>
                    
                
            </dl>
        </li>
    </ul>
</header>

<script>
layui.use('element', function(){
  var element = layui.element;
});
</script>
        <div id="content" style="min-height:80%">
<h5 id="wc" style="font-size: 1rem;text-align: center;">1200 Words|Read in about 6 Min|total read<span id="busuanzi_value_page_pv"></span></h5>

<div class="layui-container" style="margin-bottom: 10px">
    

    <div class="layui-row layui-col-space10">
        <div class="layui-col-md12 layui-col-sm12 layui-col-xs12">
            <div class="layui-card single-card">
                <br />
                <blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title" >
                    <h1></h1>
                    <h3 style="margin-top:10px; margin-bottom:10px"> 
    <i class="layui-icon layui-icon-date" style="font-size: 28px; vertical-align: -2px;"></i>
    <span>0001-01-01</span>

    
     
    <i class="layui-icon layui-icon-list" style="font-size: 32px; vertical-align: -3px;"></i>
    

    
        <a href="/categories/sw-framework-openvino/">
            <span class="layui-badge layui-bg-orange" style="vertical-align: 2px;">SW-FRAMEWORK-OpenVINO</span>
        </a>
    

    
    
    
</h3>
                </blockquote>
                <div class="layui-card-body markdown-body single-content">
                    <h1 id="overview">Overview</h1>
<p>====================================</p>
<h3 id="nn-part特點">NN part特點</h3>
<p>(1) 有支援Caffe/Kaldi/MXNet/ONNX/TF(尤其是Kaldi)<br>
(2) 還有一個demo folder,來做安裝好的驗證,script會自己下載caffe model然後做convert<br>
(3) Model optimizer只有支援
a. Cutting off parts of the model, 例如將只有訓練用的部份砍掉 e.g. dropout
b. OP fusion
(4) OpenVINO IE inference engine主要針對HW有做OP優化(e.g. CPU/GPU/VPU-Movidius) <br>
(5) 可以在inference API指定batch<br>
(6) inference的時後有支援 async call (API:StartAsync),評估建議是throughput - FPS<br>
(7) inference的時後有支援 sync call (API:Infer),評估建議是throughput - latency</p>
<h4 id="cv-part特點">CV part特點</h4>
<p>(1) 此外OpenVINO OpenCV部份也有針對一些CV function做硬體加速</p>
<h4 id="integration特點">integration特點</h4>
<p>(1) OpenCV 3.3以上本來就支援DNN
至少有支援Caffe and Tensorflow model,API的用法如下圖<br>
cv::dnn::readNetFromTensorflow(weights, prototxt);  <br>
(2) 但OpenVINO OpenCV NN部份有連結到 IE(inference engine),所以有做優化,這樣就可以用同一套串CV and NN了<br>
(3) 針對純inference engine, 也有兩者混用的case<br>
InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());<br>
直接用OpenVINO內建的algorithm,就可以做data preprocess<br>
inputInfoFirst-&gt;getPreProcess().setResizeAlgorithm(ResizeAlgorithm::RESIZE_BILINEAR);  <br>
=&gt; 但針對這一點Android Q也開始將很多CV -data操作變成是OP化, 所以也是可以達到類似的效果</p>
<p>====================================</p>
<p>因應邊緣運算的未來趨勢，不只公有雲廠商，英特爾 也推出了OpenVINO開發工具包<br>
（Open Visual Inference &amp; Neural network Optimization），結合Intel軟體、硬體產品</p>
<p>Intel表示，透過OpenVINO，使用者可以開發高效能的視覺分析應用。而該工具包，總共由三大重要元件組成。<br>
(1) 首先是Intel Vision Prodcuts中，經常用到的深度學習部署工具（Deep Learning Deployment），<br>
(2) 還有開源電腦視覺及影像處理工具OpenCV，<br>
(3) 以及電腦視覺API標準OpenVX</p>
<p>而OpenVINO，可以與Intel自家的深度學習工具結合使用，像是深度學習編譯器nGraph。<br>
該公司也表示，此工具包相容於市場上主流的AI框架，像是TensorFlow、MXNet、Caffe「開發者可以更簡單將應用，部署在邊緣運算環境」，<br>
同時該工具包，也能結合自家硬體應用，像是FPGA技術，還有視覺處理晶片VPU</p>
<h2 id="hardware">Hardware</h2>
<p>支援底下這些<br>
6th-8th Generation Intel® Core™<br>
Intel® Xeon® v5 family<br>
Intel® Xeon® v6 family<br>
Intel® Pentium® processor N4200/5, N3350/5, N3450/5 with Intel® HD Graphics<br>
Intel® Movidius™ Neural Compute Stick<br>
所以這邊就是USB stick<br>
Movidius 成立於 2005 年，專注於低功耗電腦視覺計算晶片開發，2016 年正式被 Intel 收購。<br>
他們最著名的產品就是 USB 神經計算棒（Neural Compute Stick, NCS），<br>
又稱為 VPU （Vision Processor Unit），其中主要的晶片代號為 Myriad 2（MA2x5x）。<br>
這項產品主要的特點是其 USB 型式，可插在執行 Linux（Ubuntu）的桌上型電腦或嵌入式系統，<br>
尤其可支援創客最愛用的樹莓派，使其可以加速執行深度學習推論工作</p>
<p>Intel® Neural Compute Stick 2<br>
Intel® Vision Accelerator Design with Intel® Movidius™ VPUs (這個是之前Intel收購的VPU,所以這邊看起來是要做NN)</p>
<h2 id="cv-and-nn">CV and NN</h2>
<p>所以可以看到主要是分成兩類<br>
NN (inference) and computer vision algorithm<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_011.png" alt="image"></p>
<p>透過模型優化器 （Model Optimizer）將網路模型匯入至 Intel 的平台上，且在過程中會根據目標平台的特性做進一步的優化，<br>
接著轉換成給推理引擎 （Inference Engine）， 作為 input 中間的表示文件 （IR file），<br>
推理引擎搭配硬體插件 （這裡目前就是 Movidius NCS）強化推理性能，再將最終的文件下載到目標平台上，驗證後進行應用部署</p>
<p>不僅如此，OpenVINO 的工具套件 （toolkit），實際上是分層的，意思是可以根據開發者的個人能力以及需求決定自己要使用的 API 介面， <br>
真的達到「盡可能的針對不同使用場景，進行優化再優化」的目的</p>
<h3 id="裡面有大約20個pre-trained模型">裡面有大約20個pre-trained模型</h3>
<p><img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_012.png" alt="image"></p>
<h2 id="opencv-30-and-opencl">OpenCV 3.0 and OpenCL</h2>
<p>OpenCV 除了原有對 Intel CPU 加速函式庫 IPP （Integrated Performance Primitives）、<br>
TBB （Threading Building Blocks）的支援外，發展至今已陸續整合進許多繪圖晶片（GPU）加速計算的平台，<br>
如 OpenVX、OpenCL、CUDA等<br>
由於深度學習大量應用於電腦視覺，自 OpenCV 3.0 版後就加入 DNN （Deep Neural Network）模組，<br>
3.2 版更是加入深度學習常用的 Caffe 框架及 YOLO 物件定位模組<br>
OpenVINO整合了OpenCV、 OpenVX、OpenCL 等開源軟體工具並支援自家 CPU、 GPU、FPGA、ASIC （IPU、VPU）等硬體加速晶片，<br>
更可支援 Windows、Liunx （Ubuntu、CentOS）等作業系統，<br>
更可支援常見 Caffe、TensorFlow、Mxnet、ONNX 等深度學習框架所訓練好的模型及參數</p>
<h2 id="api-and-direct-programing">API and direct programing</h2>
<p>所以從下圖可以看到, OpenVINO主要是兩大類(1) API and direct programing<br>
API是openCV and OpenVX and NN相關的(model optimize, inference engine), direct programing支援C++ and OpenCL<br>
此外有針對CPU and GPU做蠻多處理的<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_009.png" alt="image"></p>
<h2 id="inference-only-but-support-model-optimization">inference only, but support model optimization</h2>
<p>OpenVINO 主要是用來推論用的(所以不是用來做訓連用的)，<br>
特定模型的參數必須在其它框架（TensorFlow、Cafee、Mxnet）下訓練好才可使用。<br>
OpenVINO 除了可提供硬體加速外，更提供模型優化器（Model Optimizer）功能，<br>
可協助去除已訓練好的模型中的冗餘參數 (所以有pruning效果)，<br>
並可將 32bits 浮點數的參數降階，以犧牲數個百分點正確率來換取推論速度提升數十倍到百倍 (還有支援quantization)</p>
<p>優化後，產出二個中間表示（Intermediate Representation、IR）檔案（*.bin, *.xml），<br>
再交給推論引擎（Inference Engine）依指定的加速硬體（CPU、GPU、FPGA、ASIC）進行推論，如下圖所示<br>
(所以要先用&quot;其他的framework&quot;訓練完之後,再來做轉換並優化)<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_010.png" alt="image"></p>
<h2 id="models">models</h2>
<p>在 OpenVINO 中提供了多種預訓練及優化好的深度學習模型，<br>
包括影像分類（AlexNet、GooLeNet、VGG、SqueezeNet、RestNet）、<br>
物件定位（SSD、Tiny YOLO）及一種類似全卷積神經網路（Fully Convolutional Networks、FCN）的語義分割模型（like FCN-8s）</p>
<p>所以主要是底下這幾類<br>
(1). 影像分類：<br>
一張影像原則上只能被分到一個類別，所以影像中最好只有一個主要物件。<br>
若影像中出現多個物件，那分類時則可能出現多個分類結果，<br>
同時會給出每個分類的不同機率，此時誤分類的可能性就會大大提昇。</p>
<p>(2) 物件定位(object detection)：<br>
一張影像中可同時出現多個相同或不同物件，大小不据，<br>
辨識後會對每個物件產生一個邊界框（Bounding Box），<br>
如此即可獲得較為準確的物件位置（座標）及尺寸（邊界框長寬）。</p>
<p>(3) 語義分割：<br>
是一種像素級分類，意思就是每個像素都只會被歸到某一分類，<br>
如此就可取得接近物件真實邊界（Edge）。<br>
但缺點是多個相同物件類型的像素都會被分到同一類，當物件太靠近或部份重疊時就不易分清楚共有多少物件。</p>
<p>(4) 實例分割：<br>
這也是一種像素級的分類，和語義分割的差別是相同類型的不同物件所屬像素就會被區分成不同分類（顏色），<br>
包括物件有部份重疊時，如此就能更正確判別影像中的內容</p>
<h1 id="install">Install</h1>
<p>OpenVINO 安裝執行<br>
接下就開始說明如何以 Intel 電腦視覺推論及神經網路（深度學習）優化工具包「OpenVINO」土砲自駕車的視覺系統。<br>
首先到 OpenVINO 官網，如下圖所示，按下左上角黃色按鈕，依所需的作業系統（Windows, Liunx）<br>
下載工作包並依指示將開發環境（Visual Studio 2015/2017, GCC）安裝完成。<br>
=&gt;所以有支援windows and linux的tool套件</p>
<p>雖然官方指定要 Windows 10 64bit, Intel Core 6 ~ 8 代 CPU 才能執行，<br>
經實測在 Windows 7 64bit／Intel Core i5 480M （i5 第一代筆電用 CPU）、Visual Studio 2017 （
含 MSBuild）環境下還是可以順利編譯及執行</p>
<p>What&rsquo;s Included (從官網看到下載的話裡面有包含這些東西)<br>
(1) OpenCV libraries<br>
(2) OpenVX* runtime<br>
(3) Deep Learning Deployment Toolkit (DLDT) with the Model Optimizer and inference engine<br>
(4) Compute Library for Deep Neural Networks (clDNN)<br>
(5) Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN)<br>
(6) Deep learning accelerator for field-programmable gate arrays (FPGA)<br>
(7) Code samples, documentation, and models</p>
<p>其中較重要的內容包括以下四點：<br>
\computer_vision_algorithms 傳統視覺算法<br>
\inference_engine 推論引擎及相關範例程式<br>
\intel_models 預先訓練模型<br>
\ model_optimizer 模型優化器</p>
<p>底下有講怎麼使用安裝好的SDK<br>
<a href="https://makerpro.cc/2018/10/use-intel-openvino-to-make-self-driving-vision-system/">https://makerpro.cc/2018/10/use-intel-openvino-to-make-self-driving-vision-system/</a><br>
l_openvino_toolkit_p_2018.5.455(後面是版本號)</p>
<h2 id="step-1">step 1</h2>
<p>sudo -E ./install_cv_sdk_dependencies.sh<br>
These dependencies are required for the Intel-optimized version of OpenCV<br>
所以看起來CV的套件有很多相依性,還有CV版本有針對Intel做優化</p>
<h2 id="step-2">step 2</h2>
<p>Install the OpenVINO™ core components:<br>
Option 1: GUI Installation Wizard:<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_013.png" alt="image"><br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_014.png" alt="image"><br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_015.png" alt="image"><br>
安裝好的folder長這樣<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_016.png" alt="image"></p>
<h2 id="step-3">step 3</h2>
<p>Set the environment variables<br>
cd /OpenVINO/install/computer_vision_sdk_2018.5.455$ =&gt; source bin/setupvars.sh<br>
所以路徑在computer_vision_sdk_2018.5.455這個安裝folder底下</p>
<h2 id="step4">step4</h2>
<p>Configure the Model Optimizer</p>
<h4 id="important-this-section-is-required-you-must-configure-the-model-optimizer-for-at-least-one-framework">Important: This section is required. You must configure the Model Optimizer for at least one framework.</h4>
<h4 id="the-model-optimizer-will-fail-if-you-do-not-complete-the-steps-in-this-section">The Model Optimizer will fail if you do not complete the steps in this section.</h4>
<p>所以要綁定一個framework</p>
<p>The Model Optimizer is a key component of the Intel Distribution of OpenVINO toolkit.<br>
You cannot do inference on your trained model without running the model through the Model Optimizer.<br>
When you run a pre-trained model through the Model Optimizer,<br>
your output is an Intermediate Representation (IR) of the network.(所以這個也有點像是NN compiler)<br>
The IR is a pair of files that describe the whole model:(這邊的IR會包含底下兩個項目)</p>
<h4 id="xml-describes-the-network-topology-很像是graphdef">.xml: Describes the network topology (很像是graphdef)</h4>
<h4 id="bin-contains-the-weights-and-biases-binary-data-這個會包含weight">.bin: Contains the weights and biases binary data (這個會包含weight)</h4>
<p>The Inference Engine reads, loads, and infers the IR files,<br>
using a common API across the CPU, GPU, or VPU hardware<br>
(這邊有source code)<br>
The Model Optimizer is a Python*-based command line tool (mo.py), which is located in <br>
/opt/intel/openvino/deployment_tools/model_optimizer.</p>
<p>(所以已經可以轉換多種framework產生的model)<br>
Use this tool on models trained with popular deep learning frameworks<br>
such as Caffe*, TensorFlow*, MXNet*, and ONNX* to convert them to  <br>
an optimized IR format that the Inference Engine can use</p>
<h2 id="step41">step4.1</h2>
<p>Model Optimizer configuration steps<br>
Go to the Model Optimizer prerequisites directory:<br>
可以看到deployment folder裡面有很多工具</p>
<h4 id="cv-algorithm-and-demo">CV algorithm and demo</h4>
<h4 id="intel-model-model-downloader-model-optimizer">Intel model, model downloader, model optimizer</h4>
<h4 id="inference-engine-extension-generator">inference engine, extension generator</h4>
<p><img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_017.png" alt="image"></p>
<p>Configure the Model Optimizer for all supported frameworks at the same time:=&gt;所以可以針對所有framework  <br>
/home/evansin/OpenVINO/install/computer_vision_sdk/deployment_tools/model_optimizer/install_prerequisites <br>
Run the script to configure the Model Optimizer for</p>
<h4 id="支援-caffe-tensorflow-mxnet-kaldi-and-onnx-的優化">支援 Caffe, TensorFlow, MXNet, Kaldi*, and ONNX 的優化</h4>
<p>可以試試看把一個tensorflow model丟進去看看可以做到多少優化</p>
<h3 id="issue--the-repository-cdromubuntu-1804-lts-_bionic-beaver_---release-amd64-20180426-bionic-release-does-not-have-a-release-file">issue : The repository &lsquo;cdrom://Ubuntu 18.04 LTS <em>Bionic Beaver</em> - Release amd64 (20180426) bionic Release&rsquo; does not have a Release file.</h3>
<p>解法就是把他移掉 Just remove your repository above from sources.list,  <br>
edit your sources.list at /etc/apt/sources.list<br>
and try update and upgrade again your system</p>
<p>安裝完後,可以看到這可以針對很多framework<br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_018.png" alt="image"></p>
<h2 id="step42">step4.2</h2>
<p>Use the Demo Scripts to Verify Your Installation<br>
可以用demo script來驗證你是否有安裝成功 <br>
範例是下載caffe model(FP32)來做轉換 <br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_019.png" alt="image"><br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_020.png" alt="image"><br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_022.png" alt="image"><br>
<img src="../https://github.com/evansin100/OpenVINO/blob/master/Selection_023.png" alt="image"></p>
<h3 id="issue--pip3-not-found">issue : pip3 not found</h3>
<p>sudo apt-get remove python3-pip<br>
sudo apt-get install python3-pip</p>
<h2 id="step42---1">step4.2 - 1</h2>
<p>Convert a model with Model Optimizer
透過mo.py轉換並優化model<br>
然後也有自己的IR<br>
Run python3 /home/evansin/OpenVINO/install//computer_vision_sdk_2018.5.455/deployment_tools/model_optimizer/mo.py &ndash;input_model /home/evansin/openvino_models/models/FP32/classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel /home/evansin/openvino_models/ir/FP32//classification/squeezenet/1.1/caffe &ndash;data_type FP32</p>
<p>Model Optimizer arguments:<br>
Common parameters:<br>
- Path to the Input Model: 	  /home/evansin/openvino_models/models/FP32/classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel<br>
- Path for generated IR: 	/home/evansin/openvino_models/ir/FP32//classification/squeezenet/1.1/caffe<br>
- IR output name: 	squeezenet1.1<br>
- Log level: 	ERROR<br>
- Batch: 	Not specified, inherited from the model<br>
- Input layers: 	Not specified, inherited from the model<br>
- Output layers: 	Not specified, inherited from the model<br>
- Input shapes: 	Not specified, inherited from the model<br>
- Mean values: 	Not specified<br>
- Scale values: 	Not specified<br>
- Scale factor: 	Not specified<br>
- Precision of IR: 	FP32<br>
- Enable fusing: 	True =&gt;有做fusion <br>
- Enable grouped convolutions fusing: 	True<br>
- Move mean values to preprocess section: 	False<br>
- Reverse input channels: 	False<br>
Caffe specific parameters:<br>
- Enable resnet optimization: 	True<br>
- Path to the Input prototxt: 	  /home/evansin/openvino_models/models/FP32/classification/squeezenet/1.1/caffe/squeezenet1.1.prototxt<br>
- Path to CustomLayersMapping.xml: 	Default<br>
- Path to a mean file: 	Not specified<br>
- Offsets for a mean file: 	Not specified<br>
Model Optimizer version: 	1.5.12.49d067a0</p>
<p>這邊是轉換過的結果<br>
[ SUCCESS ] Generated IR model.<br>
[ SUCCESS ] XML file: /home/evansin/openvino_models/ir/FP32//classification/squeezenet/1.1/caffe/squeezenet1.1.xml<br>
[ SUCCESS ] BIN file: /home/evansin/openvino_models/ir/FP32//classification/squeezenet/1.1/caffe/squeezenet1.1.bin<br>
[ SUCCESS ] Total execution time: 2.95 seconds.</p>
<h2 id="step42---2">step4.2 - 2</h2>
<p>這個好像是TFrunner的樣子,就是一個簡單的inference engine   <br>
Build Inference Engine samples</p>
<p>&ndash; Host CPU features:<br>
會印出那些功能沒有support<br>
可以看到可以指定要用那個device來跑, -m 接的是model<br>
Run ./classification_sample -d CPU -i /home/evansin/OpenVINO/install/computer_vision_sdk/deployment_tools/demo/car.png -m /home/evansin/openvino_models/ir/FP32//classification/squeezenet/1.1/caffe/squeezenet1.1.xml<br>
如果有改變image大小也會印出來<br>
[ WARNING ] Image is resized from (787, 259) to (227, 227)<br>
輸出會是inference time,然後轉換成FPS<br>
total inference time: 7.0906649<br>
Average running time of one iteration: 7.0906649 ms<br>
Throughput: 141.0304970 FPS</p>
<h1 id="openvino-x-tensorflow">OpenVINO x Tensorflow</h1>
<p><a href="https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow">https://software.intel.com/en-us/articles/OpenVINO-Using-TensorFlow</a></p>
<p>The Model Optimizer is a cross-platform command-line tool that facilitates<br>
e transition between the training and deployment environment,<br>
performs static model analysis, and adjusts deep learning models<br>
for optimal execution on end-point target devices.</p>
<p>The Model Optimizer process assumes you have a network model trained <br>
using a supported frameworks. The scheme below illustrates the typical workflow<br>
for deploying a trained deep learning model:</p>
</div>
            </div>
        </div>

        
    </div>
</div>


        </div><footer>
    

    <span id="busuanzi_container_site_pv">
        total vistor：<span id="busuanzi_value_site_pv"></span>
    </span>
    &nbsp;
    <span id="busuanzi_container_site_uv">
        you are <span id="busuanzi_value_site_uv"></span> th visitor
    </span>

    <div class="layui-container">
        <div class="layui-row">
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs6">
                <h3> Related Sites </h3>
            </div>
        </div>
        <div class="layui-row">
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/"><p class="footer-url">home</p></a>
            </div>
            
            <div class="layui-col-md4 layui-col-sm6 layui-col-xs12">
                <a href="/about/"><p class="footer-url">About</p></a>
            </div>
            
        </div>
    </div>
    
    
    <div class="layui-container">
        <p class="copyright">&copy; All rights reserved. Powered by <a href='https://gohugo.io' style='color:#FFFFFF'>Hugo</a> and <a href='https://github.com/ertuil/erblog' style='color:#FFFFFF'>Erblog</a>.</p>
    </div>
</footer>

</body>
</html>
